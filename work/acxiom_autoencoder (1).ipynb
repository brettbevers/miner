{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# from keras.objectives import mse\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.layers import Dense, Lambda, Input\n",
    "# from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from itertools import islice\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from hyperspherical_vae.distributions import VonMisesFisher, HypersphericalUniform\n",
    "\n",
    "class VariationalAutoEncoder(object):\n",
    "    def __init__(self, n_input_units, n_hidden_layers, n_hidden_units, n_latent_units,\n",
    "                 learning_rate=0.005, batch_size=100, min_beta=1.0, max_beta=1.0,\n",
    "                 distribution='normal'):\n",
    "        self.n_input_units = n_input_units\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_latent_units = n_latent_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.min_beta = min_beta\n",
    "        self.max_beta = max_beta\n",
    "        self.distribution = distribution\n",
    "\n",
    "    class Encoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_latent_units, distribution):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_latent_units = n_latent_units\n",
    "            self.distribution = distribution\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs):\n",
    "            self.hidden_layers.append(tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_mu(self, inputs):\n",
    "            if self.distribution == 'normal':\n",
    "                self.mu = tf.layers.Dense(units=self.n_latent_units)\n",
    "            elif self.distribution == 'vmf':\n",
    "                self.mu = tf.layers.Dense(units=self.n_latent_units, \n",
    "                                          activation=lambda x: tf.nn.l2_normalize(x, axis=-1))\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "                \n",
    "            self.applied_mu = self.mu.apply(inputs)\n",
    "            return self.applied_mu\n",
    "\n",
    "        def add_sigma(self, inputs):\n",
    "            if self.distribution == 'normal':\n",
    "                self.sigma = tf.layers.Dense(units=self.n_latent_units)\n",
    "                self.applied_sigma = self.sigma.apply(inputs)\n",
    "            elif self.distribution == 'vmf':\n",
    "                self.sigma = tf.layers.Dense(units=1, activation=tf.nn.softplus)\n",
    "                self.applied_sigma = self.sigma.apply(inputs) + 1\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "            return self.applied_sigma\n",
    "\n",
    "        def build(self, inputs):\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            layer = self.add_hidden_layer(inputs)\n",
    "\n",
    "            for i in range(self.n_hidden_layers - 1):\n",
    "                layer = self.add_hidden_layer(layer)\n",
    "\n",
    "            mu = self.add_mu(layer)\n",
    "            sigma = self.add_sigma(layer)\n",
    "\n",
    "            return mu, sigma\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            mu = sess.run([self.mu.kernel, self.mu.bias])\n",
    "\n",
    "            sigma = sess.run([self.sigma.kernel, self.sigma.bias])\n",
    "\n",
    "            return layers, mu, sigma\n",
    "\n",
    "    class Decoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_output_units):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_output_units = n_output_units\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs):\n",
    "            self.hidden_layers.append(tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_output(self, inputs):\n",
    "            self.output = tf.layers.Dense(units=self.n_output_units)\n",
    "            self.applied_output = self.output.apply(inputs)\n",
    "            return self.applied_output\n",
    "\n",
    "        def build(self, inputs):\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            layer = self.add_hidden_layer(inputs)\n",
    "\n",
    "            for i in range(self.n_hidden_layers - 1):\n",
    "                layer = self.add_hidden_layer(layer)\n",
    "\n",
    "            output = self.add_output(layer)\n",
    "\n",
    "            return output\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            output = sess.run([self.output.kernel, self.output.bias])\n",
    "\n",
    "            return layers, output\n",
    "\n",
    "    def sampled_z(self, mu, sigma, batch_size):\n",
    "        if self.distribution == 'normal':\n",
    "            epsilon = tf.random_normal(tf.stack([int(batch_size), self.n_latent_units]))\n",
    "            z = mu + tf.multiply(epsilon, tf.exp(0.5 * sigma))\n",
    "            loss = -0.5 * self.beta * tf.reduce_sum(1.0 + sigma - tf.square(mu) - tf.exp(sigma), 1)\n",
    "        elif self.distribution == 'vmf':\n",
    "            self.q_z = VonMisesFisher(mu, sigma, validate_args=True, allow_nan_stats=False)\n",
    "            z = self.q_z.sample()\n",
    "            self.p_z = HypersphericalUniform(self.n_latent_units - 1, validate_args=True, allow_nan_stats=False)\n",
    "            loss = self.q_z.kl_divergence(self.p_z)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return z, loss\n",
    "\n",
    "    def build_feature_loss(self, x, output):\n",
    "        return tf.reduce_sum(tf.squared_difference(x, output), 1)\n",
    "\n",
    "    def initialize_tensors(self):\n",
    "        self.x = tf.placeholder(\"float32\", [self.batch_size, self.n_input_units])\n",
    "        self.beta = tf.placeholder(\"float32\", [1, 1])\n",
    "        self.encoder = self.Encoder(self.n_hidden_layers, self.n_hidden_units, self.n_latent_units, \n",
    "                                    self.distribution)\n",
    "        mu, sigma = self.encoder.build(self.x)\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        z, latent_loss = self.sampled_z(self.mu, self.sigma, self.batch_size)\n",
    "        self.z = z\n",
    "        self.latent_loss = latent_loss\n",
    "        \n",
    "        self.decoder = self.Decoder(self.n_hidden_layers, self.n_hidden_units, self.n_input_units)\n",
    "        self.output = self.decoder.build(self.z)\n",
    "        \n",
    "        self.feature_loss = self.build_feature_loss(self.x, self.output)\n",
    "        self.loss = tf.reduce_mean(self.feature_loss + self.latent_loss)\n",
    "        \n",
    "\n",
    "    def generate_beta_values(self, data_count, epochs):\n",
    "        num_batches = int(data_count / self.batch_size)\n",
    "        total_steps = (num_batches * epochs) - epochs\n",
    "        beta_delta = self.max_beta - self.min_beta\n",
    "        log_beta_step = 5 / float(total_steps)\n",
    "        beta_values = [\n",
    "            self.min_beta + (beta_delta * (1 - math.exp(-5 + (i * log_beta_step))))\n",
    "            for i in range(total_steps)\n",
    "        ]\n",
    "        return beta_values\n",
    "\n",
    "    def train_from_rdd(self, data_rdd, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "\n",
    "        data_count = data_rdd.count()\n",
    "        beta_values = self.generate_beta_values(data_count, epochs)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch_index in range(epochs):\n",
    "                iterator = data_rdd.toLocalIterator()\n",
    "                batch_index = 0\n",
    "                while True:\n",
    "                    batch = np.array(list(islice(iterator, self.batch_size)))\n",
    "                    if batch.shape[0] == self.batch_size:\n",
    "                        beta = beta_values.pop(0) if len(beta_values) > 0 else self.min_beta\n",
    "                        feed_dict = {self.x: np.array(batch), self.beta: np.array([[beta]])}\n",
    "\n",
    "                        if not batch_index % 100:\n",
    "                            print(\"beta: {}\".format(beta))\n",
    "                            ls, f_ls, d_ls = sess.run([self.loss, self.feature_loss, self.latent_loss],\n",
    "                                                      feed_dict=feed_dict)\n",
    "                            print(\"loss={}, avg_feature_loss={}, avg_latent_loss={}\".format(ls, np.mean(f_ls),\n",
    "                                                                                            np.mean(d_ls)))\n",
    "                            print('running batch {} in epoch {}'.format(batch_index, epoch_index))\n",
    "                        sess.run(optimizer, feed_dict=feed_dict)\n",
    "                        batch_index += 1\n",
    "                    else:\n",
    "                        print(\"incomplete batch: {}\".format(batch.shape))\n",
    "                        break\n",
    "\n",
    "            print(\"evaluating model...\")\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "    def train(self, data, visualize=False, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "        \n",
    "        data_size = data.shape[0]\n",
    "        batch_size = self.batch_size\n",
    "        beta_values = self.generate_beta_values(data_size, epochs)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            i = 0\n",
    "            while (i * batch_size) < data_size:\n",
    "                batch = data[i * batch_size:(i + 1) * batch_size]\n",
    "                beta = beta_values.pop(0) if len(beta_values) > 0 else self.min_beta\n",
    "                feed_dict = {self.x: batch, self.beta: np.array([[beta]])}\n",
    "                sess.run(optimizer, feed_dict=feed_dict)\n",
    "                if visualize and (not i % int((data_size / batch_size) / 3) or i == int(data_size / batch_size) - 1):\n",
    "                    ls, d, f_ls, d_ls = sess.run([self.loss, self.output, self.feature_loss, self.latent_loss],\n",
    "                                                 feed_dict=feed_dict)\n",
    "                    plt.scatter(batch[:, 0], batch[:, 1])\n",
    "                    plt.show()\n",
    "                    plt.scatter(d[:, 0], d[:, 1])\n",
    "                    plt.show()\n",
    "                    print(i, ls, np.mean(f_ls), np.mean(d_ls))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoderModel(object):\n",
    "    def __init__(self, encoder_layers, mu, sigma, decoder_layers, output):\n",
    "        self.encoder = self.EncoderModel(encoder_layers, mu, sigma)\n",
    "        self.decoder = self.DecoderModel(decoder_layers, output)\n",
    "\n",
    "    def save(self, path):\n",
    "        encoder_layers, encoder_mu, encoder_sigma = self.encoder.dump()\n",
    "        decoder_layers, decoder_output = self.decoder.dump()\n",
    "        serializable_model = (encoder_layers, encoder_mu, encoder_sigma, decoder_layers, decoder_output)\n",
    "        pickle.dump(serializable_model, open(path, 'w+'))\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder.encode(x)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.encoder.encode(x)[0]\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder.decode(x)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return cls(*pickle.load(open(path, 'r')))\n",
    "\n",
    "    class Layer(object):\n",
    "        def __init__(self, kernel, bias, activation='linear'):\n",
    "            self.kernel = kernel\n",
    "            self.bias = bias\n",
    "            self.activation = activation\n",
    "\n",
    "        def dump(self):\n",
    "            return (self.kernel, self.bias, self.activation)\n",
    "\n",
    "        @property\n",
    "        def apply_func(self):\n",
    "            kernel, bias = self.kernel, self.bias\n",
    "\n",
    "            linear = lambda inputs: np.matmul(inputs, kernel) + bias\n",
    "\n",
    "            if self.activation == 'linear':\n",
    "                f = linear\n",
    "            elif self.activation == 'sigmoid':\n",
    "                f = lambda inputs: 1 / (1 + np.exp(-linear(inputs)))\n",
    "\n",
    "            return f\n",
    "\n",
    "        def apply(self, inputs):\n",
    "            return self.apply_func(inputs)\n",
    "\n",
    "    class EncoderModel(object):\n",
    "        def __init__(self, encoder_layers, mu, sigma):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in encoder_layers\n",
    "            ]\n",
    "            self.mu = VariationalAutoEncoderModel.Layer(*mu)\n",
    "            self.sigma = VariationalAutoEncoderModel.Layer(*sigma)\n",
    "\n",
    "        def dump(self):\n",
    "            encoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            encoder_mu = self.mu.dump()[:2]\n",
    "            encoder_sigma = self.sigma.dump()[:2]\n",
    "            return encoder_layers, encoder_mu, encoder_sigma\n",
    "\n",
    "        def encode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.mu.apply(x), self.sigma.apply(x)\n",
    "\n",
    "    class DecoderModel(object):\n",
    "        def __init__(self, decoder_layers, output):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in decoder_layers\n",
    "            ]\n",
    "            self.output = VariationalAutoEncoderModel.Layer(*output)\n",
    "\n",
    "        def dump(self):\n",
    "            decoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            decoder_output = self.output.dump()[:2]\n",
    "            return decoder_layers, decoder_output\n",
    "\n",
    "        def decode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.output.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acxiom_df = spark.read.parquet(\"/dbfs/mnt/ddda/ml-bme-scoring/tmp/learningday_july9/acxiom_segment\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "all_columns_without_cid = acxiom_df.columns[:]\n",
    "all_columns_without_cid.remove('customer_id')\n",
    "assembler = VectorAssembler(inputCols=all_columns_without_cid, outputCol='features')\n",
    "vec_df = assembler.transform(acxiom_df).rdd.map(lambda r: r.features.toArray()).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">beta: 0.09939358477008231\nloss=5607.98876953125, avg_feature_loss=5607.91796875, avg_latent_loss=0.07105191051959991\nrunning batch 0 in epoch 0\nbeta: 0.09939171311962161\nloss=829.3583374023438, avg_feature_loss=825.2938842773438, avg_latent_loss=4.064517498016357\nrunning batch 100 in epoch 0\nbeta: 0.09938983569246654\nloss=929.0733642578125, avg_feature_loss=926.800048828125, avg_latent_loss=2.2731881141662598\nrunning batch 200 in epoch 0\nbeta: 0.09938795247078779\nloss=813.2845458984375, avg_feature_loss=808.1021728515625, avg_latent_loss=5.182384014129639\nrunning batch 300 in epoch 0\nbeta: 0.09938606343670107\nloss=665.3851318359375, avg_feature_loss=659.7977905273438, avg_latent_loss=5.587359428405762\nrunning batch 400 in epoch 0\nbeta: 0.09938416857226684\nloss=507.245361328125, avg_feature_loss=504.4390563964844, avg_latent_loss=2.8062515258789062\nrunning batch 500 in epoch 0\nbeta: 0.09938226785949024\nloss=634.777587890625, avg_feature_loss=632.7509765625, avg_latent_loss=2.026594400405884\nrunning batch 600 in epoch 0\nbeta: 0.09938036128032084\nloss=547.1231079101562, avg_feature_loss=543.2985229492188, avg_latent_loss=3.824645757675171\nrunning batch 700 in epoch 0\nbeta: 0.09937844881665248\nloss=569.5733032226562, avg_feature_loss=563.0645141601562, avg_latent_loss=6.508829116821289\nrunning batch 800 in epoch 0\nbeta: 0.09937653045032314\nloss=485.8006896972656, avg_feature_loss=484.13641357421875, avg_latent_loss=1.6642897129058838\nrunning batch 900 in epoch 0\nbeta: 0.09937460616311479\nloss=547.0964965820312, avg_feature_loss=544.556640625, avg_latent_loss=2.5398426055908203\nrunning batch 1000 in epoch 0\nbeta: 0.09937267593675307\nloss=577.81103515625, avg_feature_loss=572.7484130859375, avg_latent_loss=5.062567710876465\nrunning batch 1100 in epoch 0\nbeta: 0.09937073975290732\nloss=634.1900634765625, avg_feature_loss=629.4771728515625, avg_latent_loss=4.712913990020752\nrunning batch 1200 in epoch 0\nbeta: 0.09936879759319024\nloss=534.3768920898438, avg_feature_loss=531.71484375, avg_latent_loss=2.6621499061584473\nrunning batch 1300 in epoch 0\nbeta: 0.09936684943915779\nloss=532.5691528320312, avg_feature_loss=530.6283569335938, avg_latent_loss=1.9408513307571411\nrunning batch 1400 in epoch 0\nbeta: 0.09936489527230902\nloss=441.23809814453125, avg_feature_loss=438.0067138671875, avg_latent_loss=3.2312698364257812\nrunning batch 1500 in epoch 0\nbeta: 0.09936293507408586\nloss=467.71319580078125, avg_feature_loss=463.7857666015625, avg_latent_loss=3.9274392127990723\nrunning batch 1600 in epoch 0\nbeta: 0.09936096882587299\nloss=677.2711181640625, avg_feature_loss=675.1234130859375, avg_latent_loss=2.1476993560791016\nrunning batch 1700 in epoch 0\nbeta: 0.0993589965089976\nloss=499.7867126464844, avg_feature_loss=497.32452392578125, avg_latent_loss=2.462219476699829\nrunning batch 1800 in epoch 0\nbeta: 0.09935701810472927\nloss=405.0639953613281, avg_feature_loss=402.4110412597656, avg_latent_loss=2.6529016494750977\nrunning batch 1900 in epoch 0\nbeta: 0.09935503359427977\nloss=413.3293762207031, avg_feature_loss=409.2560119628906, avg_latent_loss=4.073336601257324\nrunning batch 2000 in epoch 0\nbeta: 0.09935304295880287\nloss=355.10394287109375, avg_feature_loss=353.0981750488281, avg_latent_loss=2.0057461261749268\nrunning batch 2100 in epoch 0\nbeta: 0.09935104617939419\nloss=393.44268798828125, avg_feature_loss=391.6315612792969, avg_latent_loss=1.8111835718154907\nrunning batch 2200 in epoch 0\nbeta: 0.09934904323709098\nloss=443.1411437988281, avg_feature_loss=441.0848388671875, avg_latent_loss=2.0563008785247803\nrunning batch 2300 in epoch 0\nbeta: 0.09934703411287199\nloss=441.05450439453125, avg_feature_loss=437.2169494628906, avg_latent_loss=3.8375251293182373\nrunning batch 2400 in epoch 0\nbeta: 0.09934501878765722\nloss=308.3763427734375, avg_feature_loss=306.5311584472656, avg_latent_loss=1.8451663255691528\nrunning batch 2500 in epoch 0\nbeta: 0.09934299724230784\nloss=1031.3497314453125, avg_feature_loss=1029.68701171875, avg_latent_loss=1.6625776290893555\nrunning batch 2600 in epoch 0\nbeta: 0.09934096945762591\nloss=408.7004699707031, avg_feature_loss=407.0294189453125, avg_latent_loss=1.6710659265518188\nrunning batch 2700 in epoch 0\nbeta: 0.09933893541435423\nloss=320.6270751953125, avg_feature_loss=316.94989013671875, avg_latent_loss=3.677187442779541\nrunning batch 2800 in epoch 0\nbeta: 0.09933689509317623\nloss=393.5968017578125, avg_feature_loss=391.7860107421875, avg_latent_loss=1.8107701539993286\nrunning batch 2900 in epoch 0\nbeta: 0.09933484847471559\nloss=306.9963684082031, avg_feature_loss=305.3931884765625, avg_latent_loss=1.6032119989395142\nrunning batch 3000 in epoch 0\nbeta: 0.09933279553953635\nloss=304.577880859375, avg_feature_loss=302.25775146484375, avg_latent_loss=2.3201136589050293\nrunning batch 3100 in epoch 0\nbeta: 0.09933073626814241\nloss=317.8263244628906, avg_feature_loss=315.1070861816406, avg_latent_loss=2.7192044258117676\nrunning batch 3200 in epoch 0\nbeta: 0.09932867064097763\nloss=306.9444274902344, avg_feature_loss=303.48858642578125, avg_latent_loss=3.455815076828003\nrunning batch 3300 in epoch 0\nbeta: 0.0993265986384254\nloss=233.39190673828125, avg_feature_loss=231.21060180664062, avg_latent_loss=2.181349039077759\nrunning batch 3400 in epoch 0\nbeta: 0.09932452024080862\nloss=475.7542724609375, avg_feature_loss=473.58538818359375, avg_latent_loss=2.1688284873962402\nrunning batch 3500 in epoch 0\nbeta: 0.09932243542838949\nloss=332.34735107421875, avg_feature_loss=329.0827331542969, avg_latent_loss=3.2645914554595947\nrunning batch 3600 in epoch 0\nbeta: 0.09932034418136922\nloss=235.31475830078125, avg_feature_loss=230.15200805664062, avg_latent_loss=5.1627116203308105\nrunning batch 3700 in epoch 0\nbeta: 0.09931824647988795\nloss=379.31500244140625, avg_feature_loss=377.6285400390625, avg_latent_loss=1.6864383220672607\nrunning batch 3800 in epoch 0\nbeta: 0.09931614230402454\nloss=268.5482482910156, avg_feature_loss=266.16534423828125, avg_latent_loss=2.382916212081909\nrunning batch 3900 in epoch 0\nbeta: 0.0993140316337963\nloss=303.14373779296875, avg_feature_loss=300.2034912109375, avg_latent_loss=2.940329074859619\nrunning batch 4000 in epoch 0\nbeta: 0.09931191444915898\nloss=269.8783264160156, avg_feature_loss=266.4870300292969, avg_latent_loss=3.391256809234619\nrunning batch 4100 in epoch 0\nbeta: 0.09930979073000631\nloss=314.46484375, avg_feature_loss=312.7406005859375, avg_latent_loss=1.7242413759231567\nrunning batch 4200 in epoch 0\nbeta: 0.09930766045617012\nloss=63.72755432128906, avg_feature_loss=54.51168441772461, avg_latent_loss=9.215871810913086\nrunning batch 4300 in epoch 0\nbeta: 0.09930552360741988\nloss=245.57423400878906, avg_feature_loss=242.26336669921875, avg_latent_loss=3.310896396636963\nrunning batch 4400 in epoch 0\nbeta: 0.09930338016346268\nloss=457.87908935546875, avg_feature_loss=453.9906005859375, avg_latent_loss=3.8885037899017334\nrunning batch 4500 in epoch 0\nbeta: 0.09930123010394296\nloss=367.3075256347656, avg_feature_loss=365.1524658203125, avg_latent_loss=2.1550745964050293\nrunning batch 4600 in epoch 0\nbeta: 0.09929907340844232\nloss=352.4922790527344, avg_feature_loss=350.8201904296875, avg_latent_loss=1.672046184539795\nrunning batch 4700 in epoch 0\nbeta: 0.09929691005647938\nloss=268.3903503417969, avg_feature_loss=265.4852294921875, avg_latent_loss=2.905130386352539\nrunning batch 4800 in epoch 0\nbeta: 0.09929474002750947\nloss=184.28622436523438, avg_feature_loss=176.3135528564453, avg_latent_loss=7.972653865814209\nrunning batch 4900 in epoch 0\nbeta: 0.09929256330092462\nloss=277.8028564453125, avg_feature_loss=276.04083251953125, avg_latent_loss=1.762020230293274\nrunning batch 5000 in epoch 0\nbeta: 0.09929037985605316\nloss=339.4725341796875, avg_feature_loss=337.1083679199219, avg_latent_loss=2.364194393157959\nrunning batch 5100 in epoch 0\nbeta: 0.09928818967215965\nloss=363.024658203125, avg_feature_loss=360.0308837890625, avg_latent_loss=2.9937527179718018\nrunning batch 5200 in epoch 0\nbeta: 0.09928599272844467\nloss=323.17279052734375, avg_feature_loss=319.362060546875, avg_latent_loss=3.810713291168213\nrunning batch 5300 in epoch 0\nbeta: 0.09928378900404458\nloss=313.8040771484375, avg_feature_loss=311.93426513671875, avg_latent_loss=1.869746446609497\nrunning batch 5400 in epoch 0\nbeta: 0.09928157847803137\nloss=273.31927490234375, avg_feature_loss=270.55059814453125, avg_latent_loss=2.768704891204834\nrunning batch 5500 in epoch 0\nbeta: 0.09927936112941241\nloss=286.5207824707031, avg_feature_loss=282.9444885253906, avg_latent_loss=3.5762665271759033\nrunning batch 5600 in epoch 0\nbeta: 0.0992771369371303\nloss=218.85116577148438, avg_feature_loss=214.31561279296875, avg_latent_loss=4.53557014465332\nrunning batch 5700 in epoch 0\nbeta: 0.09927490588006264\nloss=306.0579833984375, avg_feature_loss=304.6767272949219, avg_latent_loss=1.3812921047210693\nrunning batch 5800 in epoch 0\nbeta: 0.09927266793702183\nloss=276.1185302734375, avg_feature_loss=274.3394775390625, avg_latent_loss=1.779051661491394\nrunning batch 5900 in epoch 0\nbeta: 0.09927042308675489\nloss=275.8711853027344, avg_feature_loss=273.5320129394531, avg_latent_loss=2.33913516998291\nrunning batch 6000 in epoch 0\nbeta: 0.09926817130794324\nloss=173.8014678955078, avg_feature_loss=169.13453674316406, avg_latent_loss=4.666917324066162\nrunning batch 6100 in epoch 0\nbeta: 0.09926591257920248\nloss=309.76324462890625, avg_feature_loss=307.9303894042969, avg_latent_loss=1.8328229188919067\nrunning batch 6200 in epoch 0\nbeta: 0.09926364687908226\nloss=279.224853515625, avg_feature_loss=277.0008850097656, avg_latent_loss=2.2239859104156494\nrunning batch 6300 in epoch 0\nbeta: 0.09926137418606597\nloss=218.6807098388672, avg_feature_loss=215.9154510498047, avg_latent_loss=2.7652666568756104\nrunning batch 6400 in epoch 0\nbeta: 0.09925909447857062\nloss=261.3768615722656, avg_feature_loss=257.37054443359375, avg_latent_loss=4.006332874298096\nrunning batch 6500 in epoch 0\nbeta: 0.0992568077349466\nloss=281.9504089355469, avg_feature_loss=280.15185546875, avg_latent_loss=1.7985416650772095\nrunning batch 6600 in epoch 0\nbeta: 0.09925451393347749\nloss=290.2538757324219, avg_feature_loss=288.0639343261719, avg_latent_loss=2.189962387084961\nrunning batch 6700 in epoch 0\nbeta: 0.09925221305237981\nloss=310.5086975097656, avg_feature_loss=308.6022644042969, avg_latent_loss=1.90643310546875\nrunning batch 6800 in epoch 0\nbeta: 0.09924990506980291\nloss=351.5648193359375, avg_feature_loss=347.8138732910156, avg_latent_loss=3.7508955001831055\nrunning batch 6900 in epoch 0\nbeta: 0.09924758996382864\nloss=280.3158874511719, avg_feature_loss=278.0795593261719, avg_latent_loss=2.2363579273223877\nrunning batch 7000 in epoch 0\nbeta: 0.09924526771247122\nloss=204.41433715820312, avg_feature_loss=202.1929168701172, avg_latent_loss=2.2214200496673584\nrunning batch 7100 in epoch 0\nbeta: 0.09924293829367702\nloss=232.98976135253906, avg_feature_loss=231.11944580078125, avg_latent_loss=1.8702970743179321\nrunning batch 7200 in epoch 0\nbeta: 0.09924060168532432\nloss=246.20864868164062, avg_feature_loss=242.68060302734375, avg_latent_loss=3.5280849933624268\nrunning batch 7300 in epoch 0\nbeta: 0.09923825786522318\nloss=242.669921875, avg_feature_loss=240.46702575683594, avg_latent_loss=2.202894449234009\nrunning batch 7400 in epoch 0\nbeta: 0.09923590681111512\nloss=273.115966796875, avg_feature_loss=270.80181884765625, avg_latent_loss=2.3141255378723145\nrunning batch 7500 in epoch 0\nbeta: 0.09923354850067297\nloss=324.85064697265625, avg_feature_loss=322.8626708984375, avg_latent_loss=1.9879581928253174\nrunning batch 7600 in epoch 0\nbeta: 0.09923118291150065\nloss=223.40902709960938, avg_feature_loss=219.62159729003906, avg_latent_loss=3.7874350547790527\nrunning batch 7700 in epoch 0\nbeta: 0.09922881002113296\nloss=260.5020446777344, avg_feature_loss=257.8935852050781, avg_latent_loss=2.608457565307617\nrunning batch 7800 in epoch 0\nbeta: 0.09922642980703539\nloss=298.295654296875, avg_feature_loss=295.48846435546875, avg_latent_loss=2.8071837425231934\nrunning batch 7900 in epoch 0\nbeta: 0.09922404224660382\nloss=286.8505554199219, avg_feature_loss=284.33416748046875, avg_latent_loss=2.5163609981536865\nrunning batch 8000 in epoch 0\nbeta: 0.09922164731716442\nloss=268.376708984375, avg_feature_loss=264.5410461425781, avg_latent_loss=3.835688829421997\nrunning batch 8100 in epoch 0\nbeta: 0.09921924499597336\nloss=257.2940368652344, avg_feature_loss=253.5135955810547, avg_latent_loss=3.780460834503174\nrunning batch 8200 in epoch 0\nbeta: 0.09921683526021657\nloss=275.5503234863281, avg_feature_loss=273.2803649902344, avg_latent_loss=2.2699429988861084\nrunning batch 8300 in epoch 0\nbeta: 0.09921441808700966\nloss=342.1620178222656, avg_feature_loss=339.6664123535156, avg_latent_loss=2.495623826980591\nrunning batch 8400 in epoch 0\nbeta: 0.09921199345339753\nloss=232.9936981201172, avg_feature_loss=229.00570678710938, avg_latent_loss=3.9879872798919678\nrunning batch 8500 in epoch 0\nbeta: 0.09920956133635425\nloss=180.615234375, avg_feature_loss=174.49546813964844, avg_latent_loss=6.1197710037231445\nrunning batch 8600 in epoch 0\nbeta: 0.09920712171278284\nloss=279.61407470703125, avg_feature_loss=277.4546203613281, avg_latent_loss=2.1594278812408447\nrunning batch 8700 in epoch 0\nbeta: 0.09920467455951501\nloss=300.4849853515625, avg_feature_loss=298.099853515625, avg_latent_loss=2.385096549987793\nrunning batch 8800 in epoch 0\nbeta: 0.09920221985331099\nloss=249.05288696289062, avg_feature_loss=245.9676513671875, avg_latent_loss=3.0852320194244385\nrunning batch 8900 in epoch 0\nbeta: 0.09919975757085926\nloss=262.5938415527344, avg_feature_loss=258.1780090332031, avg_latent_loss=4.415838241577148\nrunning batch 9000 in epoch 0\nbeta: 0.09919728768877635\nloss=252.1500244140625, avg_feature_loss=249.90235900878906, avg_latent_loss=2.2476553916931152\nrunning batch 9100 in epoch 0\nbeta: 0.09919481018360664\nloss=307.2320251464844, avg_feature_loss=305.3854675292969, avg_latent_loss=1.8465760946273804\nrunning batch 9200 in epoch 0\nbeta: 0.0991923250318221\nloss=243.6920928955078, avg_feature_loss=240.25001525878906, avg_latent_loss=3.4420690536499023\nrunning batch 9300 in epoch 0\nbeta: 0.09918983220982207\nloss=248.7919158935547, avg_feature_loss=243.56326293945312, avg_latent_loss=5.228625297546387\nrunning batch 9400 in epoch 0\nbeta: 0.09918733169393308\nloss=515.9413452148438, avg_feature_loss=514.0339965820312, avg_latent_loss=1.907487154006958\nrunning batch 9500 in epoch 0\nbeta: 0.09918482346040858\nloss=323.8926696777344, avg_feature_loss=321.0921936035156, avg_latent_loss=2.800452947616577\nrunning batch 9600 in epoch 0\nbeta: 0.09918230748542871\nloss=237.94406127929688, avg_feature_loss=234.4425811767578, avg_latent_loss=3.501479387283325\nrunning batch 9700 in epoch 0\nbeta: 0.09917978374510013\nloss=231.7429656982422, avg_feature_loss=226.3691864013672, avg_latent_loss=5.373798370361328\nrunning batch 9800 in epoch 0\nbeta: 0.09917725221545572\nloss=220.10736083984375, avg_feature_loss=217.30548095703125, avg_latent_loss=2.8018784523010254\nrunning batch 9900 in epoch 0\nbeta: 0.0991747128724544\nloss=280.7308349609375, avg_feature_loss=278.6029968261719, avg_latent_loss=2.127840280532837\nrunning batch 10000 in epoch 0\nbeta: 0.0991721656919809\nloss=362.2510070800781, avg_feature_loss=358.85113525390625, avg_latent_loss=3.399862289428711\nrunning batch 10100 in epoch 0\nbeta: 0.09916961064984549\nloss=252.26805114746094, avg_feature_loss=247.73255920410156, avg_latent_loss=4.535473823547363\nrunning batch 10200 in epoch 0\nbeta: 0.09916704772178382\nloss=258.5613098144531, avg_feature_loss=255.52781677246094, avg_latent_loss=3.033508539199829\nrunning batch 10300 in epoch 0\nbeta: 0.09916447688345661\nloss=265.4056396484375, avg_feature_loss=262.3558349609375, avg_latent_loss=3.0497515201568604\nrunning batch 10400 in epoch 0\nbeta: 0.09916189811044952\nloss=284.589111328125, avg_feature_loss=282.0891418457031, avg_latent_loss=2.4999775886535645\nrunning batch 10500 in epoch 0\nbeta: 0.09915931137827277\nloss=202.6073760986328, avg_feature_loss=197.4794464111328, avg_latent_loss=5.127906322479248\nrunning batch 10600 in epoch 0\nbeta: 0.09915671666236107\nloss=269.3983459472656, avg_feature_loss=267.6320495605469, avg_latent_loss=1.7662644386291504\nrunning batch 10700 in epoch 0\nbeta: 0.09915411393807329\nloss=266.72857666015625, avg_feature_loss=263.711669921875, avg_latent_loss=3.0168869495391846\nrunning batch 10800 in epoch 0\nbeta: 0.0991515031806922\nloss=222.1315460205078, avg_feature_loss=218.78231811523438, avg_latent_loss=3.3492443561553955\nrunning batch 10900 in epoch 0\nbeta: 0.09914888436542436\nloss=254.03182983398438, avg_feature_loss=248.35206604003906, avg_latent_loss=5.6797919273376465\nrunning batch 11000 in epoch 0\nbeta: 0.09914625746739975\nloss=223.74093627929688, avg_feature_loss=221.51873779296875, avg_latent_loss=2.222184896469116\nrunning batch 11100 in epoch 0\nbeta: 0.0991436224616716\nloss=216.52769470214844, avg_feature_loss=214.5261688232422, avg_latent_loss=2.001509666442871\nrunning batch 11200 in epoch 0\nbeta: 0.0991409793232162\nloss=294.9809265136719, avg_feature_loss=291.4559326171875, avg_latent_loss=3.525000810623169\nrunning batch 11300 in epoch 0\nbeta: 0.0991383280269325\nloss=321.63714599609375, avg_feature_loss=317.8019714355469, avg_latent_loss=3.835190534591675\nrunning batch 11400 in epoch 0\nbeta: 0.09913566854764207\nloss=307.3776550292969, avg_feature_loss=305.14666748046875, avg_latent_loss=2.2309861183166504\nrunning batch 11500 in epoch 0\nbeta: 0.09913300086008872\nloss=271.8184814453125, avg_feature_loss=270.4061584472656, avg_latent_loss=1.412306785583496\nrunning batch 11600 in epoch 0\nbeta: 0.09913032493893835\nloss=278.8809509277344, avg_feature_loss=276.15960693359375, avg_latent_loss=2.7213306427001953\nrunning batch 11700 in epoch 0\nbeta: 0.09912764075877863\nloss=235.0438690185547, avg_feature_loss=230.74755859375, avg_latent_loss=4.296291351318359\nrunning batch 11800 in epoch 0\nbeta: 0.09912494829411882\nloss=219.94476318359375, avg_feature_loss=218.12173461914062, avg_latent_loss=1.8230338096618652\nrunning batch 11900 in epoch 0\nbeta: 0.09912224751938946\nloss=336.7820739746094, avg_feature_loss=334.6334228515625, avg_latent_loss=2.1486120223999023\nrunning batch 12000 in epoch 0\nbeta: 0.09911953840894225\nloss=267.2245788574219, avg_feature_loss=264.246826171875, avg_latent_loss=2.97774600982666\nrunning batch 12100 in epoch 0\nbeta: 0.09911682093704967\nloss=243.35716247558594, avg_feature_loss=239.76783752441406, avg_latent_loss=3.589334487915039\nrunning batch 12200 in epoch 0\nbeta: 0.0991140950779048\nloss=242.3330535888672, avg_feature_loss=240.62127685546875, avg_latent_loss=1.71177339553833\nrunning batch 12300 in epoch 0\nbeta: 0.09911136080562111\nloss=252.0009765625, avg_feature_loss=249.60317993164062, avg_latent_loss=2.397820472717285\nrunning batch 12400 in epoch 0\nbeta: 0.0991086180942321\nloss=285.9851379394531, avg_feature_loss=283.2039489746094, avg_latent_loss=2.781195640563965\nrunning batch 12500 in epoch 0\nbeta: 0.0991058669176912\nloss=204.53231811523438, avg_feature_loss=200.58462524414062, avg_latent_loss=3.947695016860962\nrunning batch 12600 in epoch 0\nbeta: 0.09910310724987142\nloss=208.73519897460938, avg_feature_loss=205.82081604003906, avg_latent_loss=2.9143612384796143\nrunning batch 12700 in epoch 0\nbeta: 0.09910033906456511\nloss=641.8858032226562, avg_feature_loss=639.3565063476562, avg_latent_loss=2.5293474197387695\nrunning batch 12800 in epoch 0\nbeta: 0.09909756233548377\nloss=265.0478515625, avg_feature_loss=262.30950927734375, avg_latent_loss=2.7383718490600586\nrunning batch 12900 in epoch 0\nbeta: 0.09909477703625773\nloss=204.1602020263672, avg_feature_loss=195.01040649414062, avg_latent_loss=9.149794578552246\nrunning batch 13000 in epoch 0\nbeta: 0.09909198314043595\nloss=296.1728820800781, avg_feature_loss=292.9929504394531, avg_latent_loss=3.179971694946289\nrunning batch 13100 in epoch 0\nbeta: 0.09908918062148576\nloss=225.83140563964844, avg_feature_loss=222.86740112304688, avg_latent_loss=2.964010715484619\nrunning batch 13200 in epoch 0\nbeta: 0.09908636945279257\nloss=321.62030029296875, avg_feature_loss=319.03619384765625, avg_latent_loss=2.5841128826141357\nrunning batch 13300 in epoch 0\nbeta: 0.0990835496076597\nloss=370.35394287109375, avg_feature_loss=367.08416748046875, avg_latent_loss=3.269740343093872\nrunning batch 13400 in epoch 0\nbeta: 0.09908072105930797\nloss=245.28053283691406, avg_feature_loss=240.00042724609375, avg_latent_loss=5.280078887939453\nrunning batch 13500 in epoch 0\nbeta: 0.09907788378087568\nloss=294.824462890625, avg_feature_loss=292.341064453125, avg_latent_loss=2.483381509780884\nrunning batch 13600 in epoch 0\nbeta: 0.09907503774541812\nloss=320.8274230957031, avg_feature_loss=318.15740966796875, avg_latent_loss=2.6699841022491455\nrunning batch 13700 in epoch 0\nbeta: 0.09907218292590748\nloss=288.2028503417969, avg_feature_loss=283.599853515625, avg_latent_loss=4.603038311004639\nrunning batch 13800 in epoch 0\nbeta: 0.09906931929523248\nloss=283.3305969238281, avg_feature_loss=279.0521545410156, avg_latent_loss=4.278460502624512\nrunning batch 13900 in epoch 0\nbeta: 0.09906644682619822\nloss=289.7115783691406, avg_feature_loss=286.52215576171875, avg_latent_loss=3.1894283294677734\nrunning batch 14000 in epoch 0\nbeta: 0.09906356549152581\nloss=171.4755096435547, avg_feature_loss=165.39036560058594, avg_latent_loss=6.085124969482422\nrunning batch 14100 in epoch 0\nbeta: 0.0990606752638522\nloss=270.12628173828125, avg_feature_loss=266.084716796875, avg_latent_loss=4.041516304016113\nrunning batch 14200 in epoch 0\nbeta: 0.09905777611572988\nloss=269.69659423828125, avg_feature_loss=264.766845703125, avg_latent_loss=4.929795265197754\nrunning batch 14300 in epoch 0\nbeta: 0.09905486801962662\nloss=337.6114196777344, avg_feature_loss=335.09649658203125, avg_latent_loss=2.514904737472534\nrunning batch 14400 in epoch 0\nbeta: 0.09905195094792522\nloss=303.44964599609375, avg_feature_loss=300.8075256347656, avg_latent_loss=2.6420998573303223\nrunning batch 14500 in epoch 0\nbeta: 0.09904902487292325\nloss=238.4960174560547, avg_feature_loss=235.27345275878906, avg_latent_loss=3.222578763961792\nrunning batch 14600 in epoch 0\nbeta: 0.09904608976683274\nloss=244.08560180664062, avg_feature_loss=238.5615997314453, avg_latent_loss=5.5239715576171875\nrunning batch 14700 in epoch 0\nbeta: 0.09904314560177999\nloss=196.35768127441406, avg_feature_loss=193.7611083984375, avg_latent_loss=2.5965616703033447\nrunning batch 14800 in epoch 0\nbeta: 0.09904019234980527\nloss=249.00453186035156, avg_feature_loss=246.95652770996094, avg_latent_loss=2.0480480194091797\nrunning batch 14900 in epoch 0\nbeta: 0.09903722998286255\nloss=216.68505859375, avg_feature_loss=212.5624237060547, avg_latent_loss=4.1226301193237305\nrunning batch 15000 in epoch 0\nbeta: 0.09903425847281921\nloss=231.0155029296875, avg_feature_loss=225.63304138183594, avg_latent_loss=5.382457733154297\nrunning batch 15100 in epoch 0\nbeta: 0.09903127779145586\nloss=278.1581726074219, avg_feature_loss=276.2350158691406, avg_latent_loss=1.9231672286987305\nrunning batch 15200 in epoch 0\nbeta: 0.09902828791046596\nloss=264.2594909667969, avg_feature_loss=262.0428771972656, avg_latent_loss=2.2166154384613037\nrunning batch 15300 in epoch 0\nbeta: 0.09902528880145563\nloss=268.5396728515625, avg_feature_loss=265.5283203125, avg_latent_loss=3.0113637447357178\nrunning batch 15400 in epoch 0\nbeta: 0.09902228043594336\nloss=204.4307098388672, avg_feature_loss=199.18177795410156, avg_latent_loss=5.248947620391846\nrunning batch 15500 in epoch 0\nbeta: 0.09901926278535969\nloss=322.0040588378906, avg_feature_loss=319.0743713378906, avg_latent_loss=2.929694175720215\nrunning batch 15600 in epoch 0\nbeta: 0.09901623582104703\nloss=262.5380859375, avg_feature_loss=259.64190673828125, avg_latent_loss=2.896195411682129\nrunning batch 15700 in epoch 0\nbeta: 0.09901319951425934\nloss=253.79527282714844, avg_feature_loss=250.4901580810547, avg_latent_loss=3.3051023483276367\nrunning batch 15800 in epoch 0\nbeta: 0.09901015383616182\nloss=221.50732421875, avg_feature_loss=216.22933959960938, avg_latent_loss=5.278021335601807\nrunning batch 15900 in epoch 0\nbeta: 0.09900709875783072\nloss=263.3393249511719, avg_feature_loss=261.4029235839844, avg_latent_loss=1.9364182949066162\nrunning batch 16000 in epoch 0\nbeta: 0.09900403425025298\nloss=636.517822265625, avg_feature_loss=634.2001953125, avg_latent_loss=2.317687511444092\nrunning batch 16100 in epoch 0\nbeta: 0.09900096028432602\nloss=226.34136962890625, avg_feature_loss=222.65792846679688, avg_latent_loss=3.683427095413208\nrunning batch 16200 in epoch 0\nbeta: 0.09899787683085741\nloss=237.69671630859375, avg_feature_loss=232.14353942871094, avg_latent_loss=5.553178787231445\nrunning batch 16300 in epoch 0\nbeta: 0.09899478386056465\nloss=271.6316833496094, avg_feature_loss=268.6889953613281, avg_latent_loss=2.942713737487793\nrunning batch 16400 in epoch 0\nbeta: 0.09899168134407486\nloss=321.5248718261719, avg_feature_loss=319.1687316894531, avg_latent_loss=2.3561947345733643\n\n*** WARNING: skipped 196314 bytes of output ***\n\nrunning batch 64800 in epoch 1\nbeta: 0.04541055838855983\nloss=378.6899108886719, avg_feature_loss=376.18975830078125, avg_latent_loss=2.5001320838928223\nrunning batch 64900 in epoch 1\nbeta: 0.04524207259120368\nloss=173.36978149414062, avg_feature_loss=169.30242919921875, avg_latent_loss=4.067375183105469\nrunning batch 65000 in epoch 1\nbeta: 0.045073066776366605\nloss=287.8253173828125, avg_feature_loss=285.55694580078125, avg_latent_loss=2.2683920860290527\nrunning batch 65100 in epoch 1\nbeta: 0.04490353933905769\nloss=304.0382385253906, avg_feature_loss=302.1920166015625, avg_latent_loss=1.8461772203445435\nrunning batch 65200 in epoch 1\nbeta: 0.04473348866933229\nloss=288.8377380371094, avg_feature_loss=285.7502746582031, avg_latent_loss=3.087466239929199\nrunning batch 65300 in epoch 1\nbeta: 0.044562913152276826\nloss=234.76455688476562, avg_feature_loss=230.2322998046875, avg_latent_loss=4.532232284545898\nrunning batch 65400 in epoch 1\nbeta: 0.04439181116799341\nloss=288.214111328125, avg_feature_loss=285.7538146972656, avg_latent_loss=2.4602608680725098\nrunning batch 65500 in epoch 1\nbeta: 0.044220181091584466\nloss=834.45654296875, avg_feature_loss=831.8140869140625, avg_latent_loss=2.6425812244415283\nrunning batch 65600 in epoch 1\nbeta: 0.04404802129313734\nloss=274.9134826660156, avg_feature_loss=272.6289367675781, avg_latent_loss=2.284508466720581\nrunning batch 65700 in epoch 1\nbeta: 0.043875330137708814\nloss=210.10838317871094, avg_feature_loss=205.31101989746094, avg_latent_loss=4.797353744506836\nrunning batch 65800 in epoch 1\nbeta: 0.04370210598530939\nloss=209.3391571044922, avg_feature_loss=207.0418701171875, avg_latent_loss=2.2973077297210693\nrunning batch 65900 in epoch 1\nbeta: 0.04352834719088806\nloss=310.7646179199219, avg_feature_loss=308.44305419921875, avg_latent_loss=2.3215558528900146\nrunning batch 66000 in epoch 1\nbeta: 0.04335405210431642\nloss=269.35223388671875, avg_feature_loss=266.6317138671875, avg_latent_loss=2.7205371856689453\nrunning batch 66100 in epoch 1\nbeta: 0.0431792190703731\nloss=288.46026611328125, avg_feature_loss=284.41046142578125, avg_latent_loss=4.0498270988464355\nrunning batch 66200 in epoch 1\nbeta: 0.043003846428728024\nloss=312.4211120605469, avg_feature_loss=310.3687438964844, avg_latent_loss=2.052356004714966\nrunning batch 66300 in epoch 1\nbeta: 0.04282793251392664\nloss=265.2826232910156, avg_feature_loss=263.671142578125, avg_latent_loss=1.6115351915359497\nrunning batch 66400 in epoch 1\nbeta: 0.04265147565537412\nloss=234.62828063964844, avg_feature_loss=231.94903564453125, avg_latent_loss=2.6792306900024414\nrunning batch 66500 in epoch 1\nbeta: 0.042474474177319456\nloss=224.25230407714844, avg_feature_loss=219.89370727539062, avg_latent_loss=4.358626842498779\nrunning batch 66600 in epoch 1\nbeta: 0.04229692639883961\nloss=281.2622985839844, avg_feature_loss=278.8134460449219, avg_latent_loss=2.448837995529175\nrunning batch 66700 in epoch 1\nbeta: 0.04211883063382349\nloss=282.276123046875, avg_feature_loss=280.3128662109375, avg_latent_loss=1.9632779359817505\nrunning batch 66800 in epoch 1\nbeta: 0.04194018519095595\nloss=205.66455078125, avg_feature_loss=203.2573699951172, avg_latent_loss=2.4071733951568604\nrunning batch 66900 in epoch 1\nbeta: 0.04176098837370177\nloss=161.5864715576172, avg_feature_loss=156.95587158203125, avg_latent_loss=4.630600929260254\nrunning batch 67000 in epoch 1\nbeta: 0.04158123848028956\nloss=314.35906982421875, avg_feature_loss=312.2771911621094, avg_latent_loss=2.08186411857605\nrunning batch 67100 in epoch 1\nbeta: 0.041400933803695375\nloss=298.2268371582031, avg_feature_loss=295.5853271484375, avg_latent_loss=2.641472578048706\nrunning batch 67200 in epoch 1\nbeta: 0.04122007263162686\nloss=259.1756286621094, avg_feature_loss=256.2363586425781, avg_latent_loss=2.9392361640930176\nrunning batch 67300 in epoch 1\nbeta: 0.04103865324650676\nloss=253.33229064941406, avg_feature_loss=249.21249389648438, avg_latent_loss=4.119818687438965\nrunning batch 67400 in epoch 1\nbeta: 0.04085667392545663\nloss=322.1788635253906, avg_feature_loss=320.3170166015625, avg_latent_loss=1.861831545829773\nrunning batch 67500 in epoch 1\nbeta: 0.04067413294028054\nloss=308.3638000488281, avg_feature_loss=306.263427734375, avg_latent_loss=2.100370407104492\nrunning batch 67600 in epoch 1\nbeta: 0.04049102855744861\nloss=298.11016845703125, avg_feature_loss=295.869384765625, avg_latent_loss=2.240769863128662\nrunning batch 67700 in epoch 1\nbeta: 0.040307359038080574\nloss=195.994140625, avg_feature_loss=192.69891357421875, avg_latent_loss=3.2952423095703125\nrunning batch 67800 in epoch 1\nbeta: 0.04012312263792923\nloss=263.6722412109375, avg_feature_loss=261.823974609375, avg_latent_loss=1.8482590913772583\nrunning batch 67900 in epoch 1\nbeta: 0.03993831760736391\nloss=274.7666931152344, avg_feature_loss=272.6357116699219, avg_latent_loss=2.1309256553649902\nrunning batch 68000 in epoch 1\nbeta: 0.03975294219135389\nloss=320.0162658691406, avg_feature_loss=317.98370361328125, avg_latent_loss=2.0325722694396973\nrunning batch 68100 in epoch 1\nbeta: 0.039566994629451645\nloss=253.9603271484375, avg_feature_loss=250.80386352539062, avg_latent_loss=3.1564769744873047\nrunning batch 68200 in epoch 1\nbeta: 0.03938047315577624\nloss=286.9764709472656, avg_feature_loss=284.7724914550781, avg_latent_loss=2.2039906978607178\nrunning batch 68300 in epoch 1\nbeta: 0.03919337599899635\nloss=358.6040344238281, avg_feature_loss=356.29229736328125, avg_latent_loss=2.311671257019043\nrunning batch 68400 in epoch 1\nbeta: 0.03900570138231373\nloss=267.6244812011719, avg_feature_loss=265.884765625, avg_latent_loss=1.7397161722183228\nrunning batch 68500 in epoch 1\nbeta: 0.03881744752344615\nloss=191.1270751953125, avg_feature_loss=188.06068420410156, avg_latent_loss=3.0663845539093018\nrunning batch 68600 in epoch 1\nbeta: 0.03862861263461048\nloss=316.7642822265625, avg_feature_loss=314.77520751953125, avg_latent_loss=1.989075779914856\nrunning batch 68700 in epoch 1\nbeta: 0.0384391949225058\nloss=296.6506652832031, avg_feature_loss=295.0591735839844, avg_latent_loss=1.5914850234985352\nrunning batch 68800 in epoch 1\nbeta: 0.03824919258829627\nloss=365.1259765625, avg_feature_loss=363.077392578125, avg_latent_loss=2.0485939979553223\nrunning batch 68900 in epoch 1\nbeta: 0.0380586038275941\nloss=301.01092529296875, avg_feature_loss=297.6625061035156, avg_latent_loss=3.3484303951263428\nrunning batch 69000 in epoch 1\nbeta: 0.03786742683044243\nloss=377.5726623535156, avg_feature_loss=375.9737854003906, avg_latent_loss=1.5989032983779907\nrunning batch 69100 in epoch 1\nbeta: 0.037675659781298096\nloss=289.0408020019531, avg_feature_loss=287.483154296875, avg_latent_loss=1.5576568841934204\nrunning batch 69200 in epoch 1\nbeta: 0.03748330085901441\nloss=368.53948974609375, avg_feature_loss=366.8976135253906, avg_latent_loss=1.6419172286987305\nrunning batch 69300 in epoch 1\nbeta: 0.03729034823682389\nloss=283.2110595703125, avg_feature_loss=280.50799560546875, avg_latent_loss=2.703019142150879\nrunning batch 69400 in epoch 1\nbeta: 0.0370968000823209\nloss=278.6819152832031, avg_feature_loss=277.015380859375, avg_latent_loss=1.6665377616882324\nrunning batch 69500 in epoch 1\nbeta: 0.03690265455744411\nloss=276.5838928222656, avg_feature_loss=274.39227294921875, avg_latent_loss=2.1916213035583496\nrunning batch 69600 in epoch 1\nbeta: 0.03670790981845931\nloss=334.2826232910156, avg_feature_loss=332.5448303222656, avg_latent_loss=1.7377336025238037\nrunning batch 69700 in epoch 1\nbeta: 0.0365125640159417\nloss=221.44361877441406, avg_feature_loss=218.27279663085938, avg_latent_loss=3.170802116394043\nrunning batch 69800 in epoch 1\nbeta: 0.036316615294758395\nloss=256.4494934082031, avg_feature_loss=254.6947784423828, avg_latent_loss=1.7547191381454468\nrunning batch 69900 in epoch 1\nbeta: 0.03612006179405079\nloss=398.9405212402344, avg_feature_loss=397.332763671875, avg_latent_loss=1.607663869857788\nrunning batch 70000 in epoch 1\nbeta: 0.035922901647216905\nloss=359.1869812011719, avg_feature_loss=357.8206787109375, avg_latent_loss=1.3663084506988525\nrunning batch 70100 in epoch 1\nbeta: 0.03572513298189362\nloss=243.48406982421875, avg_feature_loss=240.37857055664062, avg_latent_loss=3.105475664138794\nrunning batch 70200 in epoch 1\nbeta: 0.03552675391993896\nloss=285.7875671386719, avg_feature_loss=283.8292236328125, avg_latent_loss=1.9583289623260498\nrunning batch 70300 in epoch 1\nbeta: 0.03532776257741417\nloss=227.8524627685547, avg_feature_loss=226.27020263671875, avg_latent_loss=1.5822808742523193\nrunning batch 70400 in epoch 1\nbeta: 0.035128157064565915\nloss=326.4775085449219, avg_feature_loss=324.51434326171875, avg_latent_loss=1.963179588317871\nrunning batch 70500 in epoch 1\nbeta: 0.03492793548580828\nloss=312.6141662597656, avg_feature_loss=309.8056945800781, avg_latent_loss=2.8084418773651123\nrunning batch 70600 in epoch 1\nbeta: 0.03472709593970485\nloss=235.9512481689453, avg_feature_loss=233.16632080078125, avg_latent_loss=2.7849106788635254\nrunning batch 70700 in epoch 1\nbeta: 0.034525636518950374\nloss=247.0906982421875, avg_feature_loss=245.75160217285156, avg_latent_loss=1.3390908241271973\nrunning batch 70800 in epoch 1\nbeta: 0.034323555310353075\nloss=255.7801513671875, avg_feature_loss=254.34750366210938, avg_latent_loss=1.4326728582382202\nrunning batch 70900 in epoch 1\nbeta: 0.03412085039481619\nloss=99.6474609375, avg_feature_loss=93.50067138671875, avg_latent_loss=6.146767616271973\nrunning batch 71000 in epoch 1\nbeta: 0.03391751984731981\nloss=318.865234375, avg_feature_loss=316.757568359375, avg_latent_loss=2.107682943344116\nrunning batch 71100 in epoch 1\nbeta: 0.03371356173690267\nloss=225.25636291503906, avg_feature_loss=223.6211700439453, avg_latent_loss=1.6351935863494873\nrunning batch 71200 in epoch 1\nbeta: 0.03350897412664366\nloss=259.4844665527344, avg_feature_loss=258.1369323730469, avg_latent_loss=1.3475147485733032\nrunning batch 71300 in epoch 1\nbeta: 0.03330375507364361\nloss=228.95498657226562, avg_feature_loss=226.93283081054688, avg_latent_loss=2.022158622741699\nrunning batch 71400 in epoch 1\nbeta: 0.033097902629006706\nloss=281.6929016113281, avg_feature_loss=279.362548828125, avg_latent_loss=2.330332040786743\nrunning batch 71500 in epoch 1\nbeta: 0.03289141483782202\nloss=263.37005615234375, avg_feature_loss=261.8865966796875, avg_latent_loss=1.4834650754928589\nrunning batch 71600 in epoch 1\nbeta: 0.032684289739144994\nloss=349.4425354003906, avg_feature_loss=347.8723449707031, avg_latent_loss=1.5701626539230347\nrunning batch 71700 in epoch 1\nbeta: 0.032476525365978746\nloss=268.5064392089844, avg_feature_loss=266.2794189453125, avg_latent_loss=2.2270376682281494\nrunning batch 71800 in epoch 1\nbeta: 0.032268119745255494\nloss=317.6742248535156, avg_feature_loss=314.91278076171875, avg_latent_loss=2.761446952819824\nrunning batch 71900 in epoch 1\nbeta: 0.03205907089781755\nloss=390.4016418457031, avg_feature_loss=388.7227478027344, avg_latent_loss=1.6789295673370361\nrunning batch 72000 in epoch 1\nbeta: 0.03184937683839891\nloss=259.80792236328125, avg_feature_loss=258.4666442871094, avg_latent_loss=1.3412885665893555\nrunning batch 72100 in epoch 1\nbeta: 0.031639035575606156\nloss=243.09124755859375, avg_feature_loss=241.44204711914062, avg_latent_loss=1.6491886377334595\nrunning batch 72200 in epoch 1\nbeta: 0.0314280451118996\nloss=188.9257049560547, avg_feature_loss=185.96865844726562, avg_latent_loss=2.957024574279785\nrunning batch 72300 in epoch 1\nbeta: 0.031216403443574292\nloss=275.143310546875, avg_feature_loss=273.9771728515625, avg_latent_loss=1.1661503314971924\nrunning batch 72400 in epoch 1\nbeta: 0.031004108560741032\nloss=289.8723449707031, avg_feature_loss=288.65826416015625, avg_latent_loss=1.2140796184539795\nrunning batch 72500 in epoch 1\nbeta: 0.03079115844730726\nloss=264.1227722167969, avg_feature_loss=262.09014892578125, avg_latent_loss=2.032654047012329\nrunning batch 72600 in epoch 1\nbeta: 0.0305775510809579\nloss=268.0821838378906, avg_feature_loss=265.4017639160156, avg_latent_loss=2.6804263591766357\nrunning batch 72700 in epoch 1\nbeta: 0.030363284433136198\nloss=354.0142517089844, avg_feature_loss=352.8110046386719, avg_latent_loss=1.2031959295272827\nrunning batch 72800 in epoch 1\nbeta: 0.03014835646902439\nloss=329.0393371582031, avg_feature_loss=327.5660095214844, avg_latent_loss=1.4733086824417114\nrunning batch 72900 in epoch 1\nbeta: 0.029932765147524444\nloss=256.6815185546875, avg_feature_loss=254.7339324951172, avg_latent_loss=1.9476234912872314\nrunning batch 73000 in epoch 1\nbeta: 0.02971650842123863\nloss=226.05502319335938, avg_feature_loss=223.34747314453125, avg_latent_loss=2.7075085639953613\nrunning batch 73100 in epoch 1\nbeta: 0.029499584236450167\nloss=327.227294921875, avg_feature_loss=326.0516662597656, avg_latent_loss=1.175626277923584\nrunning batch 73200 in epoch 1\nbeta: 0.029281990533103458\nloss=428.19598388671875, avg_feature_loss=426.79132080078125, avg_latent_loss=1.4046685695648193\nrunning batch 73300 in epoch 1\nbeta: 0.029063725244784892\nloss=307.54559326171875, avg_feature_loss=305.793212890625, avg_latent_loss=1.752396583557129\nrunning batch 73400 in epoch 1\nbeta: 0.02884478629870299\nloss=261.975830078125, avg_feature_loss=259.4664001464844, avg_latent_loss=2.5094356536865234\nrunning batch 73500 in epoch 1\nbeta: 0.028625171615668785\nloss=308.3075256347656, avg_feature_loss=307.54156494140625, avg_latent_loss=0.7659670114517212\nrunning batch 73600 in epoch 1\nbeta: 0.02840487911007604\nloss=233.0547637939453, avg_feature_loss=229.9361114501953, avg_latent_loss=3.1186139583587646\nrunning batch 73700 in epoch 1\nbeta: 0.02818390668988148\nloss=450.0687255859375, avg_feature_loss=448.3141784667969, avg_latent_loss=1.7545483112335205\nrunning batch 73800 in epoch 1\nbeta: 0.02796225225658492\nloss=275.4419860839844, avg_feature_loss=272.993408203125, avg_latent_loss=2.448535919189453\nrunning batch 73900 in epoch 1\nbeta: 0.0277399137052093\nloss=239.36412048339844, avg_feature_loss=238.1748046875, avg_latent_loss=1.1893088817596436\nrunning batch 74000 in epoch 1\nbeta: 0.027516888924280766\nloss=101.06941223144531, avg_feature_loss=95.08792114257812, avg_latent_loss=5.981497764587402\nrunning batch 74100 in epoch 1\nbeta: 0.027293175795808533\nloss=879.6253051757812, avg_feature_loss=878.1175537109375, avg_latent_loss=1.5076415538787842\nrunning batch 74200 in epoch 1\nbeta: 0.02706877219526483\nloss=339.95050048828125, avg_feature_loss=337.72100830078125, avg_latent_loss=2.229504346847534\nrunning batch 74300 in epoch 1\nbeta: 0.02684367599156478\nloss=301.7608642578125, avg_feature_loss=300.54241943359375, avg_latent_loss=1.2184245586395264\nrunning batch 74400 in epoch 1\nbeta: 0.026617885047045875\nloss=66.62002563476562, avg_feature_loss=60.096435546875, avg_latent_loss=6.5236029624938965\nrunning batch 74500 in epoch 1\nbeta: 0.02639139721744809\nloss=315.1854248046875, avg_feature_loss=313.8092041015625, avg_latent_loss=1.3762328624725342\nrunning batch 74600 in epoch 1\nbeta: 0.02616421035189327\nloss=302.2685241699219, avg_feature_loss=300.6680908203125, avg_latent_loss=1.6004279851913452\nrunning batch 74700 in epoch 1\nbeta: 0.02593632229286473\nloss=305.649658203125, avg_feature_loss=304.41473388671875, avg_latent_loss=1.2349239587783813\nrunning batch 74800 in epoch 1\nbeta: 0.025707730876186795\nloss=12.112035751342773, avg_feature_loss=4.682429790496826, avg_latent_loss=7.429606914520264\nrunning batch 74900 in epoch 1\nbeta: 0.02547843393100425\nloss=294.4862060546875, avg_feature_loss=292.6583251953125, avg_latent_loss=1.827881932258606\nrunning batch 75000 in epoch 1\nbeta: 0.025248429279761713\nloss=231.56248474121094, avg_feature_loss=227.61285400390625, avg_latent_loss=3.9496452808380127\nrunning batch 75100 in epoch 1\nbeta: 0.02501771473818294\nloss=277.71551513671875, avg_feature_loss=276.86053466796875, avg_latent_loss=0.8549647331237793\nrunning batch 75200 in epoch 1\nbeta: 0.02478628811525011\nloss=96.18944549560547, avg_feature_loss=92.181640625, avg_latent_loss=4.007795333862305\nrunning batch 75300 in epoch 1\nbeta: 0.02455414721318303\nloss=265.0001220703125, avg_feature_loss=263.32843017578125, avg_latent_loss=1.6716722249984741\nrunning batch 75400 in epoch 1\nbeta: 0.024321289827418182\nloss=257.6435241699219, avg_feature_loss=254.9827117919922, avg_latent_loss=2.660792350769043\nrunning batch 75500 in epoch 1\nbeta: 0.024087713746587983\nloss=279.7093811035156, avg_feature_loss=278.6495361328125, avg_latent_loss=1.0598286390304565\nrunning batch 75600 in epoch 1\nbeta: 0.023853416752499388\nloss=315.753662109375, avg_feature_loss=314.57269287109375, avg_latent_loss=1.1809844970703125\nrunning batch 75700 in epoch 1\nbeta: 0.02361839662011332\nloss=362.809814453125, avg_feature_loss=361.450927734375, avg_latent_loss=1.3588168621063232\nrunning batch 75800 in epoch 1\nbeta: 0.023382651117523218\nloss=274.8963928222656, avg_feature_loss=272.4275817871094, avg_latent_loss=2.4688210487365723\nrunning batch 75900 in epoch 1\nbeta: 0.023146178005933923\nloss=263.2096252441406, avg_feature_loss=261.8184509277344, avg_latent_loss=1.391176462173462\nrunning batch 76000 in epoch 1\nbeta: 0.022908975039640442\nloss=280.5292053222656, avg_feature_loss=279.33074951171875, avg_latent_loss=1.1984573602676392\nrunning batch 76100 in epoch 1\nbeta: 0.02267103996600657\nloss=216.2629852294922, avg_feature_loss=214.6706085205078, avg_latent_loss=1.5923773050308228\nrunning batch 76200 in epoch 1\nbeta: 0.022432370525443557\nloss=244.41336059570312, avg_feature_loss=242.16871643066406, avg_latent_loss=2.244640588760376\nrunning batch 76300 in epoch 1\nbeta: 0.02219296445138859\nloss=302.8651123046875, avg_feature_loss=301.7112121582031, avg_latent_loss=1.1539182662963867\nrunning batch 76400 in epoch 1\nbeta: 0.021952819470283318\nloss=268.78399658203125, avg_feature_loss=268.13751220703125, avg_latent_loss=0.6464789509773254\nrunning batch 76500 in epoch 1\nbeta: 0.021711933301552223\nloss=291.3228759765625, avg_feature_loss=289.8395080566406, avg_latent_loss=1.483351707458496\nrunning batch 76600 in epoch 1\nbeta: 0.021470303657581003\nloss=283.44610595703125, avg_feature_loss=281.2548522949219, avg_latent_loss=2.1912894248962402\nrunning batch 76700 in epoch 1\nbeta: 0.02122792824369487\nloss=277.0642395019531, avg_feature_loss=275.9124755859375, avg_latent_loss=1.1517959833145142\nrunning batch 76800 in epoch 1\nbeta: 0.020984804758136508\nloss=306.61041259765625, avg_feature_loss=305.6075134277344, avg_latent_loss=1.002867341041565\nrunning batch 76900 in epoch 1\nbeta: 0.020740930892044644\nloss=290.5229797363281, avg_feature_loss=288.9078063964844, avg_latent_loss=1.6151922941207886\nrunning batch 77000 in epoch 1\nbeta: 0.020496304329431815\nloss=93.49961853027344, avg_feature_loss=87.80158233642578, avg_latent_loss=5.698019504547119\nrunning batch 77100 in epoch 1\nbeta: 0.020250922747162434\nloss=220.85963439941406, avg_feature_loss=220.08584594726562, avg_latent_loss=0.7737930417060852\nrunning batch 77200 in epoch 1\nbeta: 0.020004783814930774\nloss=297.7485046386719, avg_feature_loss=297.0242919921875, avg_latent_loss=0.724221408367157\nrunning batch 77300 in epoch 1\nbeta: 0.019757885195238814\nloss=269.6683044433594, avg_feature_loss=268.6589660644531, avg_latent_loss=1.009339451789856\nrunning batch 77400 in epoch 1\nbeta: 0.019510224543374005\nloss=32.806365966796875, avg_feature_loss=26.284303665161133, avg_latent_loss=6.522068500518799\nrunning batch 77500 in epoch 1\nbeta: 0.019261799507387087\nloss=344.8482360839844, avg_feature_loss=344.0716552734375, avg_latent_loss=0.7765686511993408\nrunning batch 77600 in epoch 1\nbeta: 0.019012607728069668\nloss=251.1524200439453, avg_feature_loss=250.2542724609375, avg_latent_loss=0.8981369137763977\nrunning batch 77700 in epoch 1\nbeta: 0.018762646838931855\nloss=339.771240234375, avg_feature_loss=338.4823913574219, avg_latent_loss=1.2888678312301636\nrunning batch 77800 in epoch 1\nbeta: 0.018511914466179794\nloss=236.08558654785156, avg_feature_loss=233.98521423339844, avg_latent_loss=2.100362539291382\nrunning batch 77900 in epoch 1\nbeta: 0.018260408228693174\nloss=312.9077453613281, avg_feature_loss=311.8061828613281, avg_latent_loss=1.1015375852584839\nrunning batch 78000 in epoch 1\nbeta: 0.018008125738002353\nloss=275.8092041015625, avg_feature_loss=274.9867858886719, avg_latent_loss=0.8224371075630188\nrunning batch 78100 in epoch 1\nbeta: 0.017755064598266063\nloss=341.97137451171875, avg_feature_loss=340.6322326660156, avg_latent_loss=1.3391830921173096\nrunning batch 78200 in epoch 1\nbeta: 0.017501222406248428\nloss=253.68682861328125, avg_feature_loss=251.8262176513672, avg_latent_loss=1.8606367111206055\nrunning batch 78300 in epoch 1\nbeta: 0.017246596751296157\nloss=254.67333984375, avg_feature_loss=253.95213317871094, avg_latent_loss=0.7212398052215576\nrunning batch 78400 in epoch 1\nbeta: 0.01699118521531569\nloss=300.3296813964844, avg_feature_loss=299.59979248046875, avg_latent_loss=0.7298763990402222\nrunning batch 78500 in epoch 1\nbeta: 0.016734985372750238\nloss=287.6304626464844, avg_feature_loss=286.7393493652344, avg_latent_loss=0.8911269307136536\nrunning batch 78600 in epoch 1\nbeta: 0.016477994790556685\nloss=315.9354553222656, avg_feature_loss=314.1164855957031, avg_latent_loss=1.8190325498580933\nrunning batch 78700 in epoch 1\nbeta: 0.01622021102818259\nloss=251.81597900390625, avg_feature_loss=251.04808044433594, avg_latent_loss=0.7678911089897156\nrunning batch 78800 in epoch 1\nbeta: 0.015961631637542894\nloss=260.89837646484375, avg_feature_loss=260.1561584472656, avg_latent_loss=0.7422329187393188\nrunning batch 78900 in epoch 1\nbeta: 0.015702254162996754\nloss=226.1675567626953, avg_feature_loss=225.10516357421875, avg_latent_loss=1.0624157190322876\nrunning batch 79000 in epoch 1\nbeta: 0.015442076141324198\nloss=273.15460205078125, avg_feature_loss=271.69488525390625, avg_latent_loss=1.459709882736206\nrunning batch 79100 in epoch 1\nbeta: 0.015181095101702704\nloss=577.9403076171875, avg_feature_loss=577.0632934570312, avg_latent_loss=0.8770280480384827\nrunning batch 79200 in epoch 1\nbeta: 0.014919308565683867\nloss=278.3950500488281, avg_feature_loss=277.4162292480469, avg_latent_loss=0.9787848591804504\nrunning batch 79300 in epoch 1\nbeta: 0.014656714047169499\nloss=275.21453857421875, avg_feature_loss=274.189208984375, avg_latent_loss=1.0253384113311768\nrunning batch 79400 in epoch 1\nbeta: 0.014393309052388534\nloss=150.20809936523438, avg_feature_loss=148.30880737304688, avg_latent_loss=1.8992974758148193\nrunning batch 79500 in epoch 1\nbeta: 0.014129091079872987\nloss=319.8129577636719, avg_feature_loss=319.275390625, avg_latent_loss=0.5375364422798157\nrunning batch 79600 in epoch 1\nbeta: 0.013864057620434309\nloss=272.23883056640625, avg_feature_loss=271.5003662109375, avg_latent_loss=0.738490879535675\nrunning batch 79700 in epoch 1\nbeta: 0.01359820615713954\nloss=254.8118896484375, avg_feature_loss=253.9141387939453, avg_latent_loss=0.8977448344230652\nrunning batch 79800 in epoch 1\nbeta: 0.013331534165287443\nloss=208.19996643066406, avg_feature_loss=206.4692840576172, avg_latent_loss=1.7306782007217407\nrunning batch 79900 in epoch 1\nbeta: 0.01306403911238445\nloss=228.26004028320312, avg_feature_loss=227.6986083984375, avg_latent_loss=0.561439573764801\nrunning batch 80000 in epoch 1\nbeta: 0.012795718458120713\nloss=245.6311492919922, avg_feature_loss=244.99246215820312, avg_latent_loss=0.6386951804161072\nrunning batch 80100 in epoch 1\nbeta: 0.012526569654345896\nloss=285.2582702636719, avg_feature_loss=284.5859375, avg_latent_loss=0.6723641157150269\nrunning batch 80200 in epoch 1\nbeta: 0.012256590145045014\nloss=208.54086303710938, avg_feature_loss=207.19454956054688, avg_latent_loss=1.3463027477264404\nrunning batch 80300 in epoch 1\nbeta: 0.011985777366314145\nloss=249.79229736328125, avg_feature_loss=249.15878295898438, avg_latent_loss=0.6335310935974121\nrunning batch 80400 in epoch 1\nbeta: 0.01171412874633619\nloss=232.44171142578125, avg_feature_loss=231.39830017089844, avg_latent_loss=1.043445110321045\nrunning batch 80500 in epoch 1\nbeta: 0.0114416417053561\nloss=253.39315795898438, avg_feature_loss=252.68150329589844, avg_latent_loss=0.7116390466690063\nrunning batch 80600 in epoch 1\nbeta: 0.011168313655656865\nloss=286.8450622558594, avg_feature_loss=285.903076171875, avg_latent_loss=0.9419590830802917\nrunning batch 80700 in epoch 1\nbeta: 0.010894142001534603\nloss=207.65884399414062, avg_feature_loss=207.0635986328125, avg_latent_loss=0.5952572822570801\nrunning batch 80800 in epoch 1\nbeta: 0.010619124139274039\nloss=274.3851623535156, avg_feature_loss=273.5068054199219, avg_latent_loss=0.8783915042877197\nrunning batch 80900 in epoch 1\nbeta: 0.01034325745712373\nloss=293.5839538574219, avg_feature_loss=292.80413818359375, avg_latent_loss=0.7798279523849487\nrunning batch 81000 in epoch 1\nbeta: 0.010066539335271267\nloss=359.2618103027344, avg_feature_loss=357.9205017089844, avg_latent_loss=1.3413136005401611\nrunning batch 81100 in epoch 1\nincomplete batch: (17, 493)\nevaluating model...\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, train a model like so:\n",
    "\n",
    "model = VariationalAutoEncoder(n_input_units=len(all_columns_without_cid), n_hidden_layers=2, \n",
    "                               n_hidden_units=493, n_latent_units=1, \n",
    "                               learning_rate=0.005, batch_size=100, \n",
    "                               min_beta=0.01, max_beta=0.1, distribution='normal')\\\n",
    "    .train_from_rdd(vec_df, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">34</span><span class=\"ansired\">]: </span>\n{&apos;kernel&apos;: array([[-0.03483483,  0.03007248, -0.04911023, ...,  0.01890367,\n          0.00477581,  0.06695753],\n        [-0.06810383, -0.05531385,  0.00313132, ...,  0.00987785,\n         -0.00476372, -0.07200421],\n        [ 0.01131103, -0.02053376, -0.03117744, ...,  0.01971988,\n         -0.01309029, -0.07793407],\n        ...,\n        [ 0.0643691 ,  0.06967107,  0.03350699, ...,  0.00326344,\n         -0.07068758, -0.05514351],\n        [-0.02716121, -0.06617416,  0.03759897, ...,  0.04376343,\n          0.04337563, -0.00626849],\n        [ 0.00545701,  0.01964737,  0.04047941, ..., -0.0142002 ,\n         -0.06282534, -0.06395233]], dtype=float32),\n &apos;activation&apos;: &apos;sigmoid&apos;,\n &apos;bias&apos;: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32)}\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers[0].__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1202086&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> dbutils<span class=\"ansiyellow\">.</span>fs<span class=\"ansiyellow\">.</span>mkdirs<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;mnt/ddda/ml-bme-scoring/learningday_july9/&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansired\"># dbutils.fs.rm(&quot;/mnt/ddda/ml-bme-scoring/learningday_july9/model_2&quot;)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> </span>model<span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/dbfs/mnt/ddda/ml-bme-scoring/learningday_july9/model_2&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;command-1202082&gt;</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, path)</span>\n<span class=\"ansigreen\">    230</span>         decoder_layers<span class=\"ansiyellow\">,</span> decoder_output <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>decoder<span class=\"ansiyellow\">.</span>dump<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    231</span>         serializable_model <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">(</span>encoder_layers<span class=\"ansiyellow\">,</span> encoder_mu<span class=\"ansiyellow\">,</span> encoder_sigma<span class=\"ansiyellow\">,</span> decoder_layers<span class=\"ansiyellow\">,</span> decoder_output<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 232</span><span class=\"ansiyellow\">         </span>pickle<span class=\"ansiyellow\">.</span>dump<span class=\"ansiyellow\">(</span>serializable_model<span class=\"ansiyellow\">,</span> open<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;w+&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    233</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    234</span>     <span class=\"ansigreen\">def</span> encode<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: write() argument must be str, not bytes</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"mnt/ddda/ml-bme-scoring/learningday_july9/\")\n",
    "# dbutils.fs.rm(\"/mnt/ddda/ml-bme-scoring/learningday_july9/model_2\")\n",
    "model.save(\"/dbfs/mnt/ddda/ml-bme-scoring/learningday_july9/model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickled_model = model\n",
    "# pickled_model = VariationalAutoEncoderModel.load(\"/dbfs/mnt/ddda/ml-bme-scoring/learningday_july9/model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/tmp/1538070765628-0/PythonShell.py:294: RuntimeWarning: overflow encountered in exp\n  def cancel(self, expectedJobGroup):\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply encoder from trained model like so:\n",
    "vec_sample = np.array(vec_df.sample(False, 0.01).collect())\n",
    "encoded_sample = pickled_model.project(vec_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">IndexError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1202106&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> plt<span class=\"ansiyellow\">.</span>clf<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> plt<span class=\"ansiyellow\">.</span>title<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;2D projection of Acxiom customer data via VAE&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> </span>plt<span class=\"ansiyellow\">.</span>hist2d<span class=\"ansiyellow\">(</span>encoded_sample<span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> encoded_sample<span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> bins<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">100</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> display<span class=\"ansiyellow\">(</span>plt<span class=\"ansiyellow\">.</span>gcf<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">IndexError</span>: index 1 is out of bounds for axis 1 with size 1</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.title('2D projection of Acxiom customer data via VAE')\n",
    "plt.hist2d(encoded_sample[:,0], encoded_sample[:,1], bins=100)\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAJYCAYAAACadoJwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\r\n/wc2PKjpFpmXQgAAAABJRU5ErkJggg==\r\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.title('1D projection of Acxiom customer data via VAE')\n",
    "plt.hist(encoded_sample[:,0], bins=500)\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">IndexError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1202464&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> plt<span class=\"ansiyellow\">.</span>clf<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> plt<span class=\"ansiyellow\">.</span>title<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;1D projection of Acxiom customer data via VAE&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> </span>plt<span class=\"ansiyellow\">.</span>hist<span class=\"ansiyellow\">(</span>encoded_sample<span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> bins<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">100</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> display<span class=\"ansiyellow\">(</span>plt<span class=\"ansiyellow\">.</span>gcf<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">IndexError</span>: index 1 is out of bounds for axis 1 with size 1</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.title('1D projection of Acxiom customer data via VAE')\n",
    "plt.hist(encoded_sample[:,1], bins=100)\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/tmp/1538070765628-0/PythonShell.py:294: RuntimeWarning: overflow encountered in exp\n  def cancel(self, expectedJobGroup):\n<span class=\"ansired\">Out[</span><span class=\"ansired\">37</span><span class=\"ansired\">]: </span>(81146, 493)\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sample = pickled_model.decode(encoded_sample)\n",
    "decoded_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">10 169814.631403283 0.35346921735155895\n11 168230.64708702313 0.2859982391769125\n12 166127.47515171467 0.3205820647145118\n13 165945.7592431185 0.31932539547241406\n14 162489.9409957651 0.34532051555849985\n15 163588.39736371854 0.1791531500358305\n16 146622.97336812405 0.2635625611171456\n17 161254.9387022793 0.3138592780749614\n18 158666.38133764602 0.40359128274422734\n19 153903.845700506 0.367401497521079\n20 138565.64718741048 0.2337919519911557\n21 149758.03473567578 0.33783680860625387\n22 150606.84898423904 0.3769716178359536\n23 141254.47062250177 0.4216266785390457\n24 136586.59535842235 0.4393252047666717\n25 145694.47891231166 0.42014155015879323\n26 133500.01217523843 0.46250850340009325\n27 131438.4941043795 0.4070118615153668\n28 131440.76420460732 0.4637340897513616\n29 134203.86782168417 0.23419941715979137\n30 134106.90759131717 0.11811509545269035\n31 124796.38952153984 0.2328476997526726\n32 122951.04679988792 0.29839460827401315\n33 117589.4044720991 0.2599342590512594\n34 121066.70690564754 0.22402704819473562\n35 120513.07031114957 0.44712503424597827\n36 121857.6751043509 0.15579033833673528\n37 119287.18637656758 0.2262699676823379\n38 115216.92698508759 0.292087640413152\n39 105530.62314476138 0.13660160562365473\n40 109631.25825995763 0.16049137040672948\n41 112707.27546844447 0.017795803456085415\n42 110835.55896505876 0.2043686438517292\n43 113993.22758175679 0.20032692318731443\n44 106471.96068511628 0.06781200489253017\n45 107368.5689006108 0.14875657680150195\n46 108757.78209539125 0.22698500307624828\n47 107781.60232121074 0.14876634065554198\n48 106558.52015031918 0.22063774494404823\n49 107419.81291796482 0.18400810596240555\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "encoded_x_sample = encoded_sample[:,0].reshape(-1,1)\n",
    "dencoded_y_sample = decoded_sample[:,0].reshape(-1,1)\n",
    "\n",
    "for i in range(10, 50):\n",
    "  z_train, z_test, x_train, x_test = train_test_split(encoded_x_sample, dencoded_y_sample, test_size=0.4)\n",
    "  gmm = GaussianMixture(i).fit(z_train)\n",
    "  bic = gmm.bic(z_test)\n",
    "  labels = gmm.predict(z_test)\n",
    "  ss = silhouette_score(x_test, labels)\n",
    "  print(i, bic, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_gmm = GaussianMixture(28).fit(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_with_cluster = lambda v: (final_gmm.predict(pickled_model.project(v))[0], v)\n",
    "\n",
    "labeled_vecs = assembler.transform(acxiom_df).rdd.map(lambda r: r.features.toArray()).map(label_with_cluster).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "cs = Statistics.colStats(labeled_vecs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = labeled_vecs.keys().distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_vecs.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1201755&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     13</span> acxiom_results <span class=\"ansiyellow\">=</span> acxiom_results<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;cluster&apos;</span><span class=\"ansiyellow\">,</span> cluster_udf<span class=\"ansiyellow\">(</span>acxiom_results<span class=\"ansiyellow\">.</span>features<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>persist<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 15</span><span class=\"ansiyellow\"> </span>clusters <span class=\"ansiyellow\">=</span> acxiom_results<span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span>acxiom_results<span class=\"ansiyellow\">.</span>cluster<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>distinct<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> r<span class=\"ansiyellow\">:</span> r<span class=\"ansiyellow\">.</span>cluster<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     16</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     17</span> total_descr <span class=\"ansiyellow\">=</span> acxiom_results<span class=\"ansiyellow\">.</span>describe<span class=\"ansiyellow\">(</span>all_columns_without_cid<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    822</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    823</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 824</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    825</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    826</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1158</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1159</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1160</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1161</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1162</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    318</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    319</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 320</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    321</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    322</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 32.0 failed 4 times, most recent failure: Lost task 15.3 in stage 32.0 (TID 200, 10.239.234.93, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1738)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1726)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1725)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1725)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:963)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:963)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:963)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1961)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1909)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1897)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:760)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2147)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2166)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2191)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:951)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:187)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:226)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "cluster_udf = F.udf(lambda v: final_gmm.predict(pickled_model.encode(v.toArray())[0])[0], T.IntegerType())\n",
    "\n",
    "labeled_vec = \n",
    "\n",
    "assembler = VectorAssembler(inputCols=all_columns_without_cid, outputCol='features')\n",
    "acxiom_results = assembler.transform(acxiom_df)\n",
    "acxiom_results = acxiom_results.withColumn('cluster', cluster_udf(acxiom_results.features)).persist()\n",
    "\n",
    "clusters = acxiom_results.select(acxiom_results.cluster).distinct().rdd.map(lambda r: r.cluster).collect()\n",
    "\n",
    "total_descr = acxiom_results.describe(all_columns_without_cid).collect()\n",
    "means = np.array([r[c] for c in all_columns_without_cid for r in total_descr if r.summary == 'mean'])\n",
    "stddevs = np.array([r[c] for c in all_columns_without_cid for r in total_descr if r.summary == 'stddev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = assembler.transform(acxiom_df).rdd.map(lambda r: pickled_model.encode(r.features)).persist()\n",
    "df_test.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in clusters:\n",
    "  df = acxiom_results.where(acxiom_results.cluster == i)\n",
    "  cluster_descr = df.describe(all_columns_without_cid).collect()\n",
    "  cluster_means = np.array([r[c] for c in all_columns_without_cid for r in cluster_descr if r.summary == 'mean'])\n",
    "  cluster_stddevs = np.array([r[c] for c in all_columns_without_cid for r in cluster_descr if r.summary == 'stddev'])\n",
    "  \n",
    "  kl_dev = np.log(cluster_stddevs / stddevs) + (((stddevs**2 - cluster_stddevs**2) + (means - cluster_means)**2) / (2 * cluster_stddevs**2))\n",
    "  \n",
    "  results.extend([(i, c, kl, c_mu, c_sig, c_mu - mu, c_sig - sig) for c, kl, c_mu, c_sig, mu, sig in \n",
    "                  zip(all_columns_without_cid, kl_dev, cluster_means, cluster_stddevs, means, stddevs)])\n",
    "\n",
    "results_df = spark.createDataFrame(results, ['cluster', 'field', 'kl_divergence', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1201382&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>display<span class=\"ansiyellow\">(</span>results_df<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;results_df&apos; is not defined</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "name": "acxiom_autoencoder (1)",
  "notebookId": 1202080.0
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
