{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.objectives import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Lambda, Input\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/dbfs/mnt/ddda/ml-bme-scoring/tmp/learningday_june8/acxiom_features.npz\", \"rb\") as in_file:\n",
    "  training_data = np.load(in_file)\n",
    "training_data = training_data.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder_model(input_dims, hidden_layer_dims, z_dims, output_activation='linear', hidden_activation='relu'):\n",
    "  x = enc_in = Input(shape=(input_dims,))\n",
    "  for layer_dims in hidden_layer_dims:\n",
    "    x = Dense(layer_dims, activation=hidden_activation)(x)\n",
    "  z = Dense(z_dims, activation=hidden_activation)(x)\n",
    "  \n",
    "  x = dec_in = Input(shape=(z_dims,))\n",
    "  for layers_dims in reversed(hidden_layer_dims):\n",
    "    x = Dense(layer_dims, activation=hidden_activation)(x)\n",
    "  x_recon = Dense(input_dims, activation=output_activation)(x)\n",
    " \n",
    "  encoder = Model(inputs=[enc_in], outputs=[z])\n",
    "  decoder = Model(inputs=[dec_in], outputs=[x_recon])\n",
    "  \n",
    "  trainer_in = Input(shape=(input_dims,))\n",
    "  trainer_out = decoder(encoder(trainer_in))\n",
    "  trainer = Model(inputs=[trainer_in], outputs=[trainer_out])\n",
    "  trainer.compile(loss='mse', optimizer='adam')\n",
    "  \n",
    "  return encoder, decoder, trainer\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "  \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "  # Arguments:\n",
    "      args (tensor): mean and log of variance of Q(z|X)\n",
    "  # Returns:\n",
    "      z (tensor): sampled latent vector\n",
    "  \"\"\"\n",
    "  z_mean, z_log_var = args\n",
    "  batch = K.shape(z_mean)[0]\n",
    "  dim = K.int_shape(z_mean)[1]\n",
    "  # by default, random_normal has mean=0 and std=1.0\n",
    "  epsilon = K.random_normal(shape=(batch, dim)) * K.cast(K.learning_phase(), K.floatx())\n",
    "  return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "  \n",
    "\n",
    "def build_vae_model(input_dims, hidden_layer_dims, z_dims, output_activation='linear', hidden_activation='relu', z_activation='linear'):\n",
    "  x = enc_in = Input(shape=(input_dims,))\n",
    "  for layer_dims in hidden_layer_dims:\n",
    "    x = Dense(layer_dims, activation=hidden_activation)(x)\n",
    "  z_mu = Dense(z_dims, activation=z_activation, name='z_mu')(x)\n",
    "  z_log_var = Dense(z_dims, activation=z_activation, name='z_log_var')(x)\n",
    "  z = Lambda(sampling, output_shape=(z_dims,))([z_mu, z_log_var])\n",
    "  \n",
    "  x = dec_in = Input(shape=(z_dims,))\n",
    "  for layers_dims in reversed(hidden_layer_dims):\n",
    "    x = Dense(layer_dims, activation=hidden_activation)(x)\n",
    "  x_recon = Dense(input_dims, activation=output_activation)(x)\n",
    " \n",
    "  encoder = Model(inputs=[enc_in], outputs=[z])\n",
    "  decoder = Model(inputs=[dec_in], outputs=[x_recon])\n",
    "  \n",
    "  trainer_in = Input(shape=(input_dims,))\n",
    "  trainer_out = decoder(encoder(enc_in))\n",
    "  trainer = Model(inputs=[enc_in], outputs=[trainer_out])\n",
    "  \n",
    "  def vae_loss(x, x_recon):\n",
    "    recon_loss = mse(x, x_recon)\n",
    "\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mu) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    return K.mean(recon_loss + kl_loss)\n",
    "  \n",
    "  optimizer = Adam(lr=0.0001)\n",
    "  trainer.compile(loss=vae_loss, optimizer=optimizer)\n",
    "  return encoder, decoder, trainer\n",
    "\n",
    "\n",
    "def build_rel_vae_model(input_dims, hidden_layer_dims, z_dims, output_activation='linear',\n",
    "                        hidden_activation='relu', z_activation='linear', alpha=0.5):\n",
    "  x = enc_in = Input(shape=(input_dims,))\n",
    "  for layer_dims in hidden_layer_dims:\n",
    "    x = Dense(layer_dims, activation=hidden_activation)(x)\n",
    "  z_mu = Dense(z_dims, activation=z_activation, name='z_mu')(x)\n",
    "  z_log_var = Dense(z_dims, activation=z_activation, name='z_log_var')(x)\n",
    "  z = Lambda(sampling, output_shape=(z_dims,))([z_mu, z_log_var])\n",
    "  \n",
    "  x = dec_in = Input(shape=(z_dims,))\n",
    "  for layers_dims in reversed(hidden_layer_dims):\n",
    "    x = Dense(layer_dims, activation=hidden_activation)(x)\n",
    "  x_recon = Dense(input_dims, activation=output_activation)(x)\n",
    " \n",
    "  encoder = Model(inputs=[enc_in], outputs=[z])\n",
    "  decoder = Model(inputs=[dec_in], outputs=[x_recon])\n",
    "  \n",
    "  trainer_in = Input(shape=(input_dims,))\n",
    "  trainer_out = decoder(encoder(enc_in))\n",
    "  trainer = Model(inputs=[enc_in], outputs=[trainer_out])\n",
    "  \n",
    "  def rel_vae_loss(x, x_recon):\n",
    "    recon_loss = mse(x, x_recon)\n",
    "    \n",
    "    batch_dims = K.shape(x)[0]\n",
    "    dims = K.int_shape(x_recon)[1:]\n",
    "    dims_prod = np.prod(dims) * K.cast(batch_dims, K.floatx())\n",
    "    r_x = K.dot(K.transpose(x), x) / dims_prod\n",
    "    r_recon = K.dot(K.transpose(x_recon), x_recon) / dims_prod\n",
    "    relational_loss = K.mean(K.square(r_x - r_recon))\n",
    "    \n",
    "    kl_loss = 1 + z_log_var - K.square(z_mu) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    return K.mean(kl_loss + (1. - alpha) * recon_loss) + alpha * relational_loss\n",
    "  \n",
    "  optimizer = Adam(lr=0.001)\n",
    "  trainer.compile(loss=rel_vae_loss, optimizer=optimizer)\n",
    "  return encoder, decoder, trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = training_data.shape[-1]\n",
    "hidden_layer_dims = ()  # shallow VAE \n",
    "hidden_layer_dims = (100,)  # deeper VAE\n",
    "z_dims = 100\n",
    "alpha = 0.5\n",
    "encoder, decoder, trainer = build_rel_vae_model(input_dims, hidden_layer_dims, z_dims, alpha=alpha)\n",
    "#encoder, decoder, trainer = build_vae_model(input_dims, hidden_layer_dims, z_dims)\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_data[:100000]\n",
    "history = trainer.fit(x, x, validation_split=0.2, verbose=False, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.ylim(0, 10)\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = customer_embeddings = encoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.title('2D projection of Acxiom customer data')\n",
    "# hw = 0.4\n",
    "# plt.xlim(-0.3, 0.3)\n",
    "#plt.ylim(-hw, 0.2)\n",
    "plt.scatter(xt[:, 0], xt[:, 1])\n",
    "\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)\n",
    "pca.fit(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.transform(xt)\n",
    "\n",
    "plt.clf()\n",
    "hw = 5\n",
    "plt.xlim(-hw, hw)\n",
    "plt.ylim(-hw, hw)\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1])\n",
    "plt.title('2D projection of Acxiom customer data using PCA')\n",
    "\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "data = sc.parallelize([a for a in xt])\n",
    "clusters = KMeans.train(data, 20, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.scatter(xt[:, 0], xt[:, 1])\n",
    "\n",
    "cs = np.array(clusters.centers)\n",
    "plt.scatter(cs[:, 0], cs[:, 1], color='yellow')\n",
    "\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "svm = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "svm.fit(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw = 0.5\n",
    "grid_samples = 100\n",
    "xx, yy = np.meshgrid(np.linspace(-hw, hw, grid_samples), np.linspace(-hw, hw, grid_samples))\n",
    "Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.clf()\n",
    "plt.xlim(-hw, hw)\n",
    "plt.ylim(-hw, hw)\n",
    "\n",
    "plt.scatter(xt[:, 0], xt[:, 1])\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='yellow')\n",
    "\n",
    "display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ^^ COOL CLUSTERS, FELLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "name": "acxiom_autoencoder",
  "notebookId": 1193471.0
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
