{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /Lib/jdbc_redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1,2), (3,4)], ['foo', 'bar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format(\"com.databricks.spark.redshift\") \\\n",
    "  .option(\"url\", urlOption) \\\n",
    "  .option(\"dbtable\", 'kafka.test_test_test') \\\n",
    "  .option(\"tempdir\", tempdir) \\\n",
    "  .option(\"extracopyoptions\", \"TRUNCATECOLUMNS ACCEPTINVCHARS\") \\\n",
    "  .option('forward_spark_s3_credentials', True) \\\n",
    "  .mode('overwrite') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"AKIAJ5FMABOIUBBOZMNA\"\n",
    "SECRET_KEY = \"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\"\n",
    "try:\n",
    "  import urllib\n",
    "  ENCODED_SECRET_KEY = urllib.quote(SECRET_KEY, \"\")\n",
    "except:\n",
    "  import urllib.parse\n",
    "  ENCODED_SECRET_KEY = urllib.parse.quote(SECRET_KEY, \"\")\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", ACCESS_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", ENCODED_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o59.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:705)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:620)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n\t... 30 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3c35cb0dc26d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macxiom_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://dsc.databricks.prod/acxiom/history\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# facts_customers = spark.read.parquet(\"/mnt/ddda/acxiom/facts_customers\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# invoices = spark.read.parquet(\"/mnt/ddda/acxiom/invoices\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o59.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:705)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:620)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n\t... 30 more\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "acxiom_df = spark.read.parquet(\"s3a://dsc.databricks.prod/acxiom/history\")\n",
    "# facts_customers = spark.read.parquet(\"/mnt/ddda/acxiom/facts_customers\")\n",
    "# invoices = spark.read.parquet(\"/mnt/ddda/acxiom/invoices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(acxiom_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# for field in acxiom_df.schema:\n",
    "#   if re.match( '^ibe', field.name):\n",
    "#     print field.name, map(lambda r: r[0], acxiom_df.select(field.name).distinct().collect())\n",
    "\n",
    "\n",
    "# acxiom_df.select(\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_06_TO_10_Unknown_Gender\").distinct().rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in boolean_fields_y_n_encoded:\n",
    "  print(field, acxiom_df.select(field).distinct().rdd.map(lambda r: r[0]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_fields_1_null_encoded = [\n",
    "  \"ibe2019_2019\", \"ibe2021_2021\", \"ibe2022_2022\", \"ibe2032_2032\", \"ibe2067_2067\", \"ibe2200_2200\", \"ibe2351_2351\", \n",
    "  \"ibe6136_6136\", \"ibe6137_6137\", \"ibe6139_6139\", \"ibe6141_6141\", \"ibe6142_6142\", \"ibe6201_6201\", \"ibe6281_6281\", \n",
    "  \"ibe6313_6313\", \"ibe6314_6314\", \"ibe6330_6330\", \"ibe6331_6331\", \"ibe6380_6380\", \"ibe6425_6425\", \"ibe6484_6484\",\n",
    "  \"ibe6537_6537\", \"ibe6538_6538\", \"ibe6556_6556\", \"ibe6579_6579\", \"ibe6703_6703\", \"ibe6793_6793\", \"ibe6805_6805\",\n",
    "  \"ibe6819_6819\", \"ibe7719_7719\", \"ibe7720_7720\", \"ibe7726_7726\", \"ibe7732_7732\", \"ibe7739_7739\", \"ibe7744_7744\", \n",
    "  \"ibe7745_7745\", \"ibe7748_7748\", \"ibe7750_7750\", \"ibe7751_7751\", \"ibe7752_7752\", \"ibe7753_7753\", \"ibe7758_7758\",\n",
    "  \"ibe7765_7765\", \"ibe7769_7769\", \"ibe7771_7771\", \"ibe7772_7772\", \"ibe7777_7777\", \"ibe7779_7779\", \"ibe7796_7796\",\n",
    "  \"ibe7801_7801\", \"ibe7803_7803\", \"ibe7804_7804\", \"ibe7809_7809\", \"ibe7824_7824\", \"ibe7827_7827\", \"ibe7831_7831\", \n",
    "  \"ibe7841_7841\", \"ibe7844_7844\", \"ibe7848_7848\", \"ibe7849_7849\", \"ibe8082_8082\", \"ibe8257_8257\", \n",
    "  \"ibe8272_8272\", \"ibe8276_8276\", \"ibe8278_8278\", \"ibe8279_8279\", \"ibe8321_8321\"\n",
    "]\n",
    "  \n",
    "boolean_fields_int_encoded = [\n",
    "    \"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Males_18_TO_24\", \"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Females_18_TO_24\", \"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Unknown_Gender_18_TO_24\", \"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Males_25_TO_34\", \"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Females_25_TO_34\", \"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Unknown_Gender_25_TO_34\",\n",
    "  \"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Males_35_TO_44\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Females_35_TO_44\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Unknown_Gender_35_TO_44\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Males_45_TO_54\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Females_45_TO_54\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Unknown_Gender_45_TO_54\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Males_55_TO_64\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Females_55_TO_64\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Unknown_Gender_55_TO_64\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Males_65_TO_74\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Females_65_TO_74\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Unknown_Gender_65_TO_74\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Males_75_PLUS\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Females_75_PLUS\",\n",
    "\"ibe8600_IBE_Premier_AdultAgeRangesOneZero_Unknown_Gender_75_PLUS\",\n",
    "  \"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_00_TO_02_Male\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_00_TO_02_Female\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_00_TO_02_Unknown_Gender\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_03_TO_05_Male\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_03_TO_05_Female\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_03_TO_05_Unknown_Gender\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_06_TO_10_Male\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_06_TO_10_Female\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_06_TO_10_Unknown_Gender\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_11_TO_15_Male\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_11_TO_15_Female\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_11_TO_15_Unknown_Gender\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_16_TO_17_Male\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_16_TO_17_Female\",\n",
    "\"ibe8601_IBE_Premier_ChildrensAgeRangesOneZero_Age_16_TO_17_Unknown_Gender\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_LT_1\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_1\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_2\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_3\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_4\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_5\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_6\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_7\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_8\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_9\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_10\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_11\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_12\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_13\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_14\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_15\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_16\",\n",
    "\"ibe8603_IBE_Premier_ChildrensAgeRangesOneYear_Age_EQ_17\",\n",
    "  \"ibe8621_IBE_Premier_CreditCardIndicatorOneZero_Bank_Card_Holder\",\n",
    "\"ibe8621_IBE_Premier_CreditCardIndicatorOneZero_Gas_Department_Retail_Card_Holder\",\n",
    "\"ibe8621_IBE_Premier_CreditCardIndicatorOneZero_Travel_and_Entertainment_Card_Holder\",\n",
    "\"ibe8621_IBE_Premier_CreditCardIndicatorOneZero_Credit_Card_Holder_Unknown_Type\",\n",
    "\"ibe8621_IBE_Premier_CreditCardIndicatorOneZero_Premium_Card_Holder\",\n",
    "\"ibe8621_IBE_Premier_CreditCardIndicatorOneZero_Upscale_Department_Store_Card_Holder\",\n",
    "  \"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Retail_Membership_Warehouse\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Retail_Main_Street_Retail\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Retail_High_Volume_Low_End_Department_Store_Buyers\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Retail_Standard_Retail\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Specialty_Sporting_Goods\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Specialty_Specialty_Apparel\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Specialty_Specialty\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Specialty_Computer_Electronics_Buyers\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Specialty_Furniture_Buyers\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Specialty_Home_Office_Supply_Purchases\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Standard_Specialty_Home_Improvement\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Upscale_Retail_High_End_Retail_Buyers_Upscale_Retail\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Upscale_Specialty_Travel_Personal_Services\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Bank_Financial_Services_Banking\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Finance_Company_Financial_Services_Install_Credit\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Oil_Company_Oil_Company\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Miscellaneous_Financial_Services_Insurance\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Miscellaneous_TV_Mail_Order_Purchases\",\n",
    "\"ibe9153_IBE_Premier_RetailPurchaseCategories_Miscellaneous_Miscellaneous\",\n",
    "]\n",
    "\n",
    "boolean_fields_y_n_encoded = [\n",
    "  \"ibe7471_7471\", \"ibe8591_8591\", \"ibe8619_8619\", \"ibe8622_8622\", \"ibe8630_8630\", \"ibe8653_8653\", \"ibe8654_8654\", \"ibe8670_8670\", \"ibe8680_8680\", \"ibe8692_8692\", \"ibe8693_8693\", \"ibe8815_8815\", \"ibe9780_9780\", \"ibe7622_Children_Presence_in_Household_Premier_COMPLETE_Children\"\n",
    "]\n",
    "\n",
    "boolean_fields_single_char_encoded = [\n",
    "  \"ibe8615_8615\", \"ibe8620_8620\", \"ibe8639_8639\", \"ibe8808_8808\", \"ibe9100_9100\", \"ibe9557_9557\", \"ibe8177_8177\"\n",
    "]\n",
    "\n",
    "int_encoded_fields = [\"ibe2300_2300\", \"ibe8666_8666\", \"ibe8850_8850\", \"ibe9040_9040\", \"ibe9509_9509\", \"ibe9514_9514\"]\n",
    "\n",
    "counts = [\n",
    "  \"ibe6604_6604\", \"ibe6605_6605\", \"ibe6610_6610\", \"ibe8628_8628\", \"ibe8629_8629\", \"ibe8640_8640\", \"ibe8816_8816\", \"ibe7602_Children_Number_in_Household_Premier_COMPLETE_Children\"\n",
    "]\n",
    "\n",
    "ordinal = [\n",
    "  \"ibe8641_8641\", \"ibe8652_8652\", \"ibe8671_8671\", \"ibe8836_8836\"\n",
    "]\n",
    "\n",
    "ages = [\"ibe8616_8616\", \"ibe8626_8626\"]\n",
    "\n",
    "# combine district id with state field to get state congressional district\n",
    "congressional_district = \"ibe2403_2403\"\n",
    "\n",
    "categorical_fields = [\"ibe7469_7469\", \"ibe8440_8440\",\"ibe8487_8487\",\"ibe8531_8531\",\"ibe8581_8581\",\"ibe8602_8602\", \"ibe8604_8604\", \"ibe8607_8607\", \"ibe8608_8608\", \"ibe8609_8609\", \"ibe8614_8614\", \"ibe8637_8637\", \"ibe8648_8648\", \"ibe8688_8688\", \"ibe8701_8701\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acxiom_df.createOrReplaceTempView('acxiom')\n",
    "facts_customers.createOrReplaceTempView('facts_customers')\n",
    "invoices.createOrReplaceTempView('invoices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_totals = sqlContext.sql(\"\"\"\n",
    "SELECT f.customer_id, SUM(grand_total) AS revenue, SUM(profit) AS profit, SUM(total_cost) AS cost, SUM(total_shipping) AS shipping\n",
    "    FROM invoices i\n",
    "    JOIN facts_customers f ON i.customer_id = f.customer_id\n",
    "    WHERE i.created_at >= f.initial_start_date \n",
    "        AND i.created_at < add_months(f.initial_start_date, 12) \n",
    "        AND i.status = 1\n",
    "        AND i.grand_total IS NOT NULL\n",
    "    GROUP BY 1\n",
    "\"\"\")\n",
    "\n",
    "base_df = acxiom_df.join(invoice_totals, 'customer_id').persist()\n",
    "base_df.createOrReplaceTempView('feature_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "categorical_field_values = {\n",
    "  field: map(lambda r: r[0], base_df.select(field).distinct().collect())\n",
    "  for field in categorical_fields\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_count = base_df.count()\n",
    "# sorted_revenues = base_df.rdd.map(lambda r: r.revenue).sortBy(lambda r: r).zipWithIndex().map(lambda x: (x[1],x[0])).persist()\n",
    "\n",
    "# deciles = [sorted_revenues.lookup(int(base_count*(i/10.0)))[0] for i in range(1,10)]\n",
    "\n",
    "# print(deciles)\n",
    "\n",
    "from decimal import Decimal\n",
    "deciles = [Decimal('30.0000'), Decimal('64.8000'), Decimal('99.0000'), Decimal('126.0000'), Decimal('156.0000'), Decimal('189.0000'), Decimal('222.0000'), Decimal('265.5000'), Decimal('336.0000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def calc_revenue_decile(revenue):\n",
    "  decile = 9\n",
    "  for i, d in enumerate(deciles):\n",
    "    if revenue < d:\n",
    "      decile = i\n",
    "      break\n",
    "  return decile\n",
    "\n",
    "calc_revenue_decile_udf = F.udf(calc_revenue_decile, T.IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def val_null_field_func(value):\n",
    "  if value is None or value == 0:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "\n",
    "val_null_field_udf = F.udf(val_null_field_func, T.IntegerType())\n",
    "\n",
    "def y_n_field_func(value):\n",
    "  if value is None or value in ('n', 'N'):\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "\n",
    "y_n_field_udf = F.udf(y_n_field_func, T.IntegerType())\n",
    "\n",
    "boolean_revenue_df = base_df.filter(base_df['ibe8614_8614'].isin('20173', '20174')).select(\n",
    "  calc_revenue_decile_udf(base_df.revenue).alias('decile'),\n",
    "  *([\n",
    "    val_null_field_udf(base_df[field]).alias(field)\n",
    "    for field in (boolean_fields_1_null_encoded + boolean_fields_single_char_encoded)\n",
    "  ] + [\n",
    "    y_n_field_udf(base_df[field]).alias(field)\n",
    "    for field in boolean_fields_y_n_encoded\n",
    "  ])\n",
    ")\n",
    "\n",
    "chosen_fields = boolean_fields_1_null_encoded + boolean_fields_single_char_encoded + boolean_fields_y_n_encoded\n",
    "\n",
    "sampled_fields = np.array(chosen_fields)[np.random.randint(0, len(chosen_fields), 4)]\n",
    "print(sampled_fields)\n",
    "\n",
    "boolean_revenue = boolean_revenue_df.rdd\\\n",
    "  .map(lambda r: (r.decile, [\n",
    "      r[field]\n",
    "      for field in sampled_fields\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.mllib.stat import Statistics as S\n",
    "import numpy as np\n",
    "\n",
    "stats = S.colStats(boolean_revenue.map(lambda t: t[1]))\n",
    "means = stats.mean()\n",
    "stds = np.sqrt(stats.variance())\n",
    "\n",
    "train = boolean_revenue.map(lambda t: Vectors.dense([t[0]/10.0] + [\n",
    "  0 if z[2] == 0 else (z[0] - z[1]) / z[2]\n",
    "  for z in zip(t[1], means, stds)\n",
    "])).persist()\n",
    "train_count = train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "from scipy.misc import logsumexp\n",
    "from scipy.stats import chi2\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "def remove_dim(matrix, index):\n",
    "  return np.delete(np.delete(matrix, index, axis=0), index, axis=1)\n",
    "\n",
    "def log_likelihood(model, data):\n",
    "  weights = model.weights\n",
    "  gaussians = [ x for x in zip(weights, model.gaussians) ]\n",
    "  d = gaussians[0][1].mu.toArray().shape[0]\n",
    "  gmms = [[\n",
    "    [ \n",
    "      (g.mu.toArray(), np.linalg.inv(g.sigma.toArray())) \n",
    "      for weight, g in gaussians\n",
    "    ],\n",
    "    np.array([ \n",
    "      weight * np.power(2*pi, -d/2.0) * np.power(np.linalg.det(g.sigma.toArray()), -0.5) \n",
    "      for weight, g in gaussians\n",
    "    ])\n",
    "  ]]\n",
    "  \n",
    "  marginal_gmms = [\n",
    "    [\n",
    "      [\n",
    "        (np.delete(g.mu.toArray(), i),  np.linalg.inv(remove_dim(g.sigma.toArray(), i)))\n",
    "        for weight, g in gaussians\n",
    "      ],\n",
    "      [\n",
    "        weight * np.power(2*pi, -(d-1)/2.0) * np.power(np.linalg.det(remove_dim(g.sigma.toArray(), i)), -0.5)\n",
    "        for weight, g in gaussians\n",
    "      ],\n",
    "      [\n",
    "        (g.mu[i], g.sigma[(i,i)])\n",
    "        for weight, g in gaussians\n",
    "      ],\n",
    "      [\n",
    "        weight * np.power(2*pi*(g.sigma[(i,i)]**2), -0.5)\n",
    "        for weight, g in gaussians\n",
    "      ]\n",
    "    ]\n",
    "    for i in range(d)\n",
    "  ]\n",
    "\n",
    "  def f(v):\n",
    "    x = v.toArray()\n",
    "    results = []\n",
    "    for gmm, coeffs in gmms:\n",
    "      exponents = []\n",
    "      for mean, cov_inv in gmm:\n",
    "        diff = x - mean\n",
    "        exponents.append(-0.5 * np.matmul(np.matmul(diff, cov_inv), diff))\n",
    "      results.append(logsumexp(np.array(exponents), b=coeffs))\n",
    "    for index, (gmm, coeffs, uni_gmm, uni_coeffs) in enumerate(marginal_gmms):\n",
    "      marginal_x = np.delete(x, index)\n",
    "      exponents = []\n",
    "      for mean, cov_inv in gmm:\n",
    "        diff = marginal_x - mean\n",
    "        exponents.append(-0.5 * np.matmul(np.matmul(diff, cov_inv), diff))\n",
    "      marginal_ll = logsumexp(np.array(exponents), b=coeffs)\n",
    "      univ_exponents = []\n",
    "      for mean, sigma in uni_gmm:\n",
    "        diff = x[index] - mean\n",
    "        univ_exponents.append(-0.5 * diff**2 / sigma**2)\n",
    "      null_ll = logsumexp(np.array(univ_exponents), b=uni_coeffs)\n",
    "      results.append(marginal_ll + null_ll)\n",
    "    return np.array(results)\n",
    "  \n",
    "  sums = data.map(f).sum()\n",
    "  ll = sums[0]\n",
    "  p_values = [ chi2.sf(-2*(x - ll), df=d-1) for x in sums[1:] ]\n",
    "  print(sums)\n",
    "  return ll, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "from scipy.stats import norm\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import GaussianMixture\n",
    "\n",
    "from math import pi\n",
    "from scipy.misc import logsumexp\n",
    "from scipy.stats import chi2\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "def remove_dim(matrix, index):\n",
    "  return np.delete(np.delete(matrix, index, axis=0), index, axis=1)\n",
    "\n",
    "def pseudo_inverse(matrix):\n",
    "  U,s,Vh = np.linalg.svd(matrix)\n",
    "  return np.dot(np.dot(np.transpose(Vh), np.diag(np.power(s,-1))), np.transpose(U))\n",
    "\n",
    "def log_likelihood(model, data):\n",
    "  weights = model.weights\n",
    "  \n",
    "  gaussians = []\n",
    "  singular_sigmas = 0\n",
    "  for weight, g in zip(weights, model.gaussians):\n",
    "    try:\n",
    "      s_inv = np.linalg.inv(g.sigma.toArray())\n",
    "      gaussians.append((weight, g, s_inv))\n",
    "    except np.linalg.LinAlgError:\n",
    "      singular_sigmas += 1\n",
    "  if singular_sigmas > 0:\n",
    "    print(\"Eliminated {} gaussians with singular covariance.\".format(singular_sigmas))\n",
    "  \n",
    "  d = gaussians[0][1].mu.toArray().shape[0]\n",
    "  b = np.power(2*pi, -d/2.0)\n",
    "  \n",
    "  gmms = [[\n",
    "    [ \n",
    "      (g.mu.toArray(), s_inv) \n",
    "      for weight, g, s_inv in gaussians\n",
    "    ],\n",
    "    np.array([ \n",
    "      weight * b * np.power(np.linalg.det(g.sigma.toArray()), -0.5) \n",
    "      for weight, g, s_inv in gaussians\n",
    "    ])\n",
    "  ]]\n",
    "  \n",
    "  col_stats = Statistics.colStats(data)\n",
    "  col_means = col_stats.mean()\n",
    "  col_variances = col_stats.variance()\n",
    "  marginal_b = np.power(2*pi, -(d-1)/2.0)\n",
    "  marginal_gmms = [\n",
    "    [\n",
    "      [\n",
    "        (np.delete(g.mu.toArray(), i),  np.linalg.inv(remove_dim(g.sigma.toArray(), i)))\n",
    "        for weight, g, s_inv in gaussians\n",
    "      ],\n",
    "      [\n",
    "        weight * marginal_b * np.power(np.linalg.det(remove_dim(g.sigma.toArray(), i)), -0.5)\n",
    "        for weight, g, s_inv in gaussians\n",
    "      ],\n",
    "      col_means[i],\n",
    "      col_variances[i],\n",
    "      np.log(col_variances[i])/2 + np.log(2*pi)/2\n",
    "    ]\n",
    "    for i in range(d)\n",
    "  ]\n",
    "\n",
    "  def f(v):\n",
    "    x = v.toArray()\n",
    "    results = []\n",
    "    for gmm, coeffs in gmms:\n",
    "      exponents = []\n",
    "      for mean, cov_inv in gmm:\n",
    "        diff = x - mean\n",
    "        exponents.append(-0.5 * np.matmul(np.matmul(diff, cov_inv), diff))\n",
    "      results.append(logsumexp(np.array(exponents), b=coeffs))\n",
    "    for index, (gmm, coeffs, sample_mean, sample_variance, const) in enumerate(marginal_gmms):\n",
    "      marginal_x = np.delete(x, index)\n",
    "      exponents = []\n",
    "      for mean, cov_inv in gmm:\n",
    "        diff = marginal_x - mean\n",
    "        exponents.append(-0.5 * np.matmul(np.matmul(diff, cov_inv), diff))\n",
    "      marginal_ll = logsumexp(np.array(exponents), b=coeffs)\n",
    "      null_ll = -((x[index] - sample_mean)**2)/(2 * sample_variance) - const\n",
    "      results.append(marginal_ll + null_ll)\n",
    "    return np.array(results)\n",
    "  \n",
    "  sums = data.map(f).sum()\n",
    "  ll = sums[0]\n",
    "  p_values = [ chi2.sf(-2*(x - ll), df=d-1) for x in sums[1:] ]\n",
    "  \n",
    "  return ll, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianMixture.train(train, 3, maxIterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood(model, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train\n",
    "weights = model.weights\n",
    "gaussians = [ x for x in zip(weights, model.gaussians) ]\n",
    "d = gaussians[0][1].mu.toArray().shape[0]\n",
    "b = np.power(2*pi, -d/2.0)\n",
    "gmms = [[\n",
    "  [ \n",
    "    (g.mu.toArray(), pseudo_inverse(g.sigma.toArray())) \n",
    "    for weight, g in gaussians\n",
    "  ],\n",
    "  np.array([ \n",
    "    weight * b * np.power(np.linalg.det(g.sigma.toArray()), -0.5) \n",
    "    for weight, g in gaussians\n",
    "  ])\n",
    "]]\n",
    "col_stats = Statistics.colStats(data)\n",
    "col_means = col_stats.mean()\n",
    "col_variances = col_stats.variance()\n",
    "marginal_b = np.power(2*pi, -(d-1)/2.0)\n",
    "marginal_gmms = [\n",
    "  [\n",
    "    [\n",
    "      (np.delete(g.mu.toArray(), i),  pseudo_inverse(remove_dim(g.sigma.toArray(), i)))\n",
    "      for weight, g in gaussians\n",
    "    ],\n",
    "    [\n",
    "      weight * marginal_b * np.power(np.linalg.det(remove_dim(g.sigma.toArray(), i)), -0.5)\n",
    "      for weight, g in gaussians\n",
    "    ],\n",
    "    col_means[i],\n",
    "    col_variances[i],\n",
    "    np.log(col_variances[i])/2 + np.log(2*pi)/2\n",
    "  ]\n",
    "  for i in range(d)\n",
    "]\n",
    "def f(v):\n",
    "  x = v.toArray()\n",
    "  results = []\n",
    "  for gmm, coeffs in gmms:\n",
    "    exponents = []\n",
    "    for mean, cov_inv in gmm:\n",
    "      diff = x - mean\n",
    "      exponents.append(-0.5 * np.matmul(np.matmul(diff, cov_inv), diff))\n",
    "    results.append(logsumexp(np.array(exponents), b=coeffs))\n",
    "  for index, (gmm, coeffs, sample_mean, sample_variance, const) in enumerate(marginal_gmms):\n",
    "    marginal_x = np.delete(x, index)\n",
    "    exponents = []\n",
    "    for mean, cov_inv in gmm:\n",
    "      diff = marginal_x - mean\n",
    "      exponents.append(-0.5 * np.matmul(np.matmul(diff, cov_inv), diff))\n",
    "    marginal_ll = logsumexp(np.array(exponents), b=coeffs)\n",
    "    null_ll = -((x[index] - sample_mean)**2)/(2 * sample_variance) - const\n",
    "    results.append(marginal_ll + null_ll)\n",
    "  return np.array(results)\n",
    "# sums = data.map(f).sum()\n",
    "# ll = sums[0]\n",
    "# p_values = [ chi2.sf(-2*(x - ll), df=d-1) for x in sums[1:] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power([ np.linalg.det(remove_dim(g.sigma.toArray(), i)) for i in range(d) ], -0.5) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "  n = i + 2\n",
    "  m = KMeans.train(train, n)\n",
    "  r = Row(\n",
    "    n=n,\n",
    "    cost=(m.computeCost(train)/train_count), \n",
    "    centers=m.clusterCenters\n",
    "  )\n",
    "  results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(map(lambda r: Row(n=r.n, cost=r.cost), results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results[2].centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "centers = results[2].centers\n",
    "\n",
    "for center in centers:\n",
    "  print 'revenue_percentile: ', center[0]\n",
    "  for field, value, mean, var in zip(chosen_fields, center[1:], means, variances):\n",
    "    if abs(value) > 1:\n",
    "      print field, ':', value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here - Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def field_func(value):\n",
    "  return value\n",
    "\n",
    "field_udf = F.udf(field_func, T.IntegerType())\n",
    "\n",
    "boolean_revenue_df = base_df.filter(base_df['ibe8614_8614'].isin('20173', '20174')).select(\n",
    "  calc_revenue_decile_udf(base_df.revenue).alias('decile'),\n",
    "  *[\n",
    "    field_udf(base_df[field]).alias(field)\n",
    "    for field in boolean_fields_int_encoded\n",
    "  ]\n",
    ")\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "boolean_revenue = boolean_revenue_df.rdd\\\n",
    ".map(lambda r: LabeledPoint(r.decile, Vectors.dense([\n",
    "    r[field]\n",
    "    for field in boolean_fields_int_encoded\n",
    "  ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(boolean_revenue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "chi = Statistics.chiSqTest(boolean_revenue)\n",
    "\n",
    "[i.pValue for i in chi] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "totals = boolean_revenue_df.agg(\n",
    "  F.count('*'),\n",
    "  *[F.sum(field) for field in boolean_fields_single_char_encoded]\n",
    ")\n",
    "\n",
    "# display(totals)\n",
    "\n",
    "pops = array(totals.collect()[0])\n",
    "pop_ratios = pops[1:]/float(pops[0])\n",
    "print(pops)\n",
    "print(pop_ratios)\n",
    "\n",
    "expected_by_decile = boolean_revenue_df.groupBy('decile').agg(\n",
    "  F.count('*').alias('total'),\n",
    "  *[(F.count('*') * pop_ratios[index]).alias(field) for index, field in enumerate(boolean_fields_single_char_encoded)]\n",
    ")\n",
    "\n",
    "observed_by_decile = boolean_revenue_df.groupBy('decile').agg(\n",
    "  F.count('*').alias('total'),\n",
    "  *[F.sum(field).alias(field) for index, field in enumerate(boolean_fields_single_char_encoded)]\n",
    ")\n",
    "\n",
    "diff_df = expected_by_decile.join(observed_by_decile, 'decile').select(\n",
    "  expected_by_decile.decile,\n",
    "  expected_by_decile.total,\n",
    "  *[((observed_by_decile[index+2] - expected_by_decile[index+2]) / float(pops[index+1])).alias(\"{}_increment\".format(field)) \n",
    "    for index, field in enumerate(boolean_fields_single_char_encoded)]\n",
    ")\n",
    "\n",
    "display(diff_df.orderBy('decile'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here - Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = categorical_fields[14]\n",
    "print(\"field: \" + field)\n",
    "\n",
    "categorical_values = sorted(categorical_field_values[field])\n",
    "print(\"values: \", categorical_values)\n",
    "\n",
    "if categorical_values[0] is not None:\n",
    "  categorical_values = [None] + categorical_values\n",
    "\n",
    "select_clause = [calc_revenue_decile_udf(base_df.revenue).alias('decile'), base_df[field].isNull().cast(T.IntegerType()).alias('is_None')]\n",
    "  \n",
    "select_clause += [\n",
    "  (base_df[field].isNotNull() & (base_df[field] == value)).cast(T.IntegerType()).alias(\"is_{}\".format(value)) \n",
    "  for value in categorical_values[1:]\n",
    "]\n",
    "\n",
    "# pivoted_df = base_df.filter(base_df[field].isNotNull()).select(*select_clause)\n",
    "pivoted_df = base_df.filter(base_df['ibe8614_8614'].isin('20173', '20174')).select(*select_clause)\n",
    "# pivoted_df = base_df.filter(base_df['ibe8614_8614'].isin('20173', '20174')).filter(base_df[field].isNotNull()).select(*select_clause)\n",
    "# pivoted_df = base_df.select(*select_clause)\n",
    "\n",
    "# display(pivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labeled_point(row):\n",
    "  return LabeledPoint(\n",
    "    row.decile, \n",
    "    Vectors.dense([row[\"is_{}\".format(value)] for value in categorical_values])\n",
    "  )\n",
    "\n",
    "categorical_revenue = pivoted_df.rdd.map(generate_labeled_point)\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "chi = Statistics.chiSqTest(categorical_revenue)\n",
    "\n",
    "print([i.pValue for i in chi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "totals = pivoted_df.agg(\n",
    "  F.count('*'),\n",
    "  *[F.sum(\"is_{}\".format(value)) for value in categorical_values]\n",
    ")\n",
    "\n",
    "# display(totals)\n",
    "\n",
    "pops = array(totals.collect()[0])\n",
    "pop_ratios = pops[1:]/float(pops[0])\n",
    "print(pops)\n",
    "print(pop_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_by_decile = pivoted_df.groupBy('decile').agg(\n",
    "  F.count('*').alias('total'),\n",
    "  *[(F.count('*') * pop_ratios[index]).alias(\"is_{}\".format(value)) for index, value in enumerate(categorical_values)]\n",
    ")\n",
    "\n",
    "# display(expected_by_decile.orderBy('decile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_by_decile = pivoted_df.groupBy('decile').agg(\n",
    "  F.count('*').alias('total'),\n",
    "  *[F.sum(\"is_{}\".format(value)).alias(\"is_{}\".format(value)) for index, value in enumerate(categorical_values)]\n",
    ")\n",
    "\n",
    "# display(observed_by_decile.orderBy('decile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = expected_by_decile.join(observed_by_decile, 'decile').select(\n",
    "  expected_by_decile.decile,\n",
    "  expected_by_decile.total,\n",
    "  *[((observed_by_decile[index+2] - expected_by_decile[index+2]) / float(pops[index+1])).alias(\"is_{}_increment\".format(value)) \n",
    "    for index, value in enumerate(categorical_values)]\n",
    ")\n",
    "\n",
    "display(diff_df.orderBy('decile'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#End Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labeled_point(row):\n",
    "  revenue = row.revenue\n",
    "  decile = 9\n",
    "  for i, d in enumerate(deciles):\n",
    "    if revenue < d:\n",
    "      decile = i\n",
    "      break\n",
    "  \n",
    "  return LabeledPoint(\n",
    "    decile, \n",
    "    Vectors.dense([categorical_field_values[field].index(row[field]) for field in categorical_fields])\n",
    "  )\n",
    "\n",
    "categorical_revenue = base_df.rdd.map(generate_labeled_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_revenue.take(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "chi = Statistics.chiSqTest(categorical_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.pValue for i in chi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "name": "axciom_exploration",
  "notebookId": 1135343.0
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
