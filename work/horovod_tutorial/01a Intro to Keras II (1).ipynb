{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Intro to Neural Networks with Keras II\n\nIn this notebook, we will build upon the concepts introduce the in previous lab to build a neural network that is more powerful than a simple linear regression model!\n\nWe will use the California Housing Dataset.\n\nObjectives:\n   * Activation Functions\n   * Loss functions\n   * Optimizer\n   * Batch Size"],"metadata":{}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nnp.random.seed(0)\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                        cal_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nprint(cal_housing.DESCR)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["# Recall from Last Lab\n![Life Cycle](https://brookewenig.github.io/img/DL/Life-Cycle-for-Neural-Network-Models-in-Keras.png)"],"metadata":{}},{"cell_type":"markdown","source":["## Define a Network\n\nLet's not just reinvent linear regression. Let's build a model, but with multiple layers using the [Sequential model](https://keras.io/getting-started/sequential-model-guide/) from Keras.\n\n![](https://brookewenig.github.io/img/DL/NN-Regression.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Activation Function\n\nIf we keep the activation as linear, then we aren't utilizing the power of neural networks!! The power of neural networks derives from the non-linear combinations of linear functions.\n\n**RECAP:** So what are our options for [activation functions](http://cs231n.github.io/neural-networks-1/#actfun)?"],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(20, input_dim=8, activation='relu')) \n\n# Automatically infers the input_dim based on the layer before it\nmodel.add(Dense(20, activation='relu')) \n\n# Output layer\nmodel.add(Dense(1, activation='linear')) "],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### Alternative Keras Model Syntax"],"metadata":{}},{"cell_type":"code","source":["def build_model():\n  return Sequential([Dense(20, input_dim=8, activation='relu'),\n                    Dense(20, activation='relu'),\n                    Dense(1, activation='linear')]) # Keep the last layer as linear because this is a regression problem"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["We can check the model definition by calling `.summary()`"],"metadata":{}},{"cell_type":"code","source":["model = build_model()\nmodel.summary()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## 2. Loss Functions + Metrics\n\nIn Keras, the *loss function* is the function for our optimizer to minimize. *[Metrics](https://keras.io/metrics/)* are similar to a loss function, except that the results from evaluating a metric are not used when training the model.\n\n**Recap:** Which loss functions should we use for regression? Classification?"],"metadata":{}},{"cell_type":"code","source":["from keras import metrics\nfrom keras import losses\n\nloss = \"mse\" # Or loss = losses.mse\nmetrics = [\"mae\", \"mse\"] # Or metrics = [metrics.mae, metrics.mse]\n\nmodel.compile(optimizer=\"sgd\", loss=loss, metrics=metrics)\nmodel.fit(X_train, y_train, epochs=10)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## 3. Optimizer\n\nWOW! We got a lot of NANs! Let's try this again, but using the Adam optimizer. There are a lot of optimizers out there, and here is a [great blog post](http://ruder.io/optimizing-gradient-descent/) illustrating the various optimizers.\n\nWhen in doubt, the Adam optimizer does a very good job. If you want to adjust any of the hyperparameters, you will need to import the optimizer from `optimizers` instead of passing in the name as a string."],"metadata":{}},{"cell_type":"code","source":["# Configure custom optimizer: \nfrom keras import optimizers\n\nmodel = build_model()\noptimizer=optimizers.Adam(lr=0.001)\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nhistory = model.fit(X_train, y_train, epochs=20)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\ndef viewModelLoss():\n  plt.clf()\n  plt.plot(history.history['loss'])\n  plt.title('model loss')\n  plt.ylabel('loss')\n  plt.xlabel('epoch')\n  display(plt.show())\nviewModelLoss()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## 4. Batch Size\n\nLet's set our `batch_size` (how much data to be processed simultaneously by the model) to 64, and increase our `epochs` to 20. Mini-batches are often a power of 2, to facilitate memory allocation on GPU (typically between 8 and 128).\n\n\nAlso, if you don't want to see all of the intermediate values print out, you can set the `verbose` parameter: 0 = silent, 1 = progress bar, 2 = one line per epoch (defaults to 1)"],"metadata":{}},{"cell_type":"code","source":["model = build_model()\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=2)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Change the batch size to 32, and notice how choppy it is!"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"01a Intro to Keras II","notebookId":1256376},"nbformat":4,"nbformat_minor":0}
