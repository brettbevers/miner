{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Intro to Neural Networks with Keras II\n\nCongrats on building your first neural network! In this notebook, we will cover even more topics to improve your model building. After you learn the concepts here, you will apply them to the neural network you just created.\n\nWe will use the California Housing Dataset.\n\nObjectives:\n   * Data Normalization\n   * Custom Metrics\n   * Validation data\n   * Checkpointing/callbacks\n   * Saving Models"],"metadata":{}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nnp.random.seed(0)\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                        cal_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nprint(cal_housing.DESCR)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Let's take a look at the distribution of our features."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\nxTrainDF = pd.DataFrame(X_train, columns=cal_housing.feature_names)\n\nprint(xTrainDF.describe())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## 1. Data Normalization\n\nBecause our features are all on different scales, it's going to be more difficult for our neural network during training. Let's do feature-wise normalization.\n\nWe are going to use the [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from Sklearn, which will remove the mean (zero-mean) and scale to unit variance.\n\n$$x' = \\frac{x - \\bar{x}}{\\sigma}$$"],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["# Keras Model\n![Life Cycle](https://brookewenig.github.io/img/DL/Life-Cycle-for-Neural-Network-Models-in-Keras.png)"],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential([\n  Dense(20, input_dim=8, activation='relu'),\n  Dense(20, activation='relu'),\n  Dense(1, activation='linear')\n])"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## 2. Custom Metrics\n\nUp until this point, we used MSE as our loss function and metric of choice. But what if we want to use RMSE?"],"metadata":{}},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"rmse\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Looks like we can't use it in our loss function. What about the metrics we print out during the evaluation?"],"metadata":{}},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"rmse\"])"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Luckily, Keras allows you to define custom metrics. So, you might implement RMSE as below."],"metadata":{}},{"cell_type":"code","source":["from keras import backend\n \ndef rmse(y_true, y_pred):\n\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\", rmse])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## 3. Validation Data\n\nLet's take a look at the [.fit()](https://keras.io/models/sequential/) method in the docs to see all of the options we have available! \n\nWe can either explicitly specify a validation dataset, or we can specify a fraction of our training data to be used as our validation dataset.\n\nThe reason why we need a validation dataeset is to evaluate how well we are performing on unseen data (neural networks will overfit if you train them for too long!).\n\nWe can specify `validation_split` to be any value between 0.0 and 1.0 (defaults to 0.0)."],"metadata":{}},{"cell_type":"code","source":["history = model.fit(X_train, y_train, validation_split=.2, epochs=10, verbose=2)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Wow! Look at how much lower our loss is to start, and that it is able to converge more quickly thanks to the data normalization!!\n\nBut, let's test: Is that RMSE correct?"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n\nnp.sqrt(history.history['mean_squared_error'][-1]) # Get MSE of last training epoch"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#### Gotcha!! \n\nBecause Keras computes the loss batch by batch, if we take the square root of the total MSE, it does not yield the same result as this RMSE function.\n\nYou can see Francois Challot's [comment](https://github.com/keras-team/keras/issues/1170) on this issue, recommending to stick with MSE. But for teaching purposes, now you see how to wrtie custom metric functions!"],"metadata":{}},{"cell_type":"markdown","source":["## 4. Checkpointing\n\nAfter each epoch, we want to save the model. However, we will pass in the flag `save_best_only=True`, which will only save the model if the validation loss decreased. This way, if our machine crashes or we start to overfit, we can always go back to the \"good\" state of the model.\n\nTo accomplish this, we will use the ModelCheckpoint [callback](https://keras.io/callbacks/). History is an example of a callback that is automatically applied to every Keras model."],"metadata":{}},{"cell_type":"code","source":["from keras.callbacks import ModelCheckpoint\n\nfilepath = '/tmp/02Keras_checkpoint_weights.hdf5'\ncheckpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n\nhistory = model.fit(X_train, y_train, validation_split=.2, epochs=10, verbose=2, callbacks=[checkpointer])"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## 5. Save Model/Load Model\n\nWhenever you train neural networks, you want to save them. This way, you can reuse them later! With the checkpointing agove, we were saving the model weights. Let's try to load them into a new model."],"metadata":{}},{"cell_type":"code","source":["newModel = Sequential()\n\nnewModel.load_weights(filepath)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["We just saved our model weights with the checkpointing above. However, we also need the model configuration if we want to load the weights into a new model object."],"metadata":{}},{"cell_type":"code","source":["from keras.models import model_from_yaml\n\nyaml_string = model.to_yaml() # Returns a representation of the model as a YAML string (only model architecture, not weights)\nnewModel = model_from_yaml(yaml_string)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Check that the model architecture is the same."],"metadata":{}},{"cell_type":"code","source":["newModel.summary()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Now we can load in the weights for this model architecture."],"metadata":{}},{"cell_type":"code","source":["newModel.load_weights(filepath)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Let's train it for one more epoch (we need to recompile), and then save those weights."],"metadata":{}},{"cell_type":"code","source":["newModel.compile(optimizer=\"adam\", loss=\"mse\")\nnewModel.fit(X_train, y_train, validation_split=.2, epochs=1, verbose=2)\nnewModel.save_weights(filepath)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Now it's your turn to try out these techniques on the Boston Housing Dataset!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"02 Keras","notebookId":1256340},"nbformat":4,"nbformat_minor":0}
