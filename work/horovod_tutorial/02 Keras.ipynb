{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Intro to Neural Networks with Keras II\n\nCongrats on building your first neural network! In this notebook, we will cover even more topics to improve your model building. After you learn the concepts here, you will apply them to the neural network you just created.\n\nWe will use the California Housing Dataset.\n\nObjectives:\n   * Data Normalization\n   * Custom Metrics\n   * Validation data\n   * Checkpointing/callbacks\n   * Saving Models"],"metadata":{}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nnp.random.seed(0)\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                        cal_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nprint(cal_housing.DESCR)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">California housing dataset.\n\nThe original database is available from StatLib\n\n    http://lib.stat.cmu.edu/\n\nThe data contains 20,640 observations on 9 variables.\n\nThis dataset contains the average house value as target variable\nand the following input variables (features): average income,\nhousing average age, average rooms, average bedrooms, population,\naverage occupation, latitude, and longitude in that order.\n\nReferences\n----------\n\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\nStatistics and Probability Letters, 33 (1997) 291-297.\n\n\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["Let's take a look at the distribution of our features."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\nxTrainDF = pd.DataFrame(X_train, columns=cal_housing.feature_names)\n\n#print(xTrainDF.describe())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\ncount  16512.000000  16512.000000  16512.000000  16512.000000  16512.000000   \nmean       3.876149     28.604469      5.441114      1.099598   1425.257146   \nstd        1.891584     12.586046      2.613727      0.507173   1123.756792   \nmin        0.499900      1.000000      0.846154      0.333333      3.000000   \n25%        2.572050     18.000000      4.439906      1.006260    786.000000   \n50%        3.544550     29.000000      5.226528      1.048797   1164.000000   \n75%        4.750000     37.000000      6.057778      1.099574   1723.000000   \nmax       15.000100     52.000000    141.909091     34.066667  35682.000000   \n\n           AveOccup      Latitude     Longitude  \ncount  16512.000000  16512.000000  16512.000000  \nmean       3.094971     35.632194   -119.574288  \nstd       11.597402      2.137087      2.007578  \nmin        0.750000     32.540000   -124.300000  \n25%        2.427283     33.930000   -121.810000  \n50%        2.813449     34.260000   -118.490000  \n75%        3.273834     37.710000   -118.010000  \nmax     1243.333333     41.950000   -114.310000  \n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## 1. Data Normalization\n\nBecause our features are all on different scales, it's going to be more difficult for our neural network during training. Let's do feature-wise normalization.\n\nWe are going to use the [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from Sklearn, which will remove the mean (zero-mean) and scale to unit variance.\n\n$$x' = \\frac{x - \\bar{x}}{\\sigma}$$"],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["# Keras Model\n![Life Cycle](https://brookewenig.github.io/img/DL/Life-Cycle-for-Neural-Network-Models-in-Keras.png)"],"metadata":{}},{"cell_type":"code","source":["X_train.shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>(16512, 8)\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["import tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential([\n  Dense(20, input_dim=8, activation='relu'),\n  Dense(20, activation='relu'),\n  Dense(1, activation='linear')\n])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from &#96;float&#96; to &#96;np.floating&#96; is deprecated. In future, it will be treated as &#96;np.float64 == np.dtype(float).type&#96;.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["## 2. Custom Metrics\n\nUp until this point, we used MSE as our loss function and metric of choice. But what if we want to use RMSE?"],"metadata":{}},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"rmse\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4254278255566291&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>model<span class=\"ansiyellow\">.</span>compile<span class=\"ansiyellow\">(</span>optimizer<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;adam&quot;</span><span class=\"ansiyellow\">,</span> loss<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;rmse&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/models.py</span> in <span class=\"ansicyan\">compile</span><span class=\"ansiblue\">(self, optimizer, loss, metrics, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)</span>\n<span class=\"ansigreen\">    822</span>                            weighted_metrics<span class=\"ansiyellow\">=</span>weighted_metrics<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    823</span>                            target_tensors<span class=\"ansiyellow\">=</span>target_tensors<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 824</span><span class=\"ansiyellow\">                            **kwargs)\n</span><span class=\"ansigreen\">    825</span>         self<span class=\"ansiyellow\">.</span>optimizer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>model<span class=\"ansiyellow\">.</span>optimizer<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    826</span>         self<span class=\"ansiyellow\">.</span>loss <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>model<span class=\"ansiyellow\">.</span>loss<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/engine/training.py</span> in <span class=\"ansicyan\">compile</span><span class=\"ansiblue\">(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)</span>\n<span class=\"ansigreen\">    634</span>             loss_functions <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>losses<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>l<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> l <span class=\"ansigreen\">in</span> loss<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    635</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 636</span><span class=\"ansiyellow\">             </span>loss_function <span class=\"ansiyellow\">=</span> losses<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>loss<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    637</span>             loss_functions <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>loss_function <span class=\"ansigreen\">for</span> _ <span class=\"ansigreen\">in</span> range<span class=\"ansiyellow\">(</span>len<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>outputs<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    638</span>         self<span class=\"ansiyellow\">.</span>loss_functions <span class=\"ansiyellow\">=</span> loss_functions<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/losses.py</span> in <span class=\"ansicyan\">get</span><span class=\"ansiblue\">(identifier)</span>\n<span class=\"ansigreen\">    120</span>     <span class=\"ansigreen\">if</span> isinstance<span class=\"ansiyellow\">(</span>identifier<span class=\"ansiyellow\">,</span> six<span class=\"ansiyellow\">.</span>string_types<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    121</span>         identifier <span class=\"ansiyellow\">=</span> str<span class=\"ansiyellow\">(</span>identifier<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 122</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> deserialize<span class=\"ansiyellow\">(</span>identifier<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    123</span>     <span class=\"ansigreen\">if</span> isinstance<span class=\"ansiyellow\">(</span>identifier<span class=\"ansiyellow\">,</span> dict<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    124</span>         <span class=\"ansigreen\">return</span> deserialize<span class=\"ansiyellow\">(</span>identifier<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/losses.py</span> in <span class=\"ansicyan\">deserialize</span><span class=\"ansiblue\">(name, custom_objects)</span>\n<span class=\"ansigreen\">    112</span>                                     module_objects<span class=\"ansiyellow\">=</span>globals<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    113</span>                                     custom_objects<span class=\"ansiyellow\">=</span>custom_objects<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 114</span><span class=\"ansiyellow\">                                     printable_module_name=&apos;loss function&apos;)\n</span><span class=\"ansigreen\">    115</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    116</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/utils/generic_utils.py</span> in <span class=\"ansicyan\">deserialize_keras_object</span><span class=\"ansiblue\">(identifier, module_objects, custom_objects, printable_module_name)</span>\n<span class=\"ansigreen\">    162</span>             <span class=\"ansigreen\">if</span> fn <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    163</span>                 raise ValueError(&apos;Unknown &apos; + printable_module_name +\n<span class=\"ansigreen\">--&gt; 164</span><span class=\"ansiyellow\">                                  &apos;:&apos; + function_name)\n</span><span class=\"ansigreen\">    165</span>         <span class=\"ansigreen\">return</span> fn<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    166</span>     <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ValueError</span>: Unknown loss function:rmse</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Looks like we can't use it in our loss function. What about the metrics we print out during the evaluation?"],"metadata":{}},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"rmse\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4254278255566293&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>model<span class=\"ansiyellow\">.</span>compile<span class=\"ansiyellow\">(</span>optimizer<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;adam&quot;</span><span class=\"ansiyellow\">,</span> loss<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;mse&quot;</span><span class=\"ansiyellow\">,</span> metrics<span class=\"ansiyellow\">=</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&quot;rmse&quot;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/models.py</span> in <span class=\"ansicyan\">compile</span><span class=\"ansiblue\">(self, optimizer, loss, metrics, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)</span>\n<span class=\"ansigreen\">    822</span>                            weighted_metrics<span class=\"ansiyellow\">=</span>weighted_metrics<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    823</span>                            target_tensors<span class=\"ansiyellow\">=</span>target_tensors<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 824</span><span class=\"ansiyellow\">                            **kwargs)\n</span><span class=\"ansigreen\">    825</span>         self<span class=\"ansiyellow\">.</span>optimizer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>model<span class=\"ansiyellow\">.</span>optimizer<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    826</span>         self<span class=\"ansiyellow\">.</span>loss <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>model<span class=\"ansiyellow\">.</span>loss<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/engine/training.py</span> in <span class=\"ansicyan\">compile</span><span class=\"ansiblue\">(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)</span>\n<span class=\"ansigreen\">    932</span>                             self<span class=\"ansiyellow\">.</span>metrics_updates <span class=\"ansiyellow\">+=</span> metric_fn<span class=\"ansiyellow\">.</span>updates<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    933</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 934</span><span class=\"ansiyellow\">                 </span>handle_metrics<span class=\"ansiyellow\">(</span>output_metrics<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    935</span>                 handle_metrics<span class=\"ansiyellow\">(</span>output_weighted_metrics<span class=\"ansiyellow\">,</span> weights<span class=\"ansiyellow\">=</span>weights<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    936</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/engine/training.py</span> in <span class=\"ansicyan\">handle_metrics</span><span class=\"ansiblue\">(metrics, weights)</span>\n<span class=\"ansigreen\">    899</span>                             metric_name <span class=\"ansiyellow\">=</span> metric_name_prefix <span class=\"ansiyellow\">+</span> suffix<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    900</span>                         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 901</span><span class=\"ansiyellow\">                             </span>metric_fn <span class=\"ansiyellow\">=</span> metrics_module<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>metric<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    902</span>                             weighted_metric_fn <span class=\"ansiyellow\">=</span> _weighted_masked_objective<span class=\"ansiyellow\">(</span>metric_fn<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    903</span>                             <span class=\"ansired\"># Get metric name as string</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/metrics.py</span> in <span class=\"ansicyan\">get</span><span class=\"ansiblue\">(identifier)</span>\n<span class=\"ansigreen\">     73</span>         <span class=\"ansigreen\">return</span> deserialize<span class=\"ansiyellow\">(</span>config<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     74</span>     <span class=\"ansigreen\">elif</span> isinstance<span class=\"ansiyellow\">(</span>identifier<span class=\"ansiyellow\">,</span> six<span class=\"ansiyellow\">.</span>string_types<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 75</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> deserialize<span class=\"ansiyellow\">(</span>str<span class=\"ansiyellow\">(</span>identifier<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     76</span>     <span class=\"ansigreen\">elif</span> callable<span class=\"ansiyellow\">(</span>identifier<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     77</span>         <span class=\"ansigreen\">return</span> identifier<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/metrics.py</span> in <span class=\"ansicyan\">deserialize</span><span class=\"ansiblue\">(config, custom_objects)</span>\n<span class=\"ansigreen\">     65</span>                                     module_objects<span class=\"ansiyellow\">=</span>globals<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     66</span>                                     custom_objects<span class=\"ansiyellow\">=</span>custom_objects<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 67</span><span class=\"ansiyellow\">                                     printable_module_name=&apos;metric function&apos;)\n</span><span class=\"ansigreen\">     68</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     69</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/keras/utils/generic_utils.py</span> in <span class=\"ansicyan\">deserialize_keras_object</span><span class=\"ansiblue\">(identifier, module_objects, custom_objects, printable_module_name)</span>\n<span class=\"ansigreen\">    162</span>             <span class=\"ansigreen\">if</span> fn <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    163</span>                 raise ValueError(&apos;Unknown &apos; + printable_module_name +\n<span class=\"ansigreen\">--&gt; 164</span><span class=\"ansiyellow\">                                  &apos;:&apos; + function_name)\n</span><span class=\"ansigreen\">    165</span>         <span class=\"ansigreen\">return</span> fn<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    166</span>     <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ValueError</span>: Unknown metric function:rmse</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Luckily, Keras allows you to define custom metrics. So, you might implement RMSE as below."],"metadata":{}},{"cell_type":"code","source":["from keras import backend\n \ndef rmse(y_true, y_pred):\n\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\", rmse])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["## 3. Validation Data\n\nLet's take a look at the [.fit()](https://keras.io/models/sequential/) method in the docs to see all of the options we have available! \n\nWe can either explicitly specify a validation dataset, or we can specify a fraction of our training data to be used as our validation dataset.\n\nThe reason why we need a validation dataeset is to evaluate how well we are performing on unseen data (neural networks will overfit if you train them for too long!).\n\nWe can specify `validation_split` to be any value between 0.0 and 1.0 (defaults to 0.0)."],"metadata":{}},{"cell_type":"code","source":["history = model.fit(X_train, y_train, validation_split=.2, epochs=10, verbose=2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Train on 13209 samples, validate on 3303 samples\nEpoch 1/10\n - 1s - loss: 1.3549 - mean_squared_error: 1.3549 - rmse: 0.7829 - val_loss: 0.6020 - val_mean_squared_error: 0.6020 - val_rmse: 0.5548\nEpoch 2/10\n - 1s - loss: 0.4916 - mean_squared_error: 0.4916 - rmse: 0.5036 - val_loss: 0.4616 - val_mean_squared_error: 0.4616 - val_rmse: 0.4808\nEpoch 3/10\n - 1s - loss: 0.4132 - mean_squared_error: 0.4132 - rmse: 0.4637 - val_loss: 0.4240 - val_mean_squared_error: 0.4240 - val_rmse: 0.4647\nEpoch 4/10\n - 1s - loss: 0.3944 - mean_squared_error: 0.3944 - rmse: 0.4504 - val_loss: 0.4037 - val_mean_squared_error: 0.4037 - val_rmse: 0.4457\nEpoch 5/10\n - 1s - loss: 0.3868 - mean_squared_error: 0.3868 - rmse: 0.4423 - val_loss: 0.3970 - val_mean_squared_error: 0.3970 - val_rmse: 0.4505\nEpoch 6/10\n - 1s - loss: 0.4010 - mean_squared_error: 0.4010 - rmse: 0.4368 - val_loss: 0.3840 - val_mean_squared_error: 0.3840 - val_rmse: 0.4379\nEpoch 7/10\n - 1s - loss: 0.3671 - mean_squared_error: 0.3671 - rmse: 0.4291 - val_loss: 0.3883 - val_mean_squared_error: 0.3883 - val_rmse: 0.4355\nEpoch 8/10\n - 1s - loss: 0.3521 - mean_squared_error: 0.3521 - rmse: 0.4245 - val_loss: 0.3755 - val_mean_squared_error: 0.3755 - val_rmse: 0.4346\nEpoch 9/10\n - 1s - loss: 0.3477 - mean_squared_error: 0.3477 - rmse: 0.4202 - val_loss: 0.3685 - val_mean_squared_error: 0.3685 - val_rmse: 0.4359\nEpoch 10/10\n - 1s - loss: 0.3458 - mean_squared_error: 0.3458 - rmse: 0.4174 - val_loss: 0.3599 - val_mean_squared_error: 0.3599 - val_rmse: 0.4269\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Wow! Look at how much lower our loss is to start, and that it is able to converge more quickly thanks to the data normalization!!\n\nBut, let's test: Is that RMSE correct?"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n\nnp.sqrt(history.history['mean_squared_error'][-1]) # Get MSE of last training epoch"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">12</span><span class=\"ansired\">]: </span>0.588017128065564\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["#### Gotcha!! \n\nBecause Keras computes the loss batch by batch, if we take the square root of the total MSE, it does not yield the same result as this RMSE function.\n\nYou can see Francois Challot's [comment](https://github.com/keras-team/keras/issues/1170) on this issue, recommending to stick with MSE. But for teaching purposes, now you see how to wrtie custom metric functions!"],"metadata":{}},{"cell_type":"markdown","source":["## 4. Checkpointing\n\nAfter each epoch, we want to save the model. However, we will pass in the flag `save_best_only=True`, which will only save the model if the validation loss decreased. This way, if our machine crashes or we start to overfit, we can always go back to the \"good\" state of the model.\n\nTo accomplish this, we will use the ModelCheckpoint [callback](https://keras.io/callbacks/). History is an example of a callback that is automatically applied to every Keras model."],"metadata":{}},{"cell_type":"code","source":["from keras.callbacks import ModelCheckpoint\n\nfilepath = '/tmp/02Keras_checkpoint_weights.hdf5'\ncheckpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n\nhistory = model.fit(X_train, y_train, validation_split=.2, epochs=10, verbose=2, callbacks=[checkpointer])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Train on 13209 samples, validate on 3303 samples\nEpoch 1/10\n - 1s - loss: 0.3363 - mean_squared_error: 0.3363 - rmse: 0.4133 - val_loss: 0.3552 - val_mean_squared_error: 0.3552 - val_rmse: 0.4163\n\nEpoch 00001: val_loss improved from inf to 0.35518, saving model to /tmp/02Keras_checkpoint_weights.hdf5\nEpoch 2/10\n - 1s - loss: 0.3303 - mean_squared_error: 0.3303 - rmse: 0.4083 - val_loss: 0.3579 - val_mean_squared_error: 0.3579 - val_rmse: 0.4301\n\nEpoch 00002: val_loss did not improve\nEpoch 3/10\n - 1s - loss: 0.3296 - mean_squared_error: 0.3296 - rmse: 0.4080 - val_loss: 0.3547 - val_mean_squared_error: 0.3547 - val_rmse: 0.4142\n\nEpoch 00003: val_loss improved from 0.35518 to 0.35470, saving model to /tmp/02Keras_checkpoint_weights.hdf5\nEpoch 4/10\n - 1s - loss: 0.3256 - mean_squared_error: 0.3256 - rmse: 0.4037 - val_loss: 0.3450 - val_mean_squared_error: 0.3450 - val_rmse: 0.4088\n\nEpoch 00004: val_loss improved from 0.35470 to 0.34496, saving model to /tmp/02Keras_checkpoint_weights.hdf5\nEpoch 5/10\n - 1s - loss: 0.3631 - mean_squared_error: 0.3631 - rmse: 0.4050 - val_loss: 0.3455 - val_mean_squared_error: 0.3455 - val_rmse: 0.4113\n\nEpoch 00005: val_loss did not improve\nEpoch 6/10\n - 1s - loss: 0.3180 - mean_squared_error: 0.3180 - rmse: 0.3983 - val_loss: 0.3399 - val_mean_squared_error: 0.3399 - val_rmse: 0.4063\n\nEpoch 00006: val_loss improved from 0.34496 to 0.33987, saving model to /tmp/02Keras_checkpoint_weights.hdf5\nEpoch 7/10\n - 1s - loss: 0.3179 - mean_squared_error: 0.3179 - rmse: 0.3968 - val_loss: 0.3435 - val_mean_squared_error: 0.3435 - val_rmse: 0.4154\n\nEpoch 00007: val_loss did not improve\nEpoch 8/10\n - 1s - loss: 0.3137 - mean_squared_error: 0.3137 - rmse: 0.3946 - val_loss: 0.3374 - val_mean_squared_error: 0.3374 - val_rmse: 0.4113\n\nEpoch 00008: val_loss improved from 0.33987 to 0.33744, saving model to /tmp/02Keras_checkpoint_weights.hdf5\nEpoch 9/10\n - 1s - loss: 0.3171 - mean_squared_error: 0.3171 - rmse: 0.3955 - val_loss: 0.3310 - val_mean_squared_error: 0.3310 - val_rmse: 0.4001\n\nEpoch 00009: val_loss improved from 0.33744 to 0.33097, saving model to /tmp/02Keras_checkpoint_weights.hdf5\nEpoch 10/10\n - 1s - loss: 0.3083 - mean_squared_error: 0.3083 - rmse: 0.3920 - val_loss: 0.3368 - val_mean_squared_error: 0.3368 - val_rmse: 0.3980\n\nEpoch 00010: val_loss did not improve\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["## 5. Save Model/Load Model\n\nWhenever you train neural networks, you want to save them. This way, you can reuse them later! With the checkpointing agove, we were saving the model weights. Let's try to load them into a new model."],"metadata":{}},{"cell_type":"code","source":["newModel = Sequential()\n\nnewModel.load_weights(filepath)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["We just saved our model weights with the checkpointing above. However, we also need the model configuration if we want to load the weights into a new model object."],"metadata":{}},{"cell_type":"code","source":["from keras.models import model_from_yaml\n\nyaml_string = model.to_yaml() # Returns a representation of the model as a YAML string (only model architecture, not weights)\nnewModel = model_from_yaml(yaml_string)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Check that the model architecture is the same."],"metadata":{}},{"cell_type":"code","source":["newModel.summary()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Now we can load in the weights for this model architecture."],"metadata":{}},{"cell_type":"code","source":["newModel.load_weights(filepath)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Let's train it for one more epoch (we need to recompile), and then save those weights."],"metadata":{}},{"cell_type":"code","source":["newModel.compile(optimizer=\"adam\", loss=\"mse\")\nnewModel.fit(X_train, y_train, validation_split=.2, epochs=1, verbose=2)\nnewModel.save_weights(filepath)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Now it's your turn to try out these techniques on the Boston Housing Dataset!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"02 Keras","notebookId":1256795},"nbformat":4,"nbformat_minor":0}
