{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Intro to Neural Networks with Keras II Lab\n\nNow we are going to take the following objectives we learned in the past lab, and apply them here!\n\nLet's load in our Boston Housing dataset."],"metadata":{}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\nimport numpy as np\nnp.random.seed(0)\n\nboston_housing = load_boston()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n                                                        boston_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nprint(boston_housing.DESCR)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## 1. Data Normalization\n\nGo ahead and standardize our training and test features. \n\nRecap: Why do we want to normalize our features? Do we use the test statistics when computing the mean/standard deviation?"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Let's use the same model architecture as in the previous lab."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=13, activation=\"relu\"))\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dense(1))\nmodel.summary()\n\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## 2. Validation Data\n\nIn the demo notebook, we showed how to use the validation_split method to split your data into training and validation dataset.\n\nKeras also allows you to specify a dedicated validation dataset.\n\nSplit your training set into 75-25 train-validation split."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nX_train_split, X_val, y_train_split, y_val = train_test_split(X_train,\n                                                              y_train,\n                                                              test_size=0.25,\n                                                              random_state=1)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## 3. Callbacks\n\nIn the demo notebook, we covered how to implement the ModelCheckpointer callback (History is automically done for us).\n\nNow, add the model checkpointing, and only save the best model. Also add a callback for EarlyStopping (if the model doesn't improve after 2 epochs, terminate training). You will need to set `patience=2` and `min_delta` to .0001.\n\nUse the [callbacks documentation](https://keras.io/callbacks/) for reference!"],"metadata":{}},{"cell_type":"code","source":["# ANSWER \nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfilepath = '/tmp/02KerasLab_checkpoint_weights.hdf5'\ndbutils.fs.rm(filepath, recurse=True)\ncheckpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\nearlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, mode='auto')"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## 4. Fit Model\n\nNow let's put everything together! Fit the model to the training and validation data, with 30 `epochs`, `batch_size` of 32, and the 2 callbacks we defined above.\n\nTake a look at the [.fit()](https://keras.io/models/sequential/) method in the docs for help."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nhistory = model.fit(X_train_split, y_train_split, validation_data=(X_val, y_val), epochs=30, batch_size=32, verbose=2, callbacks=[checkpointer, earlyStopping])"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## 5. Load Model\n\nLoad in the weights saved from this model via checkpointing to a new variable called `newModel`, and make predictions for our test data. Then compute the RMSE (and YES, I said RMSE!). See if you can do this without re-compiling the model!"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom keras.models import model_from_yaml\n\nyaml_string = model.to_yaml() # Returns a representation of the model as a YAML string (only model architecture, not weights)\nnewModel = model_from_yaml(yaml_string)\nnewModel.load_weights(filepath)\n\ny_pred = newModel.predict(X_test)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# ANSWER\n\nfrom sklearn.metrics import mean_squared_error\n\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(rmse)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Nice job with learning Keras! We will now examine a different type of neural networks: Convolutional Neural Networks in the next lab."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"02a Keras Lab II","notebookId":1256662},"nbformat":4,"nbformat_minor":0}
