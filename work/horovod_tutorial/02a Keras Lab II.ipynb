{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Intro to Neural Networks with Keras II Lab\n\nNow we are going to take the following objectives we learned in the past lab, and apply them here!\n\nLet's load in our Boston Housing dataset."],"metadata":{}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\nimport numpy as np\nnp.random.seed(0)\n\nboston_housing = load_boston()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n                                                        boston_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nprint(boston_housing.DESCR)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Boston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric/categorical predictive\n    \n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000&apos;s\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Housing\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. &apos;Hedonic\nprices and the demand for clean air&apos;, J. Environ. Economics &amp; Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, &apos;Regression diagnostics\n...&apos;, Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n**References**\n\n   - Belsley, Kuh &amp; Welsch, &apos;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&apos;, Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## 1. Data Normalization\n\nGo ahead and standardize our training and test features. \n\nRecap: Why do we want to normalize our features? Do we use the test statistics when computing the mean/standard deviation?"],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit(X_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["import pandas as pd\ndf=pd.DataFrame(X_train, columns=boston_housing.feature_names)\ndf.describe()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">19</span><span class=\"ansired\">]: </span>\n               CRIM            ZN         INDUS          CHAS           NOX  \\\ncount  4.040000e+02  4.040000e+02  4.040000e+02  4.040000e+02  4.040000e+02   \nmean   2.638154e-17 -2.858000e-17  1.514740e-15  3.187769e-17 -3.587889e-15   \nstd    1.001240e+00  1.001240e+00  1.001240e+00  1.001240e+00  1.001240e+00   \nmin   -4.015739e-01 -4.955934e-01 -1.552282e+00 -2.932942e-01 -1.439198e+00   \n25%   -3.932194e-01 -4.955934e-01 -8.607543e-01 -2.932942e-01 -8.995829e-01   \n50%   -3.765921e-01 -4.955934e-01 -2.854561e-01 -2.932942e-01 -1.544004e-01   \n75%   -1.415931e-02  3.642723e-01  1.026692e+00 -2.932942e-01  6.079127e-01   \nmax    9.343177e+00  3.803735e+00  2.436063e+00  3.409545e+00  2.723546e+00   \n\n                 RM           AGE           DIS           RAD           TAX  \\\ncount  4.040000e+02  4.040000e+02  4.040000e+02  4.040000e+02  4.040000e+02   \nmean   6.709704e-15  1.855501e-15  2.440292e-16 -9.013692e-17  7.035077e-17   \nstd    1.001240e+00  1.001240e+00  1.001240e+00  1.001240e+00  1.001240e+00   \nmin   -3.933600e+00 -2.279728e+00 -1.272916e+00 -9.770240e-01 -1.280985e+00   \n25%   -5.695181e-01 -8.653185e-01 -8.123032e-01 -6.309828e-01 -7.478768e-01   \n50%   -1.304406e-01  3.107268e-01 -2.490170e-01 -5.156357e-01 -4.398585e-01   \n75%    5.196410e-01  9.075128e-01  6.209316e-01  1.675959e+00  1.556337e+00   \nmax    3.648022e+00  1.123968e+00  3.918286e+00  1.675959e+00  1.822891e+00   \n\n            PTRATIO             B         LSTAT  \ncount  4.040000e+02  4.040000e+02  4.040000e+02  \nmean   2.162846e-14  7.949637e-15 -8.156292e-16  \nstd    1.001240e+00  1.001240e+00  1.001240e+00  \nmin   -2.694601e+00 -3.902881e+00 -1.532930e+00  \n25%   -5.600996e-01  2.071439e-01 -7.889097e-01  \n50%    2.590875e-01  3.764843e-01 -1.822471e-01  \n75%    8.129041e-01  4.266055e-01  5.999280e-01  \nmax    1.643629e+00  4.347267e-01  3.495190e+00  \n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Let's use the same model architecture as in the previous lab."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=13, activation=\"relu\"))\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dense(1))\nmodel.summary()\n\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_7 (Dense)              (None, 50)                700       \n_________________________________________________________________\ndense_8 (Dense)              (None, 20)                1020      \n_________________________________________________________________\ndense_9 (Dense)              (None, 1)                 21        \n=================================================================\nTotal params: 1,741\nTrainable params: 1,741\nNon-trainable params: 0\n_________________________________________________________________\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## 2. Validation Data\n\nIn the demo notebook, we showed how to use the validation_split method to split your data into training and validation dataset.\n\nKeras also allows you to specify a dedicated validation dataset.\n\nSplit your training set into 75-25 train-validation split."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\nval_set = (X_val, y_val)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["X_val.shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">23</span><span class=\"ansired\">]: </span>(101, 13)\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["#history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, verbose=2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Train on 303 samples, validate on 101 samples\nEpoch 1/10\n - 0s - loss: 569.9089 - mean_squared_error: 569.9089 - val_loss: 533.3938 - val_mean_squared_error: 533.3938\nEpoch 2/10\n - 0s - loss: 546.8407 - mean_squared_error: 546.8407 - val_loss: 512.1971 - val_mean_squared_error: 512.1971\nEpoch 3/10\n - 0s - loss: 522.9869 - mean_squared_error: 522.9869 - val_loss: 488.1552 - val_mean_squared_error: 488.1552\nEpoch 4/10\n - 0s - loss: 495.2826 - mean_squared_error: 495.2826 - val_loss: 460.3932 - val_mean_squared_error: 460.3932\nEpoch 5/10\n - 0s - loss: 463.4641 - mean_squared_error: 463.4641 - val_loss: 428.0783 - val_mean_squared_error: 428.0783\nEpoch 6/10\n - 0s - loss: 426.6634 - mean_squared_error: 426.6634 - val_loss: 389.4846 - val_mean_squared_error: 389.4846\nEpoch 7/10\n - 0s - loss: 382.7528 - mean_squared_error: 382.7528 - val_loss: 345.3692 - val_mean_squared_error: 345.3692\nEpoch 8/10\n - 0s - loss: 333.3818 - mean_squared_error: 333.3818 - val_loss: 297.8613 - val_mean_squared_error: 297.8613\nEpoch 9/10\n - 0s - loss: 280.9480 - mean_squared_error: 280.9480 - val_loss: 249.1338 - val_mean_squared_error: 249.1338\nEpoch 10/10\n - 0s - loss: 228.6512 - mean_squared_error: 228.6512 - val_loss: 202.4380 - val_mean_squared_error: 202.4380\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["## 3. Callbacks\n\nIn the demo notebook, we covered how to implement the ModelCheckpointer callback (History is automically done for us).\n\nNow, add the model checkpointing, and only save the best model. Also add a callback for EarlyStopping (if the model doesn't improve after 2 epochs, terminate training). You will need to set `patience=2` and `min_delta` to .0001.\n\nUse the [callbacks documentation](https://keras.io/callbacks/) for reference!"],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfilepath = '/tmp/02KerasLab_checkpoint_weights.hdf5'\ncheckpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\nearlyStopping = EarlyStopping(patience=2, min_delta=1e-4)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## 4. Fit Model\n\nNow let's put everything together! Fit the model to the training and validation data, with 30 `epochs`, `batch_size` of 32, and the 2 callbacks we defined above.\n\nTake a look at the [.fit()](https://keras.io/models/sequential/) method in the docs for help."],"metadata":{}},{"cell_type":"code","source":["# TODO\nhistory = model.fit(X_train, y_train, validation_data=val_set, epochs=30, batch_size=32, callbacks=[checkpointer, earlyStopping])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Train on 227 samples, validate on 76 samples\nEpoch 1/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 224.3950 - mean_squared_error: 224.3950\n227/227 [==============================] - 0s 78us/step - loss: 180.1372 - mean_squared_error: 180.1372 - val_loss: 183.4340 - val_mean_squared_error: 183.4340\n\nEpoch 00001: val_loss improved from inf to 183.43402, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 2/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 130.3341 - mean_squared_error: 130.3341\n227/227 [==============================] - 0s 65us/step - loss: 147.7069 - mean_squared_error: 147.7069 - val_loss: 150.1816 - val_mean_squared_error: 150.1816\n\nEpoch 00002: val_loss improved from 183.43402 to 150.18163, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 3/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 95.3276 - mean_squared_error: 95.3276\n227/227 [==============================] - 0s 66us/step - loss: 123.9462 - mean_squared_error: 123.9462 - val_loss: 123.3803 - val_mean_squared_error: 123.3803\n\nEpoch 00003: val_loss improved from 150.18163 to 123.38035, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 4/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 101.7446 - mean_squared_error: 101.7446\n227/227 [==============================] - 0s 63us/step - loss: 103.7421 - mean_squared_error: 103.7421 - val_loss: 105.7813 - val_mean_squared_error: 105.7813\n\nEpoch 00004: val_loss improved from 123.38035 to 105.78129, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 5/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 91.4769 - mean_squared_error: 91.4769\n227/227 [==============================] - 0s 64us/step - loss: 90.6744 - mean_squared_error: 90.6744 - val_loss: 93.8797 - val_mean_squared_error: 93.8797\n\nEpoch 00005: val_loss improved from 105.78129 to 93.87966, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 6/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 79.0009 - mean_squared_error: 79.0009\n227/227 [==============================] - 0s 66us/step - loss: 80.5219 - mean_squared_error: 80.5219 - val_loss: 84.4844 - val_mean_squared_error: 84.4844\n\nEpoch 00006: val_loss improved from 93.87966 to 84.48443, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 7/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 88.6834 - mean_squared_error: 88.6834\n227/227 [==============================] - 0s 63us/step - loss: 72.1509 - mean_squared_error: 72.1509 - val_loss: 76.8271 - val_mean_squared_error: 76.8271\n\nEpoch 00007: val_loss improved from 84.48443 to 76.82715, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 8/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 127.5328 - mean_squared_error: 127.5328\n227/227 [==============================] - 0s 61us/step - loss: 65.1606 - mean_squared_error: 65.1606 - val_loss: 70.5065 - val_mean_squared_error: 70.5065\n\nEpoch 00008: val_loss improved from 76.82715 to 70.50650, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 9/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 109.5135 - mean_squared_error: 109.5135\n227/227 [==============================] - 0s 92us/step - loss: 58.3331 - mean_squared_error: 58.3331 - val_loss: 65.0318 - val_mean_squared_error: 65.0318\n\nEpoch 00009: val_loss improved from 70.50650 to 65.03182, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 10/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 62.4265 - mean_squared_error: 62.4265\n227/227 [==============================] - 0s 70us/step - loss: 52.6611 - mean_squared_error: 52.6611 - val_loss: 60.0727 - val_mean_squared_error: 60.0727\n\nEpoch 00010: val_loss improved from 65.03182 to 60.07269, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 11/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 31.9556 - mean_squared_error: 31.9556\n227/227 [==============================] - 0s 64us/step - loss: 47.3245 - mean_squared_error: 47.3245 - val_loss: 55.8244 - val_mean_squared_error: 55.8244\n\nEpoch 00011: val_loss improved from 60.07269 to 55.82437, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 12/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 36.7229 - mean_squared_error: 36.7229\n227/227 [==============================] - 0s 59us/step - loss: 42.9940 - mean_squared_error: 42.9940 - val_loss: 52.3305 - val_mean_squared_error: 52.3305\n\nEpoch 00012: val_loss improved from 55.82437 to 52.33051, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 13/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 25.5132 - mean_squared_error: 25.5132\n227/227 [==============================] - 0s 62us/step - loss: 38.9285 - mean_squared_error: 38.9285 - val_loss: 49.4430 - val_mean_squared_error: 49.4430\n\nEpoch 00013: val_loss improved from 52.33051 to 49.44304, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 14/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 40.8513 - mean_squared_error: 40.8513\n227/227 [==============================] - 0s 63us/step - loss: 35.9825 - mean_squared_error: 35.9825 - val_loss: 46.9536 - val_mean_squared_error: 46.9536\n\nEpoch 00014: val_loss improved from 49.44304 to 46.95359, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 15/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 54.9260 - mean_squared_error: 54.9260\n227/227 [==============================] - 0s 67us/step - loss: 33.3489 - mean_squared_error: 33.3489 - val_loss: 44.6803 - val_mean_squared_error: 44.6803\n\nEpoch 00015: val_loss improved from 46.95359 to 44.68033, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 16/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 35.0186 - mean_squared_error: 35.0186\n227/227 [==============================] - 0s 63us/step - loss: 30.9246 - mean_squared_error: 30.9246 - val_loss: 42.2768 - val_mean_squared_error: 42.2768\n\nEpoch 00016: val_loss improved from 44.68033 to 42.27685, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 17/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 18.0894 - mean_squared_error: 18.0894\n227/227 [==============================] - 0s 63us/step - loss: 28.8326 - mean_squared_error: 28.8326 - val_loss: 40.3673 - val_mean_squared_error: 40.3673\n\nEpoch 00017: val_loss improved from 42.27685 to 40.36732, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 18/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 22.8564 - mean_squared_error: 22.8564\n227/227 [==============================] - 0s 65us/step - loss: 27.3537 - mean_squared_error: 27.3537 - val_loss: 39.1185 - val_mean_squared_error: 39.1185\n\nEpoch 00018: val_loss improved from 40.36732 to 39.11852, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 19/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 17.5064 - mean_squared_error: 17.5064\n227/227 [==============================] - 0s 64us/step - loss: 26.1631 - mean_squared_error: 26.1631 - val_loss: 38.2129 - val_mean_squared_error: 38.2129\n\nEpoch 00019: val_loss improved from 39.11852 to 38.21286, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 20/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 16.1481 - mean_squared_error: 16.1481\n227/227 [==============================] - 0s 63us/step - loss: 25.1998 - mean_squared_error: 25.1998 - val_loss: 37.4483 - val_mean_squared_error: 37.4483\n\nEpoch 00020: val_loss improved from 38.21286 to 37.44835, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 21/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 19.5819 - mean_squared_error: 19.5819\n227/227 [==============================] - 0s 64us/step - loss: 24.3588 - mean_squared_error: 24.3588 - val_loss: 36.7952 - val_mean_squared_error: 36.7952\n\nEpoch 00021: val_loss improved from 37.44835 to 36.79517, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 22/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 15.7107 - mean_squared_error: 15.7107\n227/227 [==============================] - 0s 61us/step - loss: 23.6066 - mean_squared_error: 23.6066 - val_loss: 36.1263 - val_mean_squared_error: 36.1263\n\nEpoch 00022: val_loss improved from 36.79517 to 36.12628, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 23/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 17.1683 - mean_squared_error: 17.1683\n227/227 [==============================] - 0s 63us/step - loss: 22.9595 - mean_squared_error: 22.9595 - val_loss: 35.5699 - val_mean_squared_error: 35.5699\n\nEpoch 00023: val_loss improved from 36.12628 to 35.56991, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 24/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 15.8074 - mean_squared_error: 15.8074\n227/227 [==============================] - 0s 64us/step - loss: 22.3475 - mean_squared_error: 22.3475 - val_loss: 35.2138 - val_mean_squared_error: 35.2138\n\nEpoch 00024: val_loss improved from 35.56991 to 35.21382, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 25/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 16.6967 - mean_squared_error: 16.6967\n227/227 [==============================] - 0s 64us/step - loss: 21.8082 - mean_squared_error: 21.8082 - val_loss: 34.8564 - val_mean_squared_error: 34.8564\n\nEpoch 00025: val_loss improved from 35.21382 to 34.85636, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 26/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 11.2073 - mean_squared_error: 11.2073\n227/227 [==============================] - 0s 63us/step - loss: 21.2438 - mean_squared_error: 21.2438 - val_loss: 34.9139 - val_mean_squared_error: 34.9139\n\nEpoch 00026: val_loss did not improve\nEpoch 27/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 28.9601 - mean_squared_error: 28.9601\n227/227 [==============================] - 0s 66us/step - loss: 21.1059 - mean_squared_error: 21.1059 - val_loss: 34.7008 - val_mean_squared_error: 34.7008\n\nEpoch 00027: val_loss improved from 34.85636 to 34.70078, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 28/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 25.7067 - mean_squared_error: 25.7067\n227/227 [==============================] - 0s 62us/step - loss: 20.7574 - mean_squared_error: 20.7574 - val_loss: 34.0265 - val_mean_squared_error: 34.0265\n\nEpoch 00028: val_loss improved from 34.70078 to 34.02646, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 29/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 13.7039 - mean_squared_error: 13.7039\n227/227 [==============================] - 0s 61us/step - loss: 20.1398 - mean_squared_error: 20.1398 - val_loss: 33.1834 - val_mean_squared_error: 33.1834\n\nEpoch 00029: val_loss improved from 34.02646 to 33.18339, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\nEpoch 30/30\n\n 32/227 [===&gt;..........................] - ETA: 0s - loss: 21.1116 - mean_squared_error: 21.1116\n227/227 [==============================] - 0s 61us/step - loss: 19.7615 - mean_squared_error: 19.7615 - val_loss: 32.4447 - val_mean_squared_error: 32.4447\n\nEpoch 00030: val_loss improved from 33.18339 to 32.44469, saving model to /tmp/02KerasLab_checkpoint_weights.hdf5\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["## 5. Load Model\n\nLoad in the weights saved from this model via checkpointing to a new variable called `newModel`, and make predictions for our test data. Then compute the RMSE (and YES, I said RMSE!). See if you can do this without re-compiling the model!"],"metadata":{}},{"cell_type":"code","source":["from keras import backend\n \ndef rmse(y_true, y_pred):\n\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["# TODO\nfrom keras.models import load_model\n\nnewModel = Sequential()\nnewModel = load_model(filepath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["newModel.compile(optimizer='adam', loss='mse', metrics=[rmse])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Nice job with learning Keras! We will now examine a different type of neural networks: Convolutional Neural Networks in the next lab."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"02a Keras Lab II","notebookId":1256721},"nbformat":4,"nbformat_minor":0}
