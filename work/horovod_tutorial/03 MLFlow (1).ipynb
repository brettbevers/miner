{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## MLFlow\n\nAs you might have noticed, throughout the day you tried different model architectures. But how do you remember which one worked best? That's where [MLFlow](https://mlflow.org/) comes into play!\n\n[MLFlow](https://mlflow.org/docs/latest/concepts.html) seeks to address these three core issues:\n\n* It’s difficult to keep track of experiments\n* It’s difficult to reproduce code\n* There’s no standard way to package and deploy models\n\nIn this notebook, we will show how to do experiment tracking with MLFlow on Azure Databricks! We will start with logging the metrics from the models we created with the California housing dataset today."],"metadata":{}},{"cell_type":"markdown","source":["### Install MLflow on Your Databricks Cluster\n\n1. Ensure you are using or [create a cluster](https://docs.azuredatabricks.net/user-guide/clusters/create.html#cluster-create) specifying \n  * **Databricks Runtime Version:** Databricks Runtime 4.1 (ML)\n  * **Python Version:** Python 3\n2. Add `mlflow` as a PyPi library in Databricks, and install it on your cluster\n  * Follow [Upload a Python PyPI package or Python Egg](https://docs.azuredatabricks.net/user-guide/libraries.html#upload-a-python-pypi-package-or-python-egg) to create a library\n  * Choose **PyPi** and enter `mlflow==0.5.0` (this notebook was tested with `mlflow` version 0.5.0)"],"metadata":{}},{"cell_type":"markdown","source":["### Set up a Remote MLflow Tracking Server (already done here)\n\nTo run a long-lived, shared MLflow tracking server, we'll launch a Linux VM instance to run the [MLflow Tracking server](https://mlflow.org/docs/latest/tracking.html). To do this:\n\n* Create a *Linux VM* instance\n  * Open port 5000 for MLflow server; an example of how to do this via [How to open ports to a virtual machine with the Azure portal](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/nsg-quickstart-portal). Opening up port 5000 to the Internet will allow anyone to access your server, so it is recommended to only open up the port within an [Azure VPC](https://azure.microsoft.com/en-us/services/virtual-network/) that your Databricks clusters have access to.\n  * Install conda onto your Linux instance via [Conda > Installing on Linux](https://conda.io/docs/user-guide/install/linux.html)\n\n* Run your Tracking Server\n  * Run the `server` command in MLflow passing it `--host 0.0.0.0`, e.g. `mlflow server --host 0.0.0.0`.\n    * For more information, refer to [MLflow > Running a Tracking Server](https://mlflow.org/docs/latest/tracking.html?highlight=server#running-a-tracking-server).\n  * To test connectivity of your tracking server:\n    * Get the hostname of your instance\n    * Go to http://$TRACKING_SERVER$:5000; it should look similar to this [MLflow UI](https://databricks.com/wp-content/uploads/2018/06/mlflow-web-ui.png)\n\n* **NOTE**: Currently we can only save parameters and metrics to remote tracking server"],"metadata":{}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nnp.random.seed(0)\nimport tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                        cal_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nprint(cal_housing.DESCR)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Build model architecture as before."],"metadata":{}},{"cell_type":"code","source":["from keras.models import Sequential\nfrom keras.layers import Dense\n\ndef build_model():\n  return Sequential([Dense(20, input_dim=8, activation='relu'),\n                    Dense(20, activation='relu'),\n                    Dense(1, activation='linear')]) # Keep the last layer as linear because this is a regression problem"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Start Using MLflow in a Notebook\n\nThe first step is to import call `mlflow.set_tracking_uri` to point to your server:"],"metadata":{}},{"cell_type":"code","source":["# Set this variable to your MLflow server's DNS name\nmlflow_server = '40.118.203.191'\n\n# Tracking URI\nmlflow_tracking_URI = 'http://' + mlflow_server + ':5000'\nprint (\"MLflow Tracking URI: {}\".format(mlflow_tracking_URI))\n\n# Import MLflow and set the Tracking UI\nimport mlflow\nmlflow.set_tracking_uri(mlflow_tracking_URI)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Track experiments!"],"metadata":{}},{"cell_type":"code","source":["# Note issue with **kwargs https://github.com/keras-team/keras/issues/9805\n\ndef trackExperiments(model, compile_kwargs, fit_kwargs, optional_params={}):\n  '''\n  This is a wrapper function for tracking expirements with MLFlow\n    \n  Parameters\n  ----------\n  model: Keras model\n    The model to track\n    \n  compile_kwargs: dict\n    Keyword arguments to compile model with\n  \n  fit_kwargs: dict\n    Keyword arguments to fit model with\n  '''\n  with mlflow.start_run():\n    model = model()\n    model.compile(**compile_kwargs)\n    history = model.fit(**fit_kwargs)\n    \n    for param_key, param_value in {**compile_kwargs, **fit_kwargs, **optional_params}.items():\n      if param_key not in [\"x\", \"y\", \"X_val\", \"y_val\"]:\n        mlflow.log_param(param_key, param_value)\n    \n    for key, values in history.history.items():\n      for v in values:\n          mlflow.log_metric(key, v)\n\n    for i, layer in enumerate(model.layers):\n      mlflow.log_param(\"hidden_layer_\" + str(i) + \"_units\", layer.output_shape)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Let's recall what happened when we used SGD."],"metadata":{}},{"cell_type":"code","source":["compile_kwargs = {\n  \"optimizer\": \"sgd\", \n  \"loss\": \"mse\",\n  \"metrics\": [\"mse\", \"mae\"],\n}\n\nfit_kwargs = {\n  \"x\": X_train, \n  \"y\": y_train,\n  \"epochs\": 10,\n  \"verbose\": 2\n}\n\ntrackExperiments(build_model, compile_kwargs, fit_kwargs)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Now let's change the optimizer"],"metadata":{}},{"cell_type":"code","source":["compile_kwargs[\"optimizer\"] = \"adam\" \n\ntrackExperiments(build_model, compile_kwargs, fit_kwargs)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Now let's add some data normalization, as well as a validation dataset."],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\nfit_kwargs[\"validation_split\"] = 0.2\nfit_kwargs[\"x\"] = X_train_scaled\n\noptional_params = {\n  \"normalize_data\": \"true\"\n}\n\ntrackExperiments(build_model, compile_kwargs, fit_kwargs, optional_params)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Review the MLflow UI\nOpen the URL of your tracking server in a web browser. In case you forgot it, you can get it from `mlflow.get_tracking_uri()`:"],"metadata":{}},{"cell_type":"code","source":["# Identify the location of the runs\nmlflow.tracking.get_tracking_uri()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["The MLflow UI should look something similar to the animated GIF below. Inside the UI, you can:\n* View your experiments and runs\n* Review the parameters and metrics on each run\n* Click each run for a detailed view to see the the model, images, and other artifacts produced.\n\n<img src=\"https://brookewenig.github.io/img/DL/mlflow-ui-azure.gif\"/>"],"metadata":{}},{"cell_type":"markdown","source":["Now, go back and add MLFlow to your experiments from the Boston Housing Dataset!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"03 MLFlow","notebookId":1256698},"nbformat":4,"nbformat_minor":0}
