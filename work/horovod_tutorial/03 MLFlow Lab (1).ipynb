{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Add MLFlow to your experiments from the Boston Housing Dataset!"],"metadata":{}},{"cell_type":"markdown","source":["### Set up a Remote MLflow Tracking Server (already done here)\n\nTo run a long-lived, shared MLflow tracking server, we'll launch a Linux VM instance to run the [MLflow Tracking server](https://mlflow.org/docs/latest/tracking.html). To do this:\n\n* Create a *Linux VM* instance\n  * Open port 5000 for MLflow server; an example of how to do this via [How to open ports to a virtual machine with the Azure portal](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/nsg-quickstart-portal). Opening up port 5000 to the Internet will allow anyone to access your server, so it is recommended to only open up the port within an [Azure VPC](https://azure.microsoft.com/en-us/services/virtual-network/) that your Databricks clusters have access to.\n  * Install conda onto your Linux instance via [Conda > Installing on Linux](https://conda.io/docs/user-guide/install/linux.html)\n\n* Run your Tracking Server\n  * Run the `server` command in MLflow passing it `--host 0.0.0.0`, e.g. `mlflow server --host 0.0.0.0`.\n    * For more information, refer to [MLflow > Running a Tracking Server](https://mlflow.org/docs/latest/tracking.html?highlight=server#running-a-tracking-server).\n  * To test connectivity of your tracking server:\n    * Get the hostname of your instance\n    * Go to http://$TRACKING_SERVER$:5000; it should look similar to this [MLflow UI](https://databricks.com/wp-content/uploads/2018/06/mlflow-web-ui.png)\n\n* **NOTE**: Currently we can only save parameters and metrics to remote tracking server"],"metadata":{}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\nimport numpy as np\nnp.random.seed(0)\n\nboston_housing = load_boston()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n                                                        boston_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nprint(boston_housing.DESCR)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Build_model\nCreate a `build_model()` function. Because Keras models are stateful, we want to get a fresh model every time we are trying out a new experiment."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nimport tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\ndef build_model():\n  return Sequential([Dense(50, input_dim=13, activation='relu'),\n                    Dense(20, activation='relu'),\n                    Dense(1, activation='linear')]) # Keep the last layer as linear because this is a regression problem"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Start Using MLflow in a Notebook\n\nThe first step is to import call `mlflow.set_tracking_uri` to point to your server:"],"metadata":{}},{"cell_type":"code","source":["# Set this variable to your MLflow server's DNS name\nmlflow_server = '40.118.203.191'\n\n# Tracking URI\nmlflow_tracking_URI = 'http://' + mlflow_server + ':5000'\nprint (\"MLflow Tracking URI: %s\" % (mlflow_tracking_URI))\n\n# Import MLflow and set the Tracking UI\nimport mlflow\nmlflow.set_tracking_uri(mlflow_tracking_URI)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Track experiments!"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\ndef trackExperiments(build_model=build_model, optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"], epochs=10, batch_size=32, validation_split=0.0, validation_data=None, verbose=2, normalize_data=False, callbacks=None):\n  with mlflow.start_run():\n    \n    model = build_model()\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    history = model.fit(X_train, y_train, epochs=epochs, verbose=verbose, validation_split=validation_split, validation_data=validation_data, callbacks=callbacks)\n\n    mlflow.log_param(\"loss\", loss)\n    mlflow.log_param(\"optimizer\", optimizer)\n    mlflow.log_param(\"epochs\", epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"validation_split\", validation_split)\n    \n    if normalize_data:\n      mlflow.log_param(\"normalize_data\", \"true\")\n    \n    for key, values in history.history.items():\n      for v in values:\n          mlflow.log_metric(key, v)\n\n    for i, layer in enumerate(model.layers):\n      mlflow.log_param(\"hidden_layer_\" + str(i) + \"_units\", layer.output_shape)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# ANSWER\ntrackExperiments(optimizer=\"adam\", loss='mse', metrics=[\"mse\"], epochs=100, batch_size = 32)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# ANSWER\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train,\n                                                  y_train,\n                                                  test_size=0.25,\n                                                  random_state=1)\n\nfilepath = '/tmp/02KerasLab_checkpoint_weights.hdf5'\ndbutils.fs.rm(filepath, recurse=True)\ncheckpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\nearlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, mode='auto')\n\ntrackExperiments(metrics=[\"mae\", \"mse\"], validation_data=(X_val, y_val), epochs=30, batch_size=32, verbose=2, callbacks=[checkpointer, earlyStopping], normalize_data=True)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Review the MLflow UI\nOpen the URL of your tracking server in a web browser. In case you forgot it, you can get it from `mlflow.get_tracking_uri()`:"],"metadata":{}},{"cell_type":"code","source":["# Identify the location of the runs\nmlflow.tracking.get_tracking_uri()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"03 MLFlow Lab","notebookId":1256681},"nbformat":4,"nbformat_minor":0}
