{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://brookewenig.github.io/img/DL/movie-camera.png\" style=\"float:right; height: 200px; margin: 10px; border: 1px solid #ddd; border-radius: 15px 15px 15px 15px; padding: 10px\"/>\n\n# Movie Recommendations\n\nIn the previous labs, we didn't need to do any data preprocessing. In this lab, we will use our preprocessing steps from Spark as input to Horovod. \n\nHere, we will use 1 million movie ratings from the [MovieLens stable benchmark rating dataset](http://grouplens.org/datasets/movielens/). We will start by building a benchmark model with ALS, and then see if we can beat that benchmark with a neural network!\n\nLet's start by mounting our dataset if it is not already there."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["moviesDF = spark.read.parquet(\"dbfs:/mnt/training/movielens/movies.parquet/\")\nratingsDF = spark.read.parquet(\"dbfs:/mnt/training/movielens/ratings.parquet/\")\n\nratingsDF.cache()\nmoviesDF.cache()\n\nratingsCount = ratingsDF.count()\nmoviesCount = moviesDF.count()\n\nprint('There are %s ratings and %s movies in the datasets' % (ratingsCount, moviesCount))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Let's take a quick look at some of the data in the two DataFrames."],"metadata":{}},{"cell_type":"code","source":["display(moviesDF)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(ratingsDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## **Part 2: Collaborative Filtering**\n\nLet's start by splitting our data into a training and test set."],"metadata":{}},{"cell_type":"code","source":["seed=42\n(trainingDF, testDF) = ratingsDF.randomSplit([0.8, 0.2], seed=seed)\n\nprint('Training: {0}, test: {1}'.format(trainingDF.count(), testDF.count()))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## ALS\n\n![factorization](http://spark-mooc.github.io/web-assets/images/matrix_factorization.png)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\n\nals = (ALS()\n       .setUserCol(\"userId\")\n       .setItemCol(\"movieId\")\n       .setRatingCol(\"rating\")\n       .setPredictionCol(\"prediction\")\n       .setMaxIter(2)\n       .setSeed(seed)\n       .setRegParam(0.1)\n       .setColdStartStrategy(\"drop\")\n       .setRank(12))\n\nalsModel = als.fit(trainingDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nregEval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"mse\")\n\npredictedTestDF = alsModel.transform(testDF)\n\ntestMse = regEval.evaluate(predictedTestDF)\n\nprint('The model had a MSE on the test set of {0}'.format(testMse))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Deep Learning"],"metadata":{}},{"cell_type":"code","source":["userFactors = alsModel.userFactors.selectExpr(\"id as userId\", \"features as uFeatures\")\nitemFactors = alsModel.itemFactors.selectExpr(\"id as movieId\", \"features as iFeatures\")\njoinedTrainDF = trainingDF.join(itemFactors, on=\"movieId\").join(userFactors, on=\"userId\")\njoinedTestDF = testDF.join(itemFactors, on=\"movieId\").join(userFactors, on=\"userId\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(joinedTrainDF)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from itertools import chain\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\ndef concat_arrays(*args):\n    return list(chain(*args))\n    \nconcat_arrays_udf = udf(concat_arrays, ArrayType(FloatType()))\n\nconcatTrainDF = (joinedTrainDF\n                 .select('userId', 'movieId', concat_arrays_udf(col(\"iFeatures\"), col(\"uFeatures\")).alias(\"features\"),\n                         col('rating').cast(\"float\")))\nconcatTestDF = (joinedTestDF\n                .select('userId', 'movieId', concat_arrays_udf(col(\"iFeatures\"), col(\"uFeatures\")).alias(\"features\"), \n                        col('rating').cast(\"float\")))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(concatTrainDF.limit(10))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Model_fn\n\nUnfortunately, Databricks' HorovodEstimator does not have a Keras API yet and our engineering team does not recommend the [Keras model to TF Estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator) because it has not been fully tested with our HorovodEstimator. Instead, you will need to write a Tensorflow model function directly.\n\nBut do not fear! All of the concepts you learned about Keras are directly applicable here (Keras is just a high level wrapper with a Tensorflow backend). You now get to experience all of the joy (and agony) of the very low-level Tensorflow APIs Google designed.\n\nOur tf.estimator-style `model_fn` ([see TensorFlow docs](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)) works by:\n1. Defining the model's network structure, then\n2. Specifying the model's output on a single batch of data during training, eval, and prediction (inference) phases.\n\n**Note**: If you have a single-machine `model_fn`, you can prepare it for distributed training with a one-line code change. Simply wrap your optimizer in a `HorovodDistributedOptimizer`, as in the example below."],"metadata":{}},{"cell_type":"markdown","source":["## Option 1: Make your Estimator"],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nimport horovod.tensorflow as hvd\n\ntf.set_random_seed(seed=40)\n\ndef model_fn(features, labels, mode, params):\n    features_with_shape = tf.reshape(features[\"features\"], [-1, 24]) # Explicitly specify dimensions\n    \n    hidden_layer1 = tf.layers.dense(inputs=features_with_shape, units=params[\"hidden_layer1\"], activation=tf.nn.relu)\n    hidden_layer2 = tf.layers.dense(inputs=hidden_layer1, units=params[\"hidden_layer2\"], activation=tf.nn.relu)\n    predictions = tf.squeeze(tf.layers.dense(inputs=hidden_layer2, units=1, activation=None), axis=-1)\n    \n    # If the estimator is running in PREDICT mode, we can stop building our model graph here and simply return\n    # our model's inference outputs\n    serving_key = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n    export_outputs = {serving_key: tf.estimator.export.PredictOutput({\"predictions\": predictions})}\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, export_outputs=export_outputs)\n      \n    # Calculate Loss (for both TRAIN and EVAL modes)\n    loss = tf.losses.mean_squared_error(labels, predictions)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"] * hvd.size())\n        optimizer = hvd.DistributedOptimizer(optimizer)\n        \n        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op,\n                                          export_outputs=export_outputs)\n    # If running in EVAL mode, add model evaluation metrics (accuracy) to our EstimatorSpec so that\n    # they're logged when model evaluation runs\n    eval_metric_ops = {\"rmse\": tf.metrics.root_mean_squared_error(labels=labels, predictions=predictions)}\n    return tf.estimator.EstimatorSpec(\n        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops, export_outputs=export_outputs)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Option 2: Premade estimators\n\nYou can extract the model_fn created by the premade estimator."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nimport horovod.tensorflow as hvd\n\ntf.set_random_seed(seed=40)\n\ndef model_fn(features, labels, mode, params, config):\n    feat_cols = [tf.feature_column.numeric_column(key=\"features\", shape=(24,))]\n    regressor = tf.estimator.DNNRegressor(\n      hidden_units=[params[\"hidden_layer1\"], params[\"hidden_layer2\"]],\n      feature_columns=feat_cols,\n      optimizer=hvd.DistributedOptimizer(tf.train.AdamOptimizer(params[\"learning_rate\"] * hvd.size())))\n    estimator_spec = regressor.model_fn(features, labels, mode, config)\n    export_outputs = estimator_spec.export_outputs\n    if export_outputs is not None:\n      export_outputs[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = export_outputs[\"predict\"]\n    return tf.estimator.EstimatorSpec(mode=mode, loss=estimator_spec.loss, train_op=estimator_spec.train_op,\n                                      export_outputs=export_outputs, training_hooks=estimator_spec.training_hooks, predictions=estimator_spec.predictions)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Create model directory"],"metadata":{}},{"cell_type":"code","source":["import time\n\ntrainValDF = concatTrainDF.withColumn(\"isVal\", when(rand() > 0.8, True).otherwise(False))\n\nmodel_dir = \"/tmp/horovodDemo/\" + str(int(time.time()))\nprint(model_dir)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Launch model training"],"metadata":{}},{"cell_type":"code","source":["from sparkdl.estimators.horovod_estimator.estimator import HorovodEstimator\n\nest = HorovodEstimator(modelFn=model_fn,\n                       featureMapping={\"features\":\"features\"},\n                       modelDir=model_dir,\n                       labelCol=\"rating\",\n                       batchSize=128,\n                       maxSteps=20000,\n                       isValidationCol=\"isVal\",  \n                       modelFnParams={\"hidden_layer1\": 30, \"hidden_layer2\": 20, \"learning_rate\": 0.0001},\n                       saveCheckpointsSecs=30)\ntransformer = est.fit(trainValDF)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["predDF = transformer.transform(concatTestDF)\ndisplay(predDF.select(\"userId\", \"movieId\", \"predictions\", \"rating\"))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql.types import FloatType\ndef _pred(v):\n  return float(v[0])\n\npred = udf(_pred, FloatType())\npredDF = predDF.withColumn(\"prediction\", pred(predDF.predictions))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["testMse = regEval.evaluate(predDF)\n\nprint('The model had a MSE on the test set of {0}'.format(testMse))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"05 Horovod","notebookId":1256250},"nbformat":4,"nbformat_minor":0}
