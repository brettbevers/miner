{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import islice\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from hyperspherical_vae.distributions import VonMisesFisher, HypersphericalUniform\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(*dfs):\n",
    "  dataset = reduce(lambda a,b: np.append(a, b, axis=0), dfs)\n",
    "  permutation = np.random.permutation(dataset.shape[0])\n",
    "  shuffled = dataset[permutation]\n",
    "  return shuffled\n",
    "\n",
    "\n",
    "unit = 0.15\n",
    "\n",
    "on = unit * 0.1\n",
    "off = unit * 0.00\n",
    "\n",
    "\n",
    "def generate_cluster(unit, cross_cov, mu, count):\n",
    "    mu = np.array(mu)\n",
    "    sigma_1, sigma_2, sigma_3 = unit, unit, unit\n",
    "    sigma_1_2, sigma_1_3, sigma_2_3 = cross_cov\n",
    "    cov = np.array([\n",
    "      [sigma_1, sigma_1_2, sigma_1_3],\n",
    "      [sigma_1_2, sigma_2, sigma_2_3],\n",
    "      [sigma_1_3, sigma_2_3, sigma_3]\n",
    "    ])\n",
    "    ds = np.random.multivariate_normal(mu, cov, count)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def reduce_correlation(ds, noise):\n",
    "    count = ds.shape[0]\n",
    "    ds_t = ds.T\n",
    "    for i, x in enumerate(noise):\n",
    "        num = int(count*x)\n",
    "        ds_t[i].put(np.random.choice(count, num, replace=False), np.random.choice(ds.T[i], num))\n",
    "    return ds_t.T\n",
    "\n",
    "\n",
    "rv1 = generate_cluster(unit, (on, -off, off), [2,0,0], 250000) \n",
    "rv2 = generate_cluster(unit, (on, off, -off), [0,0,2], 500000)\n",
    "rv3 = generate_cluster(unit, (-on, off, off), [0,2,0], 500000)\n",
    "rv4 = generate_cluster(unit, (-on, -off, -off), [-1,-1,-1], 1000000)\n",
    "\n",
    "# rv5 = generate_cluster(unit, (on, -off, off), [0,0,2], 1250000)\n",
    "# rv6 = generate_cluster(unit, (-on, off, -off), [0,2,0], 1000000)\n",
    "# \n",
    "# data = np.append(shuffle(rv1, rv2, rv3, rv4), shuffle(rv5, rv6), axis=1)\n",
    "\n",
    "data = reduce(lambda a,b: np.append(a, b, axis=0), [rv1, rv2, rv3, rv4])\n",
    "\n",
    "# ind_var = np.random.uniform(-1, 1, [data.shape[0], 20])\n",
    "\n",
    "# data = np.append(data, ind_var, axis=1)\n",
    "\n",
    "data = shuffle(data)\n",
    "\n",
    "# data = reduce_correlation(data, [0.75, 0.75, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]),\n array([-2.83909959, -2.70697706, -2.57485453, -2.442732  , -2.31060948,\n        -2.17848695, -2.04636442, -1.91424189, -1.78211936, -1.64999683,\n        -1.5178743 , -1.38575177, -1.25362924, -1.12150671, -0.98938418,\n        -0.85726165, -0.72513912, -0.59301659, -0.46089406, -0.32877153,\n        -0.196649  , -0.06452647,  0.06759606,  0.19971858,  0.33184111,\n         0.46396364,  0.59608617,  0.7282087 ,  0.86033123,  0.99245376,\n         1.12457629,  1.25669882,  1.38882135,  1.52094388,  1.65306641,\n         1.78518894,  1.91731147,  2.049434  ,  2.18155653,  2.31367906,\n         2.44580159,  2.57792411,  2.71004664,  2.84216917,  2.9742917 ,\n         3.10641423,  3.23853676,  3.37065929,  3.50278182,  3.63490435,\n         3.76702688]),\n array([-2.86004186, -2.72625641, -2.59247097, -2.45868552, -2.32490008,\n        -2.19111464, -2.05732919, -1.92354375, -1.7897583 , -1.65597286,\n        -1.52218741, -1.38840197, -1.25461652, -1.12083108, -0.98704563,\n        -0.85326019, -0.71947474, -0.5856893 , -0.45190385, -0.31811841,\n        -0.18433296, -0.05054752,  0.08323793,  0.21702337,  0.35080882,\n         0.48459426,  0.6183797 ,  0.75216515,  0.88595059,  1.01973604,\n         1.15352148,  1.28730693,  1.42109237,  1.55487782,  1.68866326,\n         1.82244871,  1.95623415,  2.0900196 ,  2.22380504,  2.35759049,\n         2.49137593,  2.62516138,  2.75894682,  2.89273227,  3.02651771,\n         3.16030316,  3.2940886 ,  3.42787404,  3.56165949,  3.69544493,\n         3.82923038]),\n <matplotlib.image.AxesImage at 0x7f67c8602a90>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67c858f550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist2d(data[:,0], data[:,1], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(object):\n",
    "    def __init__(self, n_input_units, n_hidden_layers, n_hidden_units, n_latent_units,\n",
    "                 learning_rate=0.005, batch_size=100, min_beta=1.0, max_beta=1.0,\n",
    "                 distribution='normal', serial_layering=None):\n",
    "        self.n_input_units = n_input_units\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_latent_units = n_latent_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.min_beta = min_beta\n",
    "        self.max_beta = max_beta\n",
    "        self.distribution = distribution\n",
    "        if serial_layering:\n",
    "            if not isinstance(serial_layering, (list, tuple)):\n",
    "                raise TypeError(\"Argument 'serial_layering' must be a list or tuple of integers.\")\n",
    "            elif not all([isinstance(x, int) for x in serial_layering]):\n",
    "                raise TypeError(\"Argument 'serial_layering' must be a list or tuple of integers.\")\n",
    "            elif sum(serial_layering) != self.n_hidden_layers:\n",
    "                raise ValueError(\"Groupings in 'serial_layering' must sum to 'n_hidden_layers'.\")\n",
    "        self.serial_layering = serial_layering or [self.n_hidden_layers]\n",
    "        self.layer_sequence = [sum(self.serial_layering[:i + 1]) for i in range(len(self.serial_layering))]\n",
    "\n",
    "    class Encoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_latent_units, distribution, initialize_layers=None):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_latent_units = n_latent_units\n",
    "            self.distribution = distribution\n",
    "            self.initialize_layers = initialize_layers or []\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs):\n",
    "            if self.initialize_layers:\n",
    "                kernel_initializer, bias_initializer = self.initialize_layers.pop(0)\n",
    "            else:\n",
    "                kernel_initializer, bias_initializer = None, None\n",
    "\n",
    "            self.hidden_layers.append(tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid,\n",
    "                                                      kernel_initializer=kernel_initializer,\n",
    "                                                      bias_initializer=bias_initializer))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_mu(self, inputs):\n",
    "            if self.distribution == 'normal':\n",
    "                self.mu = tf.layers.Dense(units=self.n_latent_units)\n",
    "            elif self.distribution == 'vmf':\n",
    "                self.mu = tf.layers.Dense(units=self.n_latent_units + 1,\n",
    "                                          activation=lambda x: tf.nn.l2_normalize(x, axis=-1))\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "\n",
    "            self.applied_mu = self.mu.apply(inputs)\n",
    "            return self.applied_mu\n",
    "\n",
    "        def add_sigma(self, inputs):\n",
    "            if self.distribution == 'normal':\n",
    "                self.sigma = tf.layers.Dense(units=self.n_latent_units)\n",
    "                self.applied_sigma = self.sigma.apply(inputs)\n",
    "            elif self.distribution == 'vmf':\n",
    "                self.sigma = tf.layers.Dense(units=1, activation=tf.nn.softplus)\n",
    "                self.applied_sigma = self.sigma.apply(inputs) + 1\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "            return self.applied_sigma\n",
    "\n",
    "        def build(self, inputs):\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            layer = self.add_hidden_layer(inputs)\n",
    "\n",
    "            for i in range(self.n_hidden_layers - 1):\n",
    "                layer = self.add_hidden_layer(layer)\n",
    "\n",
    "            mu = self.add_mu(layer)\n",
    "            sigma = self.add_sigma(layer)\n",
    "\n",
    "            return mu, sigma\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            mu = sess.run([self.mu.kernel, self.mu.bias])\n",
    "\n",
    "            sigma = sess.run([self.sigma.kernel, self.sigma.bias])\n",
    "\n",
    "            return layers, mu, sigma\n",
    "\n",
    "    class Decoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_output_units, initialize_layers=None):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_output_units = n_output_units\n",
    "            self.initialize_layers = initialize_layers or None\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs):\n",
    "            if self.initialize_layers:\n",
    "                kernel_initializer, bias_initializer = self.initialize_layers.pop(0)\n",
    "            else:\n",
    "                kernel_initializer, bias_initializer = None, None\n",
    "\n",
    "            self.hidden_layers.append(tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid,\n",
    "                                                      kernel_initializer=kernel_initializer,\n",
    "                                                      bias_initializer=bias_initializer))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_output(self, inputs):\n",
    "            self.output = tf.layers.Dense(units=self.n_output_units)\n",
    "            self.applied_output = self.output.apply(inputs)\n",
    "            return self.applied_output\n",
    "\n",
    "        def build(self, inputs):\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            layer = self.add_hidden_layer(inputs)\n",
    "\n",
    "            for i in range(self.n_hidden_layers - 1):\n",
    "                layer = self.add_hidden_layer(layer)\n",
    "\n",
    "            output = self.add_output(layer)\n",
    "\n",
    "            return output\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            output = sess.run([self.output.kernel, self.output.bias])\n",
    "\n",
    "            return layers, output\n",
    "\n",
    "    def sampled_z(self, mu, sigma, batch_size):\n",
    "        if self.distribution == 'normal':\n",
    "            epsilon = tf.random_normal(tf.stack([int(batch_size), self.n_latent_units]))\n",
    "            z = mu + tf.multiply(epsilon, tf.exp(0.5 * sigma))\n",
    "            loss = tf.reduce_mean(-0.5 * self.beta * tf.reduce_sum(1.0 + sigma - tf.square(mu) - tf.exp(sigma), 1))\n",
    "        elif self.distribution == 'vmf':\n",
    "            self.q_z = VonMisesFisher(mu, sigma, validate_args=True, allow_nan_stats=False)\n",
    "            z = self.q_z.sample()\n",
    "            self.p_z = HypersphericalUniform(self.n_latent_units, validate_args=True, allow_nan_stats=False)\n",
    "            loss = tf.reduce_mean(-self.q_z.kl_divergence(self.p_z))\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return z, loss\n",
    "\n",
    "    def build_feature_loss(self, x, output):\n",
    "        return tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x, output), 1))\n",
    "    \n",
    "    def build_encoder_initializers(self, sess, n_hidden_layers):\n",
    "        return self.build_initializers('encoder', sess, n_hidden_layers)\n",
    "    \n",
    "    def build_decoder_initializers(self, sess, n_hidden_layers):\n",
    "        return self.build_initializers('decoder', sess, n_hidden_layers)\n",
    "\n",
    "    def build_initializers(self, attr_name, sess, n_hidden_layers):\n",
    "        if hasattr(self, attr_name):\n",
    "            layers = getattr(self, attr_name).eval(sess)[0]\n",
    "            result = []\n",
    "            for i in range(n_hidden_layers):\n",
    "                if layers:\n",
    "                    kernel, bias = layers.pop(0)\n",
    "                    result.append((tf.constant_initializer(kernel), tf.constant_initializer(bias)))\n",
    "                else:\n",
    "                    result.append((\n",
    "                        tf.constant_initializer(np.diag(np.ones(self.n_latent_units))),\n",
    "                        tf.constant_initializer(np.diag(np.ones(self.n_latent_units)))\n",
    "                    ))\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def initialize_tensors(self, sess, n_hidden_layers=None):\n",
    "        n_hidden_layers = n_hidden_layers or self.n_hidden_layers\n",
    "        \n",
    "        self.x = tf.placeholder(\"float32\", [self.batch_size, self.n_input_units])\n",
    "        self.beta = tf.placeholder(\"float32\", [1, 1])\n",
    "        self.encoder = self.Encoder(n_hidden_layers, self.n_hidden_units, self.n_latent_units, self.distribution,\n",
    "                                    initialize_layers=self.build_encoder_initializers(sess, n_hidden_layers))\n",
    "        mu, sigma = self.encoder.build(self.x)\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        z, latent_loss = self.sampled_z(self.mu, self.sigma, self.batch_size)\n",
    "        self.z = z\n",
    "        self.latent_loss = latent_loss\n",
    "\n",
    "        self.decoder = self.Decoder(n_hidden_layers, self.n_hidden_units, self.n_input_units,\n",
    "                                    initialize_layers=self.build_decoder_initializers(sess, n_hidden_layers))\n",
    "        self.output = self.decoder.build(self.z)\n",
    "\n",
    "        self.feature_loss = self.build_feature_loss(self.x, self.output)\n",
    "        self.loss = self.feature_loss + self.latent_loss\n",
    "\n",
    "    def total_steps(self, data_rdd, epochs):\n",
    "        data_count = data_rdd.count()\n",
    "        num_batches = int(data_count / self.batch_size)\n",
    "        return (num_batches * epochs) - epochs\n",
    "    \n",
    "    def generate_beta_values(self, total_steps):\n",
    "        beta_delta = self.max_beta - self.min_beta\n",
    "        log_beta_step = 5 / float(total_steps)\n",
    "        beta_values = [\n",
    "            self.min_beta + (beta_delta * (1 - math.exp(-5 + (i * log_beta_step))))\n",
    "            for i in range(total_steps)\n",
    "        ]\n",
    "        return beta_values\n",
    "\n",
    "    def train_from_rdd(self, data_rdd, epochs=1):\n",
    "        total_steps = self.total_steps(data_rdd, epochs)\n",
    "        beta_values = self.generate_beta_values(total_steps)\n",
    "        n_layer_steps = len(self.layer_sequence)\n",
    "        layer_sequence_step = int(total_steps / n_layer_steps)\n",
    "        layer_sequence = self.layer_sequence.copy()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            batch_index = 0            \n",
    "            for epoch_index in range(epochs):\n",
    "                iterator = data_rdd.toLocalIterator()\n",
    "                while True:\n",
    "                    if (not batch_index % layer_sequence_step) and layer_sequence:\n",
    "                        n_hidden_layers = layer_sequence.pop(0)\n",
    "                        self.initialize_tensors(sess, n_hidden_layers)\n",
    "                        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "                        sess.run(tf.global_variables_initializer())\n",
    "                        \n",
    "                    batch = np.array(list(islice(iterator, self.batch_size)))\n",
    "                    if batch.shape[0] == self.batch_size:\n",
    "                        beta = beta_values.pop(0) if len(beta_values) > 0 else self.min_beta\n",
    "                        feed_dict = {self.x: np.array(batch), self.beta: np.array([[beta]])}\n",
    "\n",
    "                        if not batch_index % 1000:\n",
    "                            print(\"beta: {}\".format(beta))\n",
    "                            print(\"number of hidden layers: {}\".format(n_hidden_layers))\n",
    "                            ls, f_ls, d_ls = sess.run([self.loss, self.feature_loss, self.latent_loss],\n",
    "                                                      feed_dict=feed_dict)\n",
    "                            print(\"loss={}, avg_feature_loss={}, avg_latent_loss={}\".format(ls, np.mean(f_ls),\n",
    "                                                                                            np.mean(d_ls)))\n",
    "                            print('running batch {} (epoch {})'.format(batch_index, epoch_index))\n",
    "                        sess.run(optimizer, feed_dict=feed_dict)\n",
    "                        batch_index += 1\n",
    "                    else:\n",
    "                        print(\"incomplete batch: {}\".format(batch.shape))\n",
    "                        break\n",
    "\n",
    "            print(\"evaluating model...\")\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "    def train(self, data, visualize=False, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "\n",
    "        data_size = data.shape[0]\n",
    "        batch_size = self.batch_size\n",
    "        beta_values = self.generate_beta_values(data_size, epochs)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            i = 0\n",
    "            while (i * batch_size) < data_size:\n",
    "                batch = data[i * batch_size:(i + 1) * batch_size]\n",
    "                beta = beta_values.pop(0) if len(beta_values) > 0 else self.min_beta\n",
    "                feed_dict = {self.x: batch, self.beta: np.array([[beta]])}\n",
    "                sess.run(optimizer, feed_dict=feed_dict)\n",
    "                if visualize and (not i % int((data_size / batch_size) / 3) or i == int(data_size / batch_size) - 1):\n",
    "                    ls, d, f_ls, d_ls = sess.run([self.loss, self.output, self.feature_loss, self.latent_loss],\n",
    "                                                 feed_dict=feed_dict)\n",
    "                    plt.scatter(batch[:, 0], batch[:, 1])\n",
    "                    plt.show()\n",
    "                    plt.scatter(d[:, 0], d[:, 1])\n",
    "                    plt.show()\n",
    "                    print(i, ls, np.mean(f_ls), np.mean(d_ls))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoderModel(object):\n",
    "    def __init__(self, encoder_layers, mu, sigma, decoder_layers, output):\n",
    "        self.encoder = self.EncoderModel(encoder_layers, mu, sigma)\n",
    "        self.decoder = self.DecoderModel(decoder_layers, output)\n",
    "\n",
    "    def save(self, path):\n",
    "        encoder_layers, encoder_mu, encoder_sigma = self.encoder.dump()\n",
    "        decoder_layers, decoder_output = self.decoder.dump()\n",
    "        serializable_model = (encoder_layers, encoder_mu, encoder_sigma, decoder_layers, decoder_output)\n",
    "        pickle.dump(serializable_model, open(path, 'w+'))\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder.encode(x)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.encoder.encode(x)[0]\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder.decode(x)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return cls(*pickle.load(open(path, 'r')))\n",
    "\n",
    "    class Layer(object):\n",
    "        def __init__(self, kernel, bias, activation='linear'):\n",
    "            self.kernel = kernel\n",
    "            self.bias = bias\n",
    "            self.activation = activation\n",
    "\n",
    "        def dump(self):\n",
    "            return (self.kernel, self.bias, self.activation)\n",
    "\n",
    "        @property\n",
    "        def apply_func(self):\n",
    "            kernel, bias = self.kernel, self.bias\n",
    "\n",
    "            linear = lambda inputs: np.matmul(inputs, kernel) + bias\n",
    "\n",
    "            if self.activation == 'linear':\n",
    "                f = linear\n",
    "            elif self.activation == 'sigmoid':\n",
    "                f = lambda inputs: 1 / (1 + np.exp(-linear(inputs)))\n",
    "\n",
    "            return f\n",
    "\n",
    "        def apply(self, inputs):\n",
    "            return self.apply_func(inputs)\n",
    "\n",
    "    class EncoderModel(object):\n",
    "        def __init__(self, encoder_layers, mu, sigma):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in encoder_layers\n",
    "            ]\n",
    "            self.mu = VariationalAutoEncoderModel.Layer(*mu)\n",
    "            self.sigma = VariationalAutoEncoderModel.Layer(*sigma)\n",
    "\n",
    "        def dump(self):\n",
    "            encoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            encoder_mu = self.mu.dump()[:2]\n",
    "            encoder_sigma = self.sigma.dump()[:2]\n",
    "            return encoder_layers, encoder_mu, encoder_sigma\n",
    "\n",
    "        def encode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.mu.apply(x), self.sigma.apply(x)\n",
    "\n",
    "    class DecoderModel(object):\n",
    "        def __init__(self, decoder_layers, output):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in decoder_layers\n",
    "            ]\n",
    "            self.output = VariationalAutoEncoderModel.Layer(*output)\n",
    "\n",
    "        def dump(self):\n",
    "            decoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            decoder_output = self.output.dump()[:2]\n",
    "            return decoder_layers, decoder_output\n",
    "\n",
    "        def decode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.output.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9966310265004572\nnumber of hidden layers: 2\nloss=6.075193881988525, avg_feature_loss=5.7179436683654785, avg_latent_loss=0.35725024342536926\nrunning batch 0 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9962350892792429\nnumber of hidden layers: 2\nloss=3.042595386505127, avg_feature_loss=2.297854423522949, avg_latent_loss=0.7447409629821777\nrunning batch 1000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9957926196993844\nnumber of hidden layers: 2\nloss=2.577054023742676, avg_feature_loss=1.6418838500976562, avg_latent_loss=0.9351701140403748\nrunning batch 2000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9952981490646216\nnumber of hidden layers: 2\nloss=2.1917264461517334, avg_feature_loss=1.0062663555145264, avg_latent_loss=1.185460090637207\nrunning batch 3000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9947455659724211\nnumber of hidden layers: 2\nloss=2.167630672454834, avg_feature_loss=0.7773662805557251, avg_latent_loss=1.3902643918991089\nrunning batch 4000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9941280407801879\nnumber of hidden layers: 2\nloss=2.0050463676452637, avg_feature_loss=0.6300852298736572, avg_latent_loss=1.374961256980896\nrunning batch 5000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9934379411943964\nnumber of hidden layers: 2\nloss=1.8972864151000977, avg_feature_loss=0.4752904772758484, avg_latent_loss=1.421995997428894\nrunning batch 6000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9926667379393725\nnumber of hidden layers: 2\nloss=1.9049499034881592, avg_feature_loss=0.500634491443634, avg_latent_loss=1.40431547164917\nrunning batch 7000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.991804899339836\nnumber of hidden layers: 2\nloss=1.8310469388961792, avg_feature_loss=0.4542647898197174, avg_latent_loss=1.3767821788787842\nrunning batch 8000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9908417735142994\nnumber of hidden layers: 2\nloss=2.0590906143188477, avg_feature_loss=0.6465567946434021, avg_latent_loss=1.4125338792800903\nrunning batch 9000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9897654567232967\nnumber of hidden layers: 2\nloss=2.0930356979370117, avg_feature_loss=0.8311198353767395, avg_latent_loss=1.261915922164917\nrunning batch 10000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9885626462452901\nnumber of hidden layers: 2\nloss=1.8930385112762451, avg_feature_loss=0.5510019659996033, avg_latent_loss=1.342036485671997\nrunning batch 11000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9872184759618785\nnumber of hidden layers: 2\nloss=1.998871922492981, avg_feature_loss=0.6119487285614014, avg_latent_loss=1.3869231939315796\nrunning batch 12000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9857163326202268\nnumber of hidden layers: 2\nloss=1.9772007465362549, avg_feature_loss=0.7049587368965149, avg_latent_loss=1.2722420692443848\nrunning batch 13000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9840376505018111\nnumber of hidden layers: 2\nloss=2.0805516242980957, avg_feature_loss=0.6654902696609497, avg_latent_loss=1.4150614738464355\nrunning batch 14000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9821616819596946\nnumber of hidden layers: 3\nloss=9.036012649536133, avg_feature_loss=8.290133476257324, avg_latent_loss=0.7458795309066772\nrunning batch 15000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.98006524098829\nnumber of hidden layers: 3\nloss=3.0654733180999756, avg_feature_loss=2.3412084579467773, avg_latent_loss=0.7242648601531982\nrunning batch 16000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9777224166562651\nnumber of hidden layers: 3\nloss=2.0953726768493652, avg_feature_loss=0.9595906734466553, avg_latent_loss=1.13578200340271\nrunning batch 17000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9751042528607683\nnumber of hidden layers: 3\nloss=2.520604372024536, avg_feature_loss=1.4975110292434692, avg_latent_loss=1.023093342781067\nrunning batch 18000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9721783904449011\nnumber of hidden layers: 3\nloss=2.2909865379333496, avg_feature_loss=1.019160270690918, avg_latent_loss=1.271826148033142\nrunning batch 19000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9689086672551954\nnumber of hidden layers: 3\nloss=2.0145914554595947, avg_feature_loss=0.6255074143409729, avg_latent_loss=1.389083981513977\nrunning batch 20000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9652546711960094\nnumber of hidden layers: 3\nloss=1.9964359998703003, avg_feature_loss=0.5154604911804199, avg_latent_loss=1.4809755086898804\nrunning batch 21000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9611712407568264\nnumber of hidden layers: 3\nloss=1.910799264907837, avg_feature_loss=0.5997889637947083, avg_latent_loss=1.3110102415084839\nrunning batch 22000 (epoch 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incomplete batch: (0,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9566079068392301\nnumber of hidden layers: 3\nloss=2.0427799224853516, avg_feature_loss=0.629856288433075, avg_latent_loss=1.4129235744476318\nrunning batch 23000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9515082689848259\nnumber of hidden layers: 3\nloss=1.9470386505126953, avg_feature_loss=0.6021044254302979, avg_latent_loss=1.3449342250823975\nrunning batch 24000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9458092982946047\nnumber of hidden layers: 3\nloss=1.9270107746124268, avg_feature_loss=0.6585310101509094, avg_latent_loss=1.268479824066162\nrunning batch 25000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9394405584241943\nnumber of hidden layers: 3\nloss=1.8604321479797363, avg_feature_loss=0.5073621869087219, avg_latent_loss=1.3530699014663696\nrunning batch 26000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9323233350268962\nnumber of hidden layers: 3\nloss=1.887249231338501, avg_feature_loss=0.5916411280632019, avg_latent_loss=1.2956080436706543\nrunning batch 27000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9243696628848781\nnumber of hidden layers: 3\nloss=1.8843272924423218, avg_feature_loss=0.5972625017166138, avg_latent_loss=1.287064790725708\nrunning batch 28000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9154812387043565\nnumber of hidden layers: 3\nloss=1.8676793575286865, avg_feature_loss=0.5611716508865356, avg_latent_loss=1.3065077066421509\nrunning batch 29000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9055482061374855\nnumber of hidden layers: 4\nloss=9.815147399902344, avg_feature_loss=7.625065803527832, avg_latent_loss=2.1900811195373535\nrunning batch 30000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.8944477980144422\nnumber of hidden layers: 4\nloss=2.8196613788604736, avg_feature_loss=2.1152725219726562, avg_latent_loss=0.7043887972831726\nrunning batch 31000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.882042819004397\nnumber of hidden layers: 4\nloss=2.8631961345672607, avg_feature_loss=2.242546319961548, avg_latent_loss=0.6206498742103577\nrunning batch 32000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.8681799499518426\nnumber of hidden layers: 4\nloss=2.0876874923706055, avg_feature_loss=1.0911762714385986, avg_latent_loss=0.9965112209320068\nrunning batch 33000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.8526878529307473\nnumber of hidden layers: 4\nloss=2.0942978858947754, avg_feature_loss=0.8165989518165588, avg_latent_loss=1.2776988744735718\nrunning batch 34000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.8353750535959803\nnumber of hidden layers: 4\nloss=1.6259256601333618, avg_feature_loss=0.4800998270511627, avg_latent_loss=1.1458258628845215\nrunning batch 35000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.8160275746589603\nnumber of hidden layers: 4\nloss=1.6572734117507935, avg_feature_loss=0.43087300658226013, avg_latent_loss=1.226400375366211\nrunning batch 36000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.7944062912385066\nnumber of hidden layers: 4\nloss=1.8183213472366333, avg_feature_loss=0.6737823486328125, avg_latent_loss=1.1445389986038208\nrunning batch 37000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.7702439754003906\nnumber of hidden layers: 4\nloss=1.5776746273040771, avg_feature_loss=0.46567997336387634, avg_latent_loss=1.1119946241378784\nrunning batch 38000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.7432419933576138\nnumber of hidden layers: 4\nloss=1.6474132537841797, avg_feature_loss=0.6036470532417297, avg_latent_loss=1.0437661409378052\nrunning batch 39000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.7130666145105136\nnumber of hidden layers: 4\nloss=1.653568983078003, avg_feature_loss=0.6185782551765442, avg_latent_loss=1.0349907875061035\nrunning batch 40000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.679344886708328\nnumber of hidden layers: 4\nloss=1.489382266998291, avg_feature_loss=0.4874180555343628, avg_latent_loss=1.0019642114639282\nrunning batch 41000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.6416600267525775\nnumber of hidden layers: 4\nloss=1.475031852722168, avg_feature_loss=0.49319759011268616, avg_latent_loss=0.9818342328071594\nrunning batch 42000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.5995462691712561\nnumber of hidden layers: 4\nloss=1.6416029930114746, avg_feature_loss=0.683546781539917, avg_latent_loss=0.9580562114715576\nrunning batch 43000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.5524831095973372\nnumber of hidden layers: 4\nloss=1.3474218845367432, avg_feature_loss=0.48912283778190613, avg_latent_loss=0.8582990765571594\nrunning batch 44000 (epoch 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incomplete batch: (0,)\nevaluating model...\n"
     ]
    }
   ],
   "source": [
    "# rdd = sc.parallelize(data)\n",
    "\n",
    "model = VariationalAutoEncoder(n_input_units=data.shape[1], n_hidden_layers=4, \n",
    "                               n_hidden_units=9, n_latent_units=1, \n",
    "                               learning_rate=0.005, batch_size=100, \n",
    "                               min_beta=0.5, max_beta=1, distribution='normal', serial_layering=[2, 1, 1])\\\n",
    "    .train_from_rdd(rdd, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = VariationalAutoEncoder(n_input_units=data.shape[1], n_hidden_layers=2, \n",
    "#                                n_hidden_units=9, n_latent_units=1, \n",
    "#                                learning_rate=0.005, batch_size=100, \n",
    "#                                min_beta=1, max_beta=1, distribution='vmf')\\\n",
    "#     .train(data, epochs=1, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.93109e+05, 9.68780e+04, 5.57470e+04, 2.97680e+04, 1.40600e+04,\n        5.77800e+03, 2.27400e+03, 9.88000e+02, 4.77000e+02, 2.70000e+02,\n        1.57000e+02, 1.08000e+02, 5.80000e+01, 3.50000e+01, 3.90000e+01,\n        3.00000e+01, 3.20000e+01, 1.30000e+01, 1.70000e+01, 2.00000e+01,\n        1.50000e+01, 1.20000e+01, 1.00000e+01, 5.00000e+00, 9.00000e+00,\n        7.00000e+00, 6.00000e+00, 8.00000e+00, 9.00000e+00, 1.40000e+01,\n        9.00000e+00, 8.00000e+00, 1.40000e+01, 2.40000e+01, 3.70000e+01,\n        5.30000e+01, 1.10000e+02, 2.96000e+02, 1.13600e+03, 9.24200e+03,\n        1.21714e+05, 1.02233e+05, 1.20880e+04, 1.67900e+03, 6.17000e+02,\n        3.01000e+02, 1.74000e+02, 1.61000e+02, 1.56000e+02, 1.23000e+02,\n        1.47000e+02, 1.47000e+02, 1.65000e+02, 2.25000e+02, 3.14000e+02,\n        4.58000e+02, 1.08500e+03, 5.69100e+03, 9.34850e+04, 2.12388e+05,\n        1.40351e+05, 3.60040e+04, 7.14900e+03, 1.65600e+03, 5.33000e+02,\n        2.05000e+02, 1.23000e+02, 9.90000e+01, 8.80000e+01, 9.40000e+01,\n        1.05000e+02, 1.36000e+02, 1.64000e+02, 1.92000e+02, 1.90000e+02,\n        2.05000e+02, 2.45000e+02, 3.26000e+02, 3.84000e+02, 4.40000e+02,\n        4.80000e+02, 6.12000e+02, 7.48000e+02, 8.80000e+02, 1.10500e+03,\n        1.31400e+03, 1.72400e+03, 2.24400e+03, 2.78700e+03, 3.69900e+03,\n        4.92900e+03, 6.75200e+03, 9.56400e+03, 1.33910e+04, 2.00040e+04,\n        2.98590e+04, 4.62820e+04, 7.52750e+04, 1.31974e+05, 1.43455e+05]),\n array([-1.01275774e+00, -9.87450820e-01, -9.62143903e-01, -9.36836986e-01,\n        -9.11530069e-01, -8.86223152e-01, -8.60916236e-01, -8.35609319e-01,\n        -8.10302402e-01, -7.84995485e-01, -7.59688568e-01, -7.34381651e-01,\n        -7.09074734e-01, -6.83767817e-01, -6.58460900e-01, -6.33153983e-01,\n        -6.07847066e-01, -5.82540149e-01, -5.57233232e-01, -5.31926316e-01,\n        -5.06619399e-01, -4.81312482e-01, -4.56005565e-01, -4.30698648e-01,\n        -4.05391731e-01, -3.80084814e-01, -3.54777897e-01, -3.29470980e-01,\n        -3.04164063e-01, -2.78857146e-01, -2.53550229e-01, -2.28243312e-01,\n        -2.02936395e-01, -1.77629479e-01, -1.52322562e-01, -1.27015645e-01,\n        -1.01708728e-01, -7.64018108e-02, -5.10948939e-02, -2.57879770e-02,\n        -4.81060062e-04,  2.48258569e-02,  5.01327738e-02,  7.54396907e-02,\n         1.00746608e-01,  1.26053525e-01,  1.51360441e-01,  1.76667358e-01,\n         2.01974275e-01,  2.27281192e-01,  2.52588109e-01,  2.77895026e-01,\n         3.03201943e-01,  3.28508860e-01,  3.53815777e-01,  3.79122694e-01,\n         4.04429611e-01,  4.29736528e-01,  4.55043445e-01,  4.80350362e-01,\n         5.05657278e-01,  5.30964195e-01,  5.56271112e-01,  5.81578029e-01,\n         6.06884946e-01,  6.32191863e-01,  6.57498780e-01,  6.82805697e-01,\n         7.08112614e-01,  7.33419531e-01,  7.58726448e-01,  7.84033365e-01,\n         8.09340282e-01,  8.34647199e-01,  8.59954115e-01,  8.85261032e-01,\n         9.10567949e-01,  9.35874866e-01,  9.61181783e-01,  9.86488700e-01,\n         1.01179562e+00,  1.03710253e+00,  1.06240945e+00,  1.08771637e+00,\n         1.11302328e+00,  1.13833020e+00,  1.16363712e+00,  1.18894404e+00,\n         1.21425095e+00,  1.23955787e+00,  1.26486479e+00,  1.29017170e+00,\n         1.31547862e+00,  1.34078554e+00,  1.36609245e+00,  1.39139937e+00,\n         1.41670629e+00,  1.44201320e+00,  1.46732012e+00,  1.49262704e+00,\n         1.51793396e+00]),\n <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAF79JREFUeJzt3X+MXeWd3/H3Z/GSkLQEA4ZlbbomipUNQUoCI/Am0ioNWTCkiqkaJKKqeCNXblLS7jaVGqcrFZU0KqmqpouaULnBjam2AZZuips1cV0IWlUKhCEhEGCpJyQLU1OYYEJI0ZIl++0f9zG9DHdmnjFj3xnzfklX95zveZ7znOM748+cH/feVBWSJPX4pXFvgCRp5TA0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1WzXuDVhqp556aq1fv37cmyFJK8p9993346pas1C7Yy401q9fz+Tk5Lg3Q5JWlCR/1tPO01OSpG6GhiSpm6EhSerWFRpJ/lGSh5J8P8lXk7wxyVlJ7kmyP8nNSY5vbd/Q5qfa8vVD6/lMqz+a5OKh+qZWm0qyfag+cgxJ0ngsGBpJ1gL/EJioqnOA44ArgM8DX6iqDcCzwNbWZSvwbFW9DfhCa0eSs1u/dwKbgC8lOS7JccAXgUuAs4GPtrbMM4YkaQx6T0+tAk5Isgp4E/Ak8AHg1rZ8F3BZm97c5mnLL0ySVr+pql6sqh8CU8D57TFVVY9V1c+Bm4DNrc9cY0iSxmDB0Kiq/w38a+BxBmHxHHAf8JOqeqk1mwbWtum1wBOt70ut/SnD9Vl95qqfMs8YkqQx6Dk9tZrBUcJZwK8Cb2ZwKmm2Q98bmzmWLVV91DZuSzKZZHJmZmZUE0nSEug5PfVB4IdVNVNVfwH8EfBe4KR2ugpgHXCgTU8DZwK05W8BDg7XZ/WZq/7jecZ4haraUVUTVTWxZs2Cb2iUJB2mntB4HNiY5E3tOsOFwMPAN4GPtDZbgNva9O42T1t+Z1VVq1/R7q46C9gAfBu4F9jQ7pQ6nsHF8t2tz1xjHBHrt//xyw9J0qv1XNO4h8HF6O8AD7Y+O4BPA59KMsXg+sMNrcsNwCmt/ilge1vPQ8AtDALnG8BVVfWLds3ik8Be4BHgltaWecaQJI1BBn/QHzsmJibqcD97avgI40fXfmipNkmSlr0k91XVxELtfEe4JKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSp24KhkeTtSe4fevw0ye8mOTnJviT72/Pq1j5JrksyleSBJOcOrWtLa78/yZah+nlJHmx9rmvfRc5cY0iSxqPnO8Ifrap3V9W7gfOAF4CvMfju7zuqagNwR5sHuATY0B7bgOthEADA1cAFwPnA1UMhcH1re6jfplafawxJ0hgs9vTUhcAPqurPgM3ArlbfBVzWpjcDN9bA3cBJSc4ALgb2VdXBqnoW2AdsastOrKpv1eALy2+cta5RY0iSxmCxoXEF8NU2fXpVPQnQnk9r9bXAE0N9plttvvr0iPp8Y0iSxqA7NJIcD3wY+MOFmo6o1WHUuyXZlmQyyeTMzMxiukqSFmExRxqXAN+pqqfa/FPt1BLt+elWnwbOHOq3DjiwQH3diPp8Y7xCVe2oqomqmlizZs0idkmStBiLCY2P8v9PTQHsBg7dAbUFuG2ofmW7i2oj8Fw7tbQXuCjJ6nYB/CJgb1v2fJKN7a6pK2eta9QYkqQxWNXTKMmbgN8C/t5Q+VrgliRbgceBy1t9D3ApMMXgTquPAVTVwSSfBe5t7a6pqoNt+hPAV4ATgNvbY74xJElj0BUaVfUCcMqs2jMM7qaa3baAq+ZYz05g54j6JHDOiPrIMSRJ4+E7wiVJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd26QiPJSUluTfKnSR5J8htJTk6yL8n+9ry6tU2S65JMJXkgyblD69nS2u9PsmWofl6SB1uf69p3hTPXGJKk8eg90vh94BtV9evAu4BHgO3AHVW1AbijzQNcAmxoj23A9TAIAOBq4ALgfODqoRC4vrU91G9Tq881hiRpDBYMjSQnAr8J3ABQVT+vqp8Am4Fdrdku4LI2vRm4sQbuBk5KcgZwMbCvqg5W1bPAPmBTW3ZiVX2rfb/4jbPWNWoMSdIY9BxpvBWYAf5jku8m+XKSNwOnV9WTAO35tNZ+LfDEUP/pVpuvPj2izjxjSJLGoCc0VgHnAtdX1XuA/8v8p4kyolaHUe+WZFuSySSTMzMzi+kqSVqEntCYBqar6p42fyuDEHmqnVqiPT891P7Mof7rgAML1NeNqDPPGK9QVTuqaqKqJtasWdOxS5Kkw7FgaFTV/wGeSPL2VroQeBjYDRy6A2oLcFub3g1c2e6i2gg8104t7QUuSrK6XQC/CNjblj2fZGO7a+rKWesaNYYkaQxWdbb7B8AfJDkeeAz4GIPAuSXJVuBx4PLWdg9wKTAFvNDaUlUHk3wWuLe1u6aqDrbpTwBfAU4Abm8PgGvnGEOSNAZdoVFV9wMTIxZdOKJtAVfNsZ6dwM4R9UngnBH1Z0aNIUkaD98RLknqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6dYVGkh8leTDJ/UkmW+3kJPuS7G/Pq1s9Sa5LMpXkgSTnDq1nS2u/P8mWofp5bf1TrW/mG0OSNB6LOdL461X17qo69F3h24E7qmoDcEebB7gE2NAe24DrYRAAwNXABcD5wNVDIXB9a3uo36YFxpAkjcFrOT21GdjVpncBlw3Vb6yBu4GTkpwBXAzsq6qDVfUssA/Y1JadWFXfqqoCbpy1rlFjSJLGoDc0CvjvSe5Lsq3VTq+qJwHa82mtvhZ4YqjvdKvNV58eUZ9vjFdIsi3JZJLJmZmZzl2SJC3Wqs5276uqA0lOA/Yl+dN52mZErQ6j3q2qdgA7ACYmJhbVV5LUr+tIo6oOtOenga8xuCbxVDu1RHt+ujWfBs4c6r4OOLBAfd2IOvOMIUkagwVDI8mbk/zVQ9PARcD3gd3AoTugtgC3tendwJXtLqqNwHPt1NJe4KIkq9sF8IuAvW3Z80k2trumrpy1rlFjSJLGoOf01OnA19pdsKuA/1xV30hyL3BLkq3A48Dlrf0e4FJgCngB+BhAVR1M8lng3tbumqo62KY/AXwFOAG4vT0Arp1jDEnSGCwYGlX1GPCuEfVngAtH1Au4ao517QR2jqhPAuf0jiFJGg/fES5J6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSerWHRpJjkvy3SRfb/NnJbknyf4kNyc5vtXf0Oan2vL1Q+v4TKs/muTiofqmVptKsn2oPnIMSdJ4LOZI43eAR4bmPw98oao2AM8CW1t9K/BsVb0N+EJrR5KzgSuAdwKbgC+1IDoO+CJwCXA28NHWdr4xJElj0BUaSdYBHwK+3OYDfAC4tTXZBVzWpje3edryC1v7zcBNVfViVf0QmALOb4+pqnqsqn4O3ARsXmAMSdIY9B5p/FvgnwB/2eZPAX5SVS+1+WlgbZteCzwB0JY/19q/XJ/VZ676fGO8QpJtSSaTTM7MzHTukiRpsRYMjSR/A3i6qu4bLo9oWgssW6r6q4tVO6pqoqom1qxZM6qJJGkJrOpo8z7gw0kuBd4InMjgyOOkJKvakcA64EBrPw2cCUwnWQW8BTg4VD9kuM+o+o/nGUOSNAYLHmlU1Weqal1VrWdwIfvOqvrbwDeBj7RmW4Db2vTuNk9bfmdVVatf0e6uOgvYAHwbuBfY0O6UOr6Nsbv1mWsMSdIYvJb3aXwa+FSSKQbXH25o9RuAU1r9U8B2gKp6CLgFeBj4BnBVVf2iHUV8EtjL4O6sW1rb+caQJI1Bz+mpl1XVXcBdbfoxBnc+zW7z58Dlc/T/HPC5EfU9wJ4R9ZFjSJLGw3eES5K6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSui0YGknemOTbSb6X5KEk/7zVz0pyT5L9SW5u3+9N+w7wm5NMteXrh9b1mVZ/NMnFQ/VNrTaVZPtQfeQYkqTx6DnSeBH4QFW9C3g3sCnJRuDzwBeqagPwLLC1td8KPFtVbwO+0NqR5GzgCuCdwCbgS0mOS3Ic8EXgEuBs4KOtLfOMIUkagwVDowZ+1mZ/uT0K+ABwa6vvAi5r05vbPG35hUnS6jdV1YtV9UNgisH3f58PTFXVY1X1c+AmYHPrM9cYkqQx6Lqm0Y4I7geeBvYBPwB+UlUvtSbTwNo2vRZ4AqAtfw44Zbg+q89c9VPmGWP29m1LMplkcmZmpmeXJEmHoSs0quoXVfVuYB2DI4N3jGrWnjPHsqWqj9q+HVU1UVUTa9asGdVEkrQEFnX3VFX9BLgL2AiclGRVW7QOONCmp4EzAdrytwAHh+uz+sxV//E8Y0iSxqDn7qk1SU5q0ycAHwQeAb4JfKQ12wLc1qZ3t3na8jurqlr9inZ31VnABuDbwL3Ahnan1PEMLpbvbn3mGkOSNAarFm7CGcCudpfTLwG3VNXXkzwM3JTkXwDfBW5o7W8A/lOSKQZHGFcAVNVDSW4BHgZeAq6qql8AJPkksBc4DthZVQ+1dX16jjEkSWOwYGhU1QPAe0bUH2NwfWN2/c+By+dY1+eAz42o7wH29I4hSRoP3xEuSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnq1vMd4Wcm+WaSR5I8lOR3Wv3kJPuS7G/Pq1s9Sa5LMpXkgSTnDq1rS2u/P8mWofp5SR5sfa5LkvnGkLSw9dv/+OWHtFR6jjReAv5xVb0D2AhcleRsYDtwR1VtAO5o8wCXABvaYxtwPQwCALgauIDBV7hePRQC17e2h/ptavW5xpAkjcGCoVFVT1bVd9r088AjwFpgM7CrNdsFXNamNwM31sDdwElJzgAuBvZV1cGqehbYB2xqy06sqm9VVQE3zlrXqDEkSWOwqGsaSdYD7wHuAU6vqidhECzAaa3ZWuCJoW7TrTZffXpEnXnGkCSNQXdoJPkrwH8Bfreqfjpf0xG1Oox6tyTbkkwmmZyZmVlMV0nSInSFRpJfZhAYf1BVf9TKT7VTS7Tnp1t9GjhzqPs64MAC9XUj6vON8QpVtaOqJqpqYs2aNT27JEk6DD13TwW4AXikqv7N0KLdwKE7oLYAtw3Vr2x3UW0EnmunlvYCFyVZ3S6AXwTsbcueT7KxjXXlrHWNGkOSNAarOtq8D/g7wINJ7m+1fwpcC9ySZCvwOHB5W7YHuBSYAl4APgZQVQeTfBa4t7W7pqoOtulPAF8BTgBubw/mGUOSNAYLhkZV/U9GX3cAuHBE+wKummNdO4GdI+qTwDkj6s+MGkOSNB6+I1yS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUredjRCRJy9DwtzL+6NoPHZUxDQ3pdWAc/7no2OTpKUlSN480JGkFGT5qHAePNCRJ3QwNSVI3T0/pdceLwtLh80hDktSt5zvCdyZ5Osn3h2onJ9mXZH97Xt3qSXJdkqkkDyQ5d6jPltZ+f5ItQ/XzkjzY+lzXvid8zjEkSePTc6TxFWDTrNp24I6q2gDc0eYBLgE2tMc24HoYBABwNXABcD5w9VAIXN/aHuq3aYExJEljsmBoVNWfAAdnlTcDu9r0LuCyofqNNXA3cFKSM4CLgX1VdbCqngX2AZvashOr6lvtu8VvnLWuUWNIksbkcC+En15VTwJU1ZNJTmv1tcATQ+2mW22++vSI+nxjSEvGi+LS4iz1hfCMqNVh1Bc3aLItyWSSyZmZmcV2lyR1OtwjjaeSnNGOAM4Anm71aeDMoXbrgAOt/v5Z9btafd2I9vON8SpVtQPYATAxMbHo0BnFv0C1Eo373cI69h3ukcZu4NAdUFuA24bqV7a7qDYCz7VTTHuBi5KsbhfALwL2tmXPJ9nY7pq6cta6Ro0hSRqTBY80knyVwVHCqUmmGdwFdS1wS5KtwOPA5a35HuBSYAp4AfgYQFUdTPJZ4N7W7pqqOnRx/RMM7tA6Abi9PZhnDEnSmCwYGlX10TkWXTiibQFXzbGencDOEfVJ4JwR9WdGjSFJGh/fES5J6uZnT0nSMrecbnDwSEOS1M3QkCR1MzQkSd0MDUlSNy+Ed/Dd4ZI04JGGJKmboSFJ6ubpKUlahpbTezOGeaQhSermkYb0OuONHXotPNKQJHXzSGOR/CtN0pGyXK9jDPNIQ5LUzdCQJHXz9JQkjdFKOCU1bNmHRpJNwO8DxwFfrqprx7xJL/P6hqTDsdKCYtiyDo0kxwFfBH4LmAbuTbK7qh4e75Zppen5JfWPAB1JKzkohi3r0ADOB6aq6jGAJDcBm4FlFxr+h6NxeS3/Gflze2QdK0ExbLmHxlrgiaH5aeCCMW1LN38RV77X42v4etznuRyL/9kvleUeGhlRq1c1SrYB29rsz5I8+hrHPRX48WtcBwD5/FKs5Yhbsv1dIRa1vyvkNZzPol/fFb7Pr8uf5yV4zX6tp9FyD41p4Myh+XXAgdmNqmoHsGOpBk0yWVUTS7W+5c79Pba5v8e2o72/y/19GvcCG5KcleR44Apg95i3SZJet5b1kUZVvZTkk8BeBrfc7qyqh8a8WZL0urWsQwOgqvYAe47ysEt2qmuFcH+Pbe7vse2o7m+qXnVdWZKkkZb7NQ1J0jJiaABJLk/yUJK/TDLnXQhJNiV5NMlUku1HcxuXUpKTk+xLsr89r56j3S+S3N8eK+4GhIVeryRvSHJzW35PkvVHfyuXTsf+/naSmaHX9O+OYzuXQpKdSZ5O8v05lifJde3f4oEk5x7tbVxKHfv7/iTPDb22/+yIbUxVve4fwDuAtwN3ARNztDkO+AHwVuB44HvA2ePe9sPc338FbG/T24HPz9HuZ+Pe1tewjwu+XsDfB/59m74CuHnc232E9/e3gX837m1dov39TeBc4PtzLL8UuJ3Be702AveMe5uP8P6+H/j60dgWjzSAqnqkqhZ6Q+DLH2lSVT8HDn2kyUq0GdjVpncBl41xW46Untdr+N/hVuDCJKPeULoSHEs/nwuqqj8BDs7TZDNwYw3cDZyU5Iyjs3VLr2N/jxpDo9+ojzRZO6Ztea1Or6onAdrzaXO0e2OSySR3J1lpwdLzer3cpqpeAp4DTjkqW7f0en8+/1Y7XXNrkjNHLD9WHEu/r71+I8n3ktye5J1HapBlf8vtUknyP4BfGbHo96rqtp5VjKgt21vP5tvfRazmr1XVgSRvBe5M8mBV/WBptvCI63m9VtRruoCefflvwFer6sUkH2dwlPWBI75l43EsvbY9vgP8WlX9LMmlwH8FNhyJgV43oVFVH3yNq+j6SJPlYr79TfJUkjOq6sl2yP70HOs40J4fS3IX8B4G581Xgp7X61Cb6SSrgLewTE4BHIYF97eqnhma/Q/Ayv6EqfmtqN/X16qqfjo0vSfJl5KcWlVL/hlcnp7qdyx9pMluYEub3gK86kgryeokb2jTpwLvYxl+JP08el6v4X+HjwB3VruquAItuL+zzul/GHjkKG7f0bYbuLLdRbUReO7QKdljUZJfOXQ9Lsn5DP5vf2b+Xodp3HcFLIcH8DcZ/GXyIvAUsLfVfxXYM9TuUuB/Mfhr+/fGvd2vYX9PAe4A9rfnk1t9gsG3IwK8F3iQwV04DwJbx73dh7Gfr3q9gGuAD7fpNwJ/CEwB3wbeOu5tPsL7+y+Bh9pr+k3g18e9za9hX78KPAn8Rfvd3Qp8HPh4Wx4GX+D2g/bzO/KuyJXy6NjfTw69tncD7z1S2+I7wiVJ3Tw9JUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSp2/8Dmcotulyu2ioAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67b6338748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sample = model.project(data)\n",
    "plt.hist(encoded_sample, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22535, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sample = np.array(rdd.sample(False, 0.01).collect())\n",
    "encoded_sample = model.project(vec_sample)\n",
    "\n",
    "# decoded_sample = data\n",
    "decoded_sample = model.decode(encoded_sample)\n",
    "decoded_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 22860.687369517636, 0.41072610349447514)\n",
      "(3, 12763.132987421812, 0.4873411057035125)\n",
      "(4, 9621.984985738864, 0.6049687548656997)\n",
      "(5, 9496.53577122682, 0.5700197372127715)\n",
      "(6, 9048.603016026918, 0.37332278940894126)\n",
      "(7, 8913.499825276993, 0.31950360789236504)\n",
      "(8, 8888.716481612686, 0.2757845583639317)\n",
      "(9, 8776.807228609861, 0.30912203993217174)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "encoded_x_sample = encoded_sample[:,0].reshape(-1,1)\n",
    "dencoded_y_sample = decoded_sample[:,0].reshape(-1,1)\n",
    "\n",
    "for i in range(2, 10):\n",
    "  z_train, z_test, x_train, x_test = train_test_split(encoded_x_sample, dencoded_y_sample, test_size=0.4)\n",
    "  gmm = GaussianMixture(i).fit(z_train)\n",
    "  bic = gmm.bic(z_test)\n",
    "  labels = gmm.predict(z_test)\n",
    "  ss = silhouette_score(x_test, labels)\n",
    "  print(i, bic, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.append(labels.reshape(-1,1), x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    1, ..., 9012, 9013, 9013]),\n",
       " array([0, 1, 0, ..., 1, 0, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
