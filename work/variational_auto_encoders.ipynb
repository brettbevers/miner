{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-76e177d0766e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from itertools import islice\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from hyperspherical_vae.distributions import VonMisesFisher, HypersphericalUniform\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(*dfs):\n",
    "  dataset = reduce(lambda a,b: np.append(a, b, axis=0), dfs)\n",
    "  permutation = np.random.permutation(dataset.shape[0])\n",
    "  shuffled = dataset[permutation]\n",
    "  return shuffled\n",
    "\n",
    "\n",
    "unit = 0.15\n",
    "\n",
    "on = unit * 0.1\n",
    "off = unit * 0.00\n",
    "\n",
    "\n",
    "def generate_cluster(unit, cross_cov, mu, count):\n",
    "    mu = np.array(mu)\n",
    "    sigma_1, sigma_2, sigma_3 = unit, unit, unit\n",
    "    sigma_1_2, sigma_1_3, sigma_2_3 = cross_cov\n",
    "    cov = np.array([\n",
    "      [sigma_1, sigma_1_2, sigma_1_3],\n",
    "      [sigma_1_2, sigma_2, sigma_2_3],\n",
    "      [sigma_1_3, sigma_2_3, sigma_3]\n",
    "    ])\n",
    "    ds = np.random.multivariate_normal(mu, cov, count)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def reduce_correlation(ds, noise):\n",
    "    count = ds.shape[0]\n",
    "    ds_t = ds.T\n",
    "    for i, x in enumerate(noise):\n",
    "        num = int(count*x)\n",
    "        ds_t[i].put(np.random.choice(count, num, replace=False), np.random.choice(ds.T[i], num))\n",
    "    return ds_t.T\n",
    "\n",
    "\n",
    "rv1 = generate_cluster(unit, (on, -off, off), [2,0,0], 250000) \n",
    "rv2 = generate_cluster(unit, (on, off, -off), [0,0,2], 500000)\n",
    "rv3 = generate_cluster(unit, (-on, off, off), [0,2,0], 500000)\n",
    "rv4 = generate_cluster(unit, (-on, -off, -off), [-1,-1,-1], 1000000)\n",
    "\n",
    "# rv5 = generate_cluster(unit, (on, -off, off), [0,0,2], 1250000)\n",
    "# rv6 = generate_cluster(unit, (-on, off, -off), [0,2,0], 1000000)\n",
    "# \n",
    "# data = np.append(shuffle(rv1, rv2, rv3, rv4), shuffle(rv5, rv6), axis=1)\n",
    "\n",
    "data = reduce(lambda a,b: np.append(a, b, axis=0), [rv1, rv2, rv3, rv4])\n",
    "\n",
    "# ind_var = np.random.uniform(-1, 1, [data.shape[0], 20])\n",
    "\n",
    "# data = np.append(data, ind_var, axis=1)\n",
    "\n",
    "data = shuffle(data)\n",
    "\n",
    "# data = reduce_correlation(data, [0.75, 0.75, 0.75])\n",
    "plt.hist2d(data[:, 0], data[:, 1], bins=50)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(object):\n",
    "    def __init__(self, n_input_units, n_hidden_layers, n_hidden_units, n_latent_units,\n",
    "                 learning_rate=0.005, batch_size=100, min_beta=1.0, max_beta=1.0,\n",
    "                 distribution='normal', serial_layering=None):\n",
    "        self.n_input_units = n_input_units\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_latent_units = n_latent_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.min_beta = min_beta\n",
    "        self.max_beta = max_beta\n",
    "        self.distribution = distribution\n",
    "        self.serial_layering = serial_layering or [self.n_hidden_layers]\n",
    "        if serial_layering:\n",
    "            if not isinstance(serial_layering, (list, tuple)):\n",
    "                raise TypeError(\"Argument 'serial_layering' must be a list or tuple of integers.\")\n",
    "            elif not all([isinstance(x, int) for x in serial_layering]):\n",
    "                raise TypeError(\"Argument 'serial_layering' must be a list or tuple of integers.\")\n",
    "            elif sum(serial_layering) != self.n_hidden_layers:\n",
    "                raise ValueError(\"Groupings in 'serial_layering' must sum to 'n_hidden_layers'.\")\n",
    "\n",
    "    class Encoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_latent_units, distribution, serial_layering=None):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_latent_units = n_latent_units\n",
    "            self.distribution = distribution\n",
    "            self.serial_layering = serial_layering or [self.n_hidden_layers]\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "            self.mu_list = []\n",
    "            self.applied_mu_list = []\n",
    "            self.sigma_list = []\n",
    "            self.applied_sigma_list = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs, initializer=None):\n",
    "            self.hidden_layers.append(\n",
    "                tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid, initializer=initializer))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_mu_and_sigma(self, inputs):\n",
    "            if self.distribution == 'normal':\n",
    "                mu = tf.layers.Dense(units=self.n_latent_units)\n",
    "                applied_mu = mu.apply(inputs)\n",
    "                sigma = tf.layers.Dense(units=self.n_latent_units)\n",
    "                applied_sigma = sigma.apply(inputs)\n",
    "            elif self.distribution == 'vmf':\n",
    "                mu = tf.layers.Dense(units=self.n_latent_units + 1,\n",
    "                                     activation=lambda x: tf.nn.l2_normalize(x, axis=-1))\n",
    "                applied_mu = mu.apply(inputs)\n",
    "                sigma = tf.layers.Dense(units=1, activation=tf.nn.softplus)\n",
    "                applied_sigma = sigma.apply(inputs) + 1\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "\n",
    "            self.mu_list.append(mu)\n",
    "            self.applied_mu_list.append(applied_mu)\n",
    "            self.sigma_list.append(sigma)\n",
    "            self.applied_sigma_list.append(applied_sigma)\n",
    "\n",
    "            return self.applied_mu_list[-1], self.applied_sigma_list[-1]\n",
    "\n",
    "        def build(self, inputs):\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            initializer = None\n",
    "            for group_size in self.serial_layering:\n",
    "\n",
    "                layer = inputs\n",
    "                for i in range(group_size):\n",
    "                    layer = self.add_hidden_layer(layer, initializer=initializer)\n",
    "                self.add_mu_and_sigma(layer)\n",
    "\n",
    "                initializer = tf.initializers.constant(np.diag(np.ones(self.n_hidden_units)))\n",
    "\n",
    "            return self.applied_mu_list, self.applied_sigma_list\n",
    "\n",
    "        @property\n",
    "        def mu(self):\n",
    "            return self.mu_list[-1]\n",
    "\n",
    "        @property\n",
    "        def sigma(self):\n",
    "            return self.sigma_list[-1]\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            mu = sess.run([self.mu.kernel, self.mu.bias])\n",
    "\n",
    "            sigma = sess.run([self.sigma.kernel, self.sigma.bias])\n",
    "\n",
    "            return layers, mu, sigma\n",
    "\n",
    "    class Decoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_output_units, serial_layering=None):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_output_units = n_output_units\n",
    "            self.serial_layering = serial_layering or [self.n_hidden_layers]\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "            self.output_list = []\n",
    "            self.applied_output_list = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs, initializer=None):\n",
    "            self.hidden_layers.append(\n",
    "                tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid, initializer=initializer))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_output(self, inputs):\n",
    "            output = tf.layers.Dense(units=self.n_output_units)\n",
    "            applied_output = output.apply(inputs)\n",
    "            self.output_list.append(output)\n",
    "            self.applied_output_list.append(applied_output)\n",
    "            return applied_output\n",
    "\n",
    "        def build(self, inputs_list):\n",
    "            if len(inputs_list) != len(self.serial_layering):\n",
    "                raise ValueError(\"Number of inputs ({}) must equal number of serial layering groups ({}).\"\n",
    "                                 .format(len(inputs_list), len(self.serial_layering)))\n",
    "\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            initializer = None\n",
    "            for group_size, inputs in zip(self.serial_layering, inputs_list):\n",
    "\n",
    "                layer = inputs\n",
    "                for i in range(group_size):\n",
    "                    layer = self.add_hidden_layer(layer, initializer=initializer)\n",
    "                self.add_output(layer)\n",
    "\n",
    "                initializer = tf.initializers.constant(np.diag(np.ones(self.n_hidden_units)))\n",
    "\n",
    "            return self.applied_output_list\n",
    "\n",
    "        @property\n",
    "        def output(self):\n",
    "            return self.output_list[-1]\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            output = sess.run([self.output.kernel, self.output.bias])\n",
    "\n",
    "            return layers, output\n",
    "\n",
    "    def sampled_z(self, mu, sigma, batch_size):\n",
    "        if self.distribution == 'normal':\n",
    "            epsilon = tf.random_normal(tf.stack([int(batch_size), self.n_latent_units]))\n",
    "            z = mu + tf.multiply(epsilon, tf.exp(0.5 * sigma))\n",
    "            loss = tf.reduce_mean(-0.5 * self.beta * tf.reduce_sum(1.0 + sigma - tf.square(mu) - tf.exp(sigma), 1))\n",
    "        elif self.distribution == 'vmf':\n",
    "            self.q_z = VonMisesFisher(mu, sigma, validate_args=True, allow_nan_stats=False)\n",
    "            z = self.q_z.sample()\n",
    "            self.p_z = HypersphericalUniform(self.n_latent_units, validate_args=True, allow_nan_stats=False)\n",
    "            loss = tf.reduce_mean(-self.q_z.kl_divergence(self.p_z))\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return z, loss\n",
    "\n",
    "    def build_feature_loss(self, x, output):\n",
    "        return tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x, output), 1))\n",
    "\n",
    "    def initialize_tensors(self):\n",
    "        self.x = tf.placeholder(\"float32\", [self.batch_size, self.n_input_units])\n",
    "        self.beta = tf.placeholder(\"float32\", [1, 1])\n",
    "        self.encoder = self.Encoder(self.n_hidden_layers, self.n_hidden_units, self.n_latent_units,\n",
    "                                    self.distribution, self.serial_layering)\n",
    "        mu_list, sigma_list = self.encoder.build(self.x)\n",
    "        self.mu_list = mu_list\n",
    "        self.sigma_list = sigma_list\n",
    "\n",
    "        self.z_list = []\n",
    "        self.latent_loss_list = []\n",
    "        for mu, sigma in zip(self.mu_list, self.sigma_list):\n",
    "            z, latent_loss = self.sampled_z(mu, sigma, self.batch_size)\n",
    "            self.z_list.append(z)\n",
    "            self.latent_loss_list.append(latent_loss)\n",
    "\n",
    "        self.decoder = self.Decoder(self.n_hidden_layers, self.n_hidden_units, self.n_input_units, self.serial_layering)\n",
    "        self.output_list = self.decoder.build(self.z_list)\n",
    "\n",
    "        self.feature_loss_list = [self.build_feature_loss(self.x, output) for output in self.output_list]\n",
    "        self.loss_list = [feature_loss + latent_loss for feature_loss, latent_loss in\n",
    "                          zip(self.feature_loss_list, self.latent_loss_list)]\n",
    "\n",
    "    def total_steps(self, data_count, epochs):\n",
    "        num_batches = int(data_count / self.batch_size)\n",
    "        return (num_batches * epochs) - epochs\n",
    "\n",
    "    def generate_beta_values(self, data_count, epochs):\n",
    "        total_steps = self.total_steps(data_count, epochs)\n",
    "        beta_delta = self.max_beta - self.min_beta\n",
    "        log_beta_step = 5 / float(total_steps)\n",
    "        return [\n",
    "            self.min_beta + (beta_delta * (1 - math.exp(-5 + (i * log_beta_step))))\n",
    "            for i in range(total_steps)\n",
    "        ]\n",
    "\n",
    "    def generate_optimizers(self, loss_list, data_count, epochs):\n",
    "        total_steps = self.total_steps(data_count, epochs)\n",
    "        num_groups = len(loss_list)\n",
    "        group_steps = int(total_steps / num_groups)\n",
    "        optimizers = [tf.train.AdamOptimizer(self.learning_rate).minimize(loss) for loss in loss_list]\n",
    "        return [\n",
    "            optimizers[min(num_groups - 1, int(i / group_steps))]\n",
    "            for i in range(total_steps)\n",
    "        ]\n",
    "\n",
    "    def train_from_rdd(self, data_rdd, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "\n",
    "        data_count = data_rdd.count()\n",
    "        beta_values = self.generate_beta_values(data_count, epochs)\n",
    "        optimizers = self.generate_optimizers(self.loss_list, data_count, epochs)\n",
    "        last_optimizer = optimizers[-1]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch_index in range(epochs):\n",
    "                iterator = data_rdd.toLocalIterator()\n",
    "                batch_index = 0\n",
    "                while True:\n",
    "                    batch = np.array(list(islice(iterator, self.batch_size)))\n",
    "                    if batch.shape[0] == self.batch_size:\n",
    "                        beta = beta_values.pop(0) if beta_values else self.min_beta\n",
    "                        optimizer = optimizers.pop(0) if optimizers else last_optimizer\n",
    "                        feed_dict = {self.x: np.array(batch), self.beta: np.array([[beta]])}\n",
    "\n",
    "                        if not batch_index % 1000:\n",
    "                            print(\"beta: {}\".format(beta))\n",
    "                            ls, f_ls, d_ls = sess.run([self.loss, self.feature_loss, self.latent_loss],\n",
    "                                                      feed_dict=feed_dict)\n",
    "                            print(\"loss={}, avg_feature_loss={}, avg_latent_loss={}\".format(ls, np.mean(f_ls),\n",
    "                                                                                            np.mean(d_ls)))\n",
    "                            print('running batch {} in epoch {}'.format(batch_index, epoch_index))\n",
    "                        sess.run(optimizer, feed_dict=feed_dict)\n",
    "                        batch_index += 1\n",
    "                    else:\n",
    "                        print(\"incomplete batch: {}\".format(batch.shape))\n",
    "                        break\n",
    "\n",
    "            print(\"evaluating model...\")\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "    def train(self, data, visualize=False, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "\n",
    "        data_size = data.shape[0]\n",
    "        batch_size = self.batch_size\n",
    "        beta_values = self.generate_beta_values(data_size, epochs)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            i = 0\n",
    "            while (i * batch_size) < data_size:\n",
    "                batch = data[i * batch_size:(i + 1) * batch_size]\n",
    "                beta = beta_values.pop(0) if len(beta_values) > 0 else self.min_beta\n",
    "                feed_dict = {self.x: batch, self.beta: np.array([[beta]])}\n",
    "                sess.run(optimizer, feed_dict=feed_dict)\n",
    "                if visualize and (not i % int((data_size / batch_size) / 3) or i == int(data_size / batch_size) - 1):\n",
    "                    ls, d, f_ls, d_ls = sess.run([self.loss, self.output, self.feature_loss, self.latent_loss],\n",
    "                                                 feed_dict=feed_dict)\n",
    "                    plt.scatter(batch[:, 0], batch[:, 1])\n",
    "                    plt.show()\n",
    "                    plt.scatter(d[:, 0], d[:, 1])\n",
    "                    plt.show()\n",
    "                    print(i, ls, np.mean(f_ls), np.mean(d_ls))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoderModel(object):\n",
    "    def __init__(self, encoder_layers, mu, sigma, decoder_layers, output):\n",
    "        self.encoder = self.EncoderModel(encoder_layers, mu, sigma)\n",
    "        self.decoder = self.DecoderModel(decoder_layers, output)\n",
    "\n",
    "    def save(self, path):\n",
    "        encoder_layers, encoder_mu, encoder_sigma = self.encoder.dump()\n",
    "        decoder_layers, decoder_output = self.decoder.dump()\n",
    "        serializable_model = (encoder_layers, encoder_mu, encoder_sigma, decoder_layers, decoder_output)\n",
    "        pickle.dump(serializable_model, open(path, 'w+'))\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder.encode(x)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.encoder.encode(x)[0]\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder.decode(x)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return cls(*pickle.load(open(path, 'r')))\n",
    "\n",
    "    class Layer(object):\n",
    "        def __init__(self, kernel, bias, activation='linear'):\n",
    "            self.kernel = kernel\n",
    "            self.bias = bias\n",
    "            self.activation = activation\n",
    "\n",
    "        def dump(self):\n",
    "            return (self.kernel, self.bias, self.activation)\n",
    "\n",
    "        @property\n",
    "        def apply_func(self):\n",
    "            kernel, bias = self.kernel, self.bias\n",
    "\n",
    "            linear = lambda inputs: np.matmul(inputs, kernel) + bias\n",
    "\n",
    "            if self.activation == 'linear':\n",
    "                f = linear\n",
    "            elif self.activation == 'sigmoid':\n",
    "                f = lambda inputs: 1 / (1 + np.exp(-linear(inputs)))\n",
    "\n",
    "            return f\n",
    "\n",
    "        def apply(self, inputs):\n",
    "            return self.apply_func(inputs)\n",
    "\n",
    "    class EncoderModel(object):\n",
    "        def __init__(self, encoder_layers, mu, sigma):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in encoder_layers\n",
    "            ]\n",
    "            self.mu = VariationalAutoEncoderModel.Layer(*mu)\n",
    "            self.sigma = VariationalAutoEncoderModel.Layer(*sigma)\n",
    "\n",
    "        def dump(self):\n",
    "            encoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            encoder_mu = self.mu.dump()[:2]\n",
    "            encoder_sigma = self.sigma.dump()[:2]\n",
    "            return encoder_layers, encoder_mu, encoder_sigma\n",
    "\n",
    "        def encode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.mu.apply(x), self.sigma.apply(x)\n",
    "\n",
    "    class DecoderModel(object):\n",
    "        def __init__(self, decoder_layers, output):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in decoder_layers\n",
    "            ]\n",
    "            self.output = VariationalAutoEncoderModel.Layer(*output)\n",
    "\n",
    "        def dump(self):\n",
    "            decoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            decoder_output = self.output.dump()[:2]\n",
    "            return decoder_layers, decoder_output\n",
    "\n",
    "        def decode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.output.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.993329432471\n",
      "loss=4.47393751144, avg_feature_loss=4.15156364441, avg_latent_loss=0.322373718023\n",
      "running batch 0 in epoch 0\n",
      "beta: 0.938384389304\n",
      "loss=2.28316926956, avg_feature_loss=1.24345874786, avg_latent_loss=1.03971064091\n",
      "running batch 1000 in epoch 0\n",
      "beta: 0.430860498018\n",
      "loss=1.22265672684, avg_feature_loss=0.52480918169, avg_latent_loss=0.697847485542\n",
      "running batch 2000 in epoch 0\n",
      "incomplete batch: (0,)\n",
      "evaluating model...\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = VariationalAutoEncoder(n_input_units=data.shape[1], n_hidden_layers=2, \n",
    "                               n_hidden_units=9, n_latent_units=1, \n",
    "                               learning_rate=0.005, batch_size=1000, \n",
    "                               min_beta=0.01, max_beta=1, distribution='normal')\\\n",
    "    .train_from_rdd(rdd, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = VariationalAutoEncoder(n_input_units=data.shape[1], n_hidden_layers=2, \n",
    "#                                n_hidden_units=9, n_latent_units=1, \n",
    "#                                learning_rate=0.005, batch_size=100, \n",
    "#                                min_beta=1, max_beta=1, distribution='vmf')\\\n",
    "#     .train(data, epochs=1, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 68., 248., 335., 357., 391., 415., 359., 391., 384., 343., 324.,\n",
       "        231., 233., 173., 141., 119.,  93.,  76.,  49.,  40.,  34.,  23.,\n",
       "         27.,  24.,  20.,  11.,  11.,  20.,  14.,   0.,   7.,   9.,   8.,\n",
       "         11.,  33., 136., 772., 653., 420., 250., 123.,  52.,  17.,  15.,\n",
       "         16.,  32.,  58.,  81., 143., 278., 490., 712., 806., 810., 691.,\n",
       "        443., 246.,  98.,  45.,  10.,   5.,   5.,   2.,   0.,   3.,   2.,\n",
       "         11.,   2.,   7.,   7.,  15.,   7.,  25.,  43.,  44.,  50.,  66.,\n",
       "         85., 116., 151., 166., 246., 267., 340., 430., 503., 598., 670.,\n",
       "        788., 891., 820., 805., 799., 721., 553., 420., 241., 128.,  68.,\n",
       "         16.]),\n",
       " array([-1.68547198, -1.65597804, -1.62648411, -1.59699018, -1.56749624,\n",
       "        -1.53800231, -1.50850838, -1.47901444, -1.44952051, -1.42002658,\n",
       "        -1.39053264, -1.36103871, -1.33154477, -1.30205084, -1.27255691,\n",
       "        -1.24306297, -1.21356904, -1.18407511, -1.15458117, -1.12508724,\n",
       "        -1.09559331, -1.06609937, -1.03660544, -1.0071115 , -0.97761757,\n",
       "        -0.94812364, -0.9186297 , -0.88913577, -0.85964184, -0.8301479 ,\n",
       "        -0.80065397, -0.77116004, -0.7416661 , -0.71217217, -0.68267823,\n",
       "        -0.6531843 , -0.62369037, -0.59419643, -0.5647025 , -0.53520857,\n",
       "        -0.50571463, -0.4762207 , -0.44672677, -0.41723283, -0.3877389 ,\n",
       "        -0.35824496, -0.32875103, -0.2992571 , -0.26976316, -0.24026923,\n",
       "        -0.2107753 , -0.18128136, -0.15178743, -0.1222935 , -0.09279956,\n",
       "        -0.06330563, -0.0338117 , -0.00431776,  0.02517617,  0.05467011,\n",
       "         0.08416404,  0.11365797,  0.14315191,  0.17264584,  0.20213977,\n",
       "         0.23163371,  0.26112764,  0.29062157,  0.32011551,  0.34960944,\n",
       "         0.37910338,  0.40859731,  0.43809124,  0.46758518,  0.49707911,\n",
       "         0.52657304,  0.55606698,  0.58556091,  0.61505484,  0.64454878,\n",
       "         0.67404271,  0.70353665,  0.73303058,  0.76252451,  0.79201845,\n",
       "         0.82151238,  0.85100631,  0.88050025,  0.90999418,  0.93948811,\n",
       "         0.96898205,  0.99847598,  1.02796992,  1.05746385,  1.08695778,\n",
       "         1.11645172,  1.14594565,  1.17543958,  1.20493352,  1.23442745,\n",
       "         1.26392138]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11054e6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sample = model.project(data)\n",
    "plt.hist(encoded_sample, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22535, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sample = np.array(rdd.sample(False, 0.01).collect())\n",
    "encoded_sample = model.project(vec_sample)\n",
    "\n",
    "# decoded_sample = data\n",
    "decoded_sample = model.decode(encoded_sample)\n",
    "decoded_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 22860.687369517636, 0.41072610349447514)\n",
      "(3, 12763.132987421812, 0.4873411057035125)\n",
      "(4, 9621.984985738864, 0.6049687548656997)\n",
      "(5, 9496.53577122682, 0.5700197372127715)\n",
      "(6, 9048.603016026918, 0.37332278940894126)\n",
      "(7, 8913.499825276993, 0.31950360789236504)\n",
      "(8, 8888.716481612686, 0.2757845583639317)\n",
      "(9, 8776.807228609861, 0.30912203993217174)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "encoded_x_sample = encoded_sample[:,0].reshape(-1,1)\n",
    "dencoded_y_sample = decoded_sample[:,0].reshape(-1,1)\n",
    "\n",
    "for i in range(2, 10):\n",
    "  z_train, z_test, x_train, x_test = train_test_split(encoded_x_sample, dencoded_y_sample, test_size=0.4)\n",
    "  gmm = GaussianMixture(i).fit(z_train)\n",
    "  bic = gmm.bic(z_test)\n",
    "  labels = gmm.predict(z_test)\n",
    "  ss = silhouette_score(x_test, labels)\n",
    "  print(i, bic, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.append(labels.reshape(-1,1), x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    1, ..., 9012, 9013, 9013]),\n",
       " array([0, 1, 0, ..., 1, 0, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
