{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import islice\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from hyperspherical_vae.distributions import VonMisesFisher, HypersphericalUniform\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]),\n array([-2.95910589, -2.82438468, -2.68966346, -2.55494224, -2.42022102,\n        -2.28549981, -2.15077859, -2.01605737, -1.88133616, -1.74661494,\n        -1.61189372, -1.47717251, -1.34245129, -1.20773007, -1.07300885,\n        -0.93828764, -0.80356642, -0.6688452 , -0.53412399, -0.39940277,\n        -0.26468155, -0.12996034,  0.00476088,  0.1394821 ,  0.27420332,\n         0.40892453,  0.54364575,  0.67836697,  0.81308818,  0.9478094 ,\n         1.08253062,  1.21725183,  1.35197305,  1.48669427,  1.62141549,\n         1.7561367 ,  1.89085792,  2.02557914,  2.16030035,  2.29502157,\n         2.42974279,  2.564464  ,  2.69918522,  2.83390644,  2.96862766,\n         3.10334887,  3.23807009,  3.37279131,  3.50751252,  3.64223374,\n         3.77695496]),\n array([-2.94659271, -2.81353957, -2.68048643, -2.5474333 , -2.41438016,\n        -2.28132702, -2.14827389, -2.01522075, -1.88216762, -1.74911448,\n        -1.61606134, -1.48300821, -1.34995507, -1.21690193, -1.0838488 ,\n        -0.95079566, -0.81774253, -0.68468939, -0.55163625, -0.41858312,\n        -0.28552998, -0.15247684, -0.01942371,  0.11362943,  0.24668256,\n         0.3797357 ,  0.51278884,  0.64584197,  0.77889511,  0.91194825,\n         1.04500138,  1.17805452,  1.31110765,  1.44416079,  1.57721393,\n         1.71026706,  1.8433202 ,  1.97637334,  2.10942647,  2.24247961,\n         2.37553274,  2.50858588,  2.64163902,  2.77469215,  2.90774529,\n         3.04079843,  3.17385156,  3.3069047 ,  3.43995783,  3.57301097,\n         3.70606411]),\n <matplotlib.image.AxesImage at 0x7f0a16966cc0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAFsZJREFUeJzt3V+oZeV5x/Hfs/+cs885zvFPHDVVqbYJoSKJgpgEL1rUi0lIGxIQ4kUITWFuGmggF2nwouSuJRAIJBCGKmlBEkITMSQRM9KIBKpxIka0k4iRBqeajlN15sycP/vf04tzBIvreWbO2mvOPvvd3w8Inv2etfa71t7zzDvPs973NXcXAKAcrWl3AADQLAI7ABSGwA4AhSGwA0BhCOwAUBgCOwAUhsAOAIUhsANAYQjsAFCYzjTedMEWvaeVabw1AMysNb15yt0Pnu/3phLYe1rRh+2uabw1AMysx/zffn8hv0cqBgAKQ2AHgMIQ2AGgMAR2ACgMgR0ACkNgB4DCENgBoDAEdgAoDIEdAApDYAeAwhDYAaAwBHYAKAyBHQAKQ2AHgMIQ2AGgMFNZjx1zwCx4veGxhI8bPp83ez5gChixA0BhCOwAUJiJA7uZ9czsl2b2azN7wcy+2kTHAAD1NJFj35J0p7ufNbOupF+Y2SPu/mQD5wYA7NLEgd3dXdLZnR+7O/9RgSpFVASV0kKotWoUT6NjJFnQDx/VK576aBS01CzGUnTFPtJIjt3M2mb2rKSTko66+1MVv3PYzI6Z2bGBtpp4WwBAhUYCu7uP3P0WSddJut3Mbq74nSPufpu739bVYhNvCwCo0OhTMe7+lqTHJR1q8rwAgAs3cY7dzA5KGrj7W2a2JOluSf80cc+wd+rm0bvx18fa7erXOzW/ctH5kglKWf7dBoNdHxPn5aU0N0/+HXusiadi3ivpX8ysre1/AXzf3X/cwHkBADU08VTMc5JubaAvAIAGMPMUAApDYAeAwrC64zwJiqRRoXO7MSmeLizEbYvVbbaYPOraSfrRCvoxTgqk/eoCqSR5v1/dkByj6BhJPhjGx0WFVYqquEgYsQNAYQjsAFAYAjsAFIYc+zyJ8uVpHr0bt/V6cdvKUuXr40uWw2N8KX6vcae6j5ZMKGptxvlyW69er6h1dj3uQ9iSi/PvTGrCxcGIHQAKQ2AHgMIQ2AGgMOTYS5Ms6BVtfpEu5tWLnzu3JF8+Xq1u678nPmawGj/H7lHfx3Euur0R933xjep8fiu7f1neO3meXkEffVg3aw/kGLEDQGEI7ABQGAI7ABSGwA4AhaF4inB3IkmybjxpyHvxImCDy6snKG0cjM/XX40Ll8NeVDwND9Hi6WQ3pHb1+eIpV1I7W+hrK96g3YfBccnEMHm2WxOQY8QOAIUhsANAYQjsAFAYcuzzJMvpRpL8+zjJsQ+Xq4/rH4jz6BtXxm2jIPltyZyh0eLur7e9GV9T61yy6NlmnGMPN/wYxXl09/hesEAYzocROwAUhsAOAIUhsANAYSbOsZvZ9ZL+VdI12t454Ii7f2PS82LvWLLwVbaoWDYsiJ4TH8ePsWscp7c1OFD9TLqNkv4pbhucrW4bLcUX5d3kef9sQ/CoLa158Bw76muieDqU9CV3f8bMDkj6lZkddff/bODcAIBdmjgV4+6vufszO/+/Jum4pGsnPS8AoJ5Gc+xmdoOkWyU91eR5AQAXrrHn2M3sEkk/kPRFdz9T0X5Y0mFJ6inebAEAMJlGAruZdbUd1B909x9W/Y67H5F0RJJW7QpmWOwjnkx4yXYNstHu21rBXB0pX9CrNQgKoVlttx+3Rcd5Uiz2VvIP3KzIDOyxiVMxtv1Ixf2Sjrv71yfvEgBgEk3k2O+Q9FlJd5rZszv/fbyB8wIAapg4FePuv1D6D2IAwF5iEbB54kESe5xsSBFtEiHJ1uOFrzrr1Qtmdc/FE3kW34jHBzasbvPkG9zejNs669Wvt4ZJ3WCQTBpKFvQK72/0eQATYkkBACgMgR0ACkNgB4DCkGMvTfLcuY+DtiyPnmzSbFuLYVv3zY3K15c6WZ09XiGss14jxx53Xb03q3Pi3dPxw++WbGbtwyTHzsYY2GOM2AGgMAR2ACgMgR0ACkNgB4DCUDydJ9GEmGRyjfeTVbvOBbN8JLWCRbEWk8Wy2ptxP0a96olN44V4bJJNNuqsVV9X53R10VeSbD2e8eSDuOjqwf0Ni9nAhBixA0BhCOwAUBgCOwAUhhz7PAkmykQ5YElSP9mtosbmEq1RvPBVd6N64TBJ6nSDr2o7WzgsWWQrqB3YRjyryZOaQlqLiO5vtggYk5owAUbsAFAYAjsAFIbADgCFIceOfOGwNP+e5JWjcyYLjmkzzm+3OsEGHe144470uoJ+ZLlyTxZE82yBMJ5Xxx5jxA4AhSGwA0BhCOwAUBgCOwAUhuIpaksLq8Hkm+yYbIciRROU6goKmlFRVTpPgbTGvZjpSUjZ5LTsumpMapvp+zQljNgBoDCNBHYze8DMTprZ802cDwBQX1Mj9u9IOtTQuQAAE2gkcenuT5jZDU2cC/tMmt9MFrFSMnGoxnuF+e1WMjYZ11hkKz0mu959rk5u22qO+2q8Va7GfZ/zvDw5dgAozJ49FWNmhyUdlqSelvfqbQFg7uzZiN3dj7j7be5+W1eLe/W2ADB3SMUAQGEaScWY2Xcl/YWkK83shKR/cPf7mzg3piwrumXFtWDVRUtWY7RO8nWMjmvVrNQFxdh8olFckDPFx3lyykbV/KwsuofZ55vcd0v64WHROil2enK+8Lj53p2qqadi7m3iPACAyZGKAYDCENgBoDAsAob6udlkYS5bWKh+fbH69e3zdeN+LARt2Q5KdXZ/GiS7Qm31wyZPdn+S1cgD11H3swo+/+gzlJTXNrJ+hIvDJfci+0yC4/K6Rvn5d0bsAFAYAjsAFIbADgCFIceO2rnZ1mI8g9iWlypf90vi5STGy/H5fKG6H+NunGNvDeJEa2ujOm9rZ9fDY9LnurPFw4LUfJoHThYci+YCpPMAkvpFdFxWD0lrG5noPg2Tm5HVgPpx3SPiwxlezO0CMWIHgMIQ2AGgMAR2ACgMgR0ACkPxdJ60Gl6Ya6kXNvmBlcrXR5dXvy5J/cvj4uloqXoMMm7HhbX2VjzZZGGtunja6cRjnVa2UNW43uJh8TFJwTD6vLICaTYxbCkodC9lxezkvbLrDRZfs2jCmCTPFhyLjtnKJoxl49mgsDpjE5cYsQNAYQjsAFAYAjsAFIYc+yyqs+O84g0VrJ1MUOolk5BW4slGo8uqc+lbB+O8/MZ74q9jf7W6754MTTrrcV501Ks+39IoPqa7mSxGlU2UiXLi2SJlSuoe0QSlLO8d5NElyVeDeshKnJf3ZGJYxoJFu9pnNnd9jCT5MMjZZ/c2aduzTVEuMkbsAFAYAjsAFIbADgCFIcc+bXU3iw4P2f3zz9mGClnbeDnOl/cvrT5u44r4K7dxMO57fzVsCnWDPLok2aj63nbX4v511uIcdit73j/6jOvWSqJFu5Ln2LNn0sfL1Z/V8JLks1+oNyZsb1UnsW0Yv1crecZdm9Em50n/0j9XZSTZGbEDQGEI7ABQGAI7ABSGwA4AhWmkeGpmhyR9Q9uzKv7Z3f+xifPOnLBIlkwAqlPszIpu2YJe0W5IyU5IvpJMbOnFX5/hSnU/BpeEh2hwIG4brux+1xtLZi+Ng1pdWhRMC931CqG1RJ9xVsDtxN+L0WJ122gxu3/ZvUiahtFEs+SgpBAaTa7zbEerTPRndcZmLk08YjeztqRvSfqYpJsk3WtmN016XgBAPU2kYm6X9JK7v+zufUnfk/TJBs4LAKihicB+raRX3vHziZ3X/h8zO2xmx8zs2EDJWskAgIk0kWOvSo69ayUldz8i6YgkrdoVs7Vq/TvVmFAU5raVb2QRHtdNJhQludQwB5sc48kGDaMkxz7uBLnUZGOMd39r3tkWLGCWHNNK1uWyIAXbGmSbaSR522xhqb3apCH5bno2YafOW2Up7ORyo88rW+gru++N31uvmZvfZ5r4tE9Iuv4dP18n6dUGzgsAqKGJwP60pPeb2Y1mtiDpM5J+1MB5AQA1TJyKcfehmX1B0qPaftzxAXd/YeKeAQBqaeQ5dnf/qaSfNnGufaHmwlxRTryVPCduy/Fz4tFm0d7LFmeKP9Iwz5r8uy0733hh95stZBtMt/rxfV84HeTYk8eLF9bitu7Z6n50zlVv3CBJlmy04cPdb+yQ5YfTuQpRHjjJRWcbT0QLc3k3eY49abNks5L2ZvV7tTaT+76V3ffguCz3XkgePcPMUwAoDIEdAApDYAeAwhDYAaAw7KBUJSuQJotsRUVSW1kOj/HVeFWs0aXVhdX+5XExdricTDYKmrICZGuYFPjGWVv16531+Jilk3E/xsHmQK1kc53Ftfi9eqeqD+ysxbOibSNuC4t4UlzUTO5fsn6ZFE3mSXYayvreCgq1naQAmS7alWifq541Zuub8UGDpHgaXLNnE8aS+14KRuwAUBgCOwAUhsAOAIWZ3xx73R3ikwW9og0r/MBKeMjgmnh3ifWrq8+3flX893E/2azCg663k8U2O2fjtsXT8USPdr86j9lNcuwL53Y/caQVvI8kddfivHfnTHVOt3X6XHiMr6/HbVs18++RcTLminLOwaYTkqTNpG4UvN4aJP3OFptLFvSyKCe+keTYs7boXiQ59nSC0l4t2HaRMWIHgMIQ2AGgMAR2ACgMgR0ACjO/xdNENgkp3fGoV13sHB2oXqVRkjYOxis1rl1f/ffu2RviwlDn4EbY1m5XF4bOrQezfyS1Xk92UHotHhcs/091gaq7nvT9XLICYbD6X7oq4Ea8hVI0ISYtkG4mBdKk0BhOiMmKeNnWUJFkglI2GSoqdlpWjE1kq1ZGheRoopGkfIJSdL60eFpGgTTDiB0ACkNgB4DCENgBoDDzm2NPFvpStsBR1hZMXhoeiBft2rw87sf6H1XnPq/60/8Nj7nj6pfDtquCLYVe2bwiPObxV94Xtm2MVsO2zrnq+9R7IzxEnbNxTry1Vp0Tt634GGU58c0gx57ketM8ep0JMWmuN86/e/JW8emSfH60w1O049Yk7xVcc3r/sgW9orY5yKNnGLEDQGEI7ABQGAI7ABRmfnPsdXcqT55x9+C53/FC/PfnsBfn7EeXVud7P3Tlf4fH3HP502Hbn3Sr88ovD5Ln7EfxM+7/fvoDYdvwZLAgWju+XhvEudRaz51n+fJ+kJuvk889nzr53hr59zT3nn3fk0W7Ghf0I938Yg4W7WoaI3YAKAyBHQAKM1FgN7N7zOwFMxub2W1NdQoAUN+kI/bnJX1a0hMN9AUA0ICJiqfuflySrOZuRDMpK/JE9yE9JmnqVheNltpxUfDSVjwp56p29U5OfY+3Sbpu6c2wrbuUFCdb8aSs+KDkPgULQaUF0mSCUljEm4XFo8J+1J3UVLMo3KT9cm8LsWdPxZjZYUmHJamn5b16WwCYO+cN7Gb2mKRrKpruc/eHL/SN3P2IpCOStGpX8NczAFwk5w3s7n73XnQEANCM+Z2glKk5WcKCRaLam8nmEhvJe71VvcnF8beq/gG17amVG+Lz6b8qX315eGV4xKubl4Vtg4148tJSsF6WjeLrtWGSI45ysDV3oy9y8ahZ7jsaNenjjp8ysxOSPirpJ2b2aDPdAgDUNelTMQ9JeqihvgAAGsDMUwAoDDn2KlluNnlu2oLnprMNJJZOxc97L/2hesGxF1evDo95UB8O246t3lj5+plh3IdnXrs+bOskG113z1bne9ubyYJOyUYW0UbN2XPn6cJSQMEYsQNAYQjsAFAYAjsAFIbADgCFmd/iaTKZIy26ZcW6oHjaOhPv8tNLCpAHlqOdjeJi5+/OXBu2vXQgKLoml9t9PZ6EtPyHeAWzpVPV96l7unonJEmybEGv6L6z8w7wLozYAaAwBHYAKAyBHQAKM7859ky6o3uSY98KNnZYOxce0z1ZPQlJklaDNHB3Pc6xb7we/109WKnO51tyudFEI0laPhVPKFo8VX0v2m/G98LPxbUI7weTvLLPCphTjNgBoDAEdgAoDIEdAApDjr1K9ox7trFDlAdONvvOtgHvjKvzxyvr8Z6xveS581EvyOe34l60+nEOu7MWbxbdOl2dL0/z6FvxYmk+muHNp4E9xogdAApDYAeAwhDYAaAwBHYAKAzF092qs3hYNHHpPOezcXVhsH02LkC2e/HkJe8GH3dW3B0mxclkxyPf2Kh+PSuQBouoSUmRlAIp8C6M2AGgMAR2ACgMgR0ACjNRjt3MvibpLyX1Jf1O0l+7+1tNdGwmBTlx92QaUjSpSXFe2drxwmG2Xp3bliRFxyUTlLIJQNGkIUnhYmme5eXTyUYs9gVcqElH7Ecl3ezuH5T0oqSvTN4lAMAkJgrs7v4zd397CPakpOsm7xIAYBJN5tg/L+mRBs8HAKjhvDl2M3tM0jUVTfe5+8M7v3OfpKGkB5PzHJZ0WJJ6ihexKlLdRcWC5+KzPLW147Ysl77bPmx3ZPd5bxbtAi6+8wZ2d787azezz0n6hKS73OM/me5+RNIRSVq1K/gTDAAXyaRPxRyS9GVJf+7u8XRIAMCemTTH/k1JByQdNbNnzezbDfQJADCBiUbs7v6+pjoCAGgGi4BNW1Yw9KDQmCza5cOGJ/JY8o+6OpOGKJACFx1LCgBAYQjsAFAYAjsAFIYc+yzayzx1lOcHsG8xYgeAwhDYAaAwBHYAKAyBHQAKQ2AHgMIQ2AGgMAR2ACgMgR0ACkNgB4DCENgBoDAEdgAoDIEdAApDYAeAwhDYAaAwBHYAKAyBHQAKQ2AHgMIQ2AGgMAR2ACgMgR0ACkNgB4DCmO/ljvdvv6nZ65J+v+dv3JwrJZ2adicmVMI1SFzHflLCNUj7+zr+2N0Pnu+XphLYZ52ZHXP326bdj0mUcA0S17GflHANUhnXQSoGAApDYAeAwhDY6zky7Q40oIRrkLiO/aSEa5AKuA5y7ABQGEbsAFAYAnsNZvY1M/uNmT1nZg+Z2WXT7lMdZnaPmb1gZmMzm7mnAMzskJn91sxeMrO/n3Z/6jCzB8zspJk9P+2+1GVm15vZz83s+M736e+m3ac6zKxnZr80s1/vXMdXp92nugjs9RyVdLO7f1DSi5K+MuX+1PW8pE9LemLaHdktM2tL+pakj0m6SdK9ZnbTdHtVy3ckHZp2JyY0lPQld/8zSR+R9Lcz+llsSbrT3T8k6RZJh8zsI1PuUy0E9hrc/WfuPtz58UlJ102zP3W5+3F3/+20+1HT7ZJecveX3b0v6XuSPjnlPu2auz8h6Y1p92MS7v6auz+z8/9rko5Luna6vdo933Z258fuzn8zWYQksE/u85IemXYn5tC1kl55x88nNIPBpDRmdoOkWyU9Nd2e1GNmbTN7VtJJSUfdfSavozPtDuxXZvaYpGsqmu5z94d3fuc+bf8z9MG97NtuXMh1zCireG0mR1elMLNLJP1A0hfd/cy0+1OHu48k3bJTN3vIzG5295mrfxDYA+5+d9ZuZp+T9AlJd/k+fmb0fNcxw05Iuv4dP18n6dUp9WXumVlX20H9QXf/4bT7Myl3f8vMHtd2/WPmAjupmBrM7JCkL0v6K3dfn3Z/5tTTkt5vZjea2YKkz0j60ZT7NJfMzCTdL+m4u3992v2py8wOvv2Em5ktSbpb0m+m26t6COz1fFPSAUlHzexZM/v2tDtUh5l9ysxOSPqopJ+Y2aPT7tOF2ilef0HSo9ou1n3f3V+Ybq92z8y+K+k/JH3AzE6Y2d9Mu0813CHps5Lu3Pnz8KyZfXzanarhvZJ+bmbPaXvgcNTdfzzlPtXCzFMAKAwjdgAoDIEdAApDYAeAwhDYAaAwBHYAKAyBHQAKQ2AHgMIQ2AGgMP8HEOrDxI2ZlVMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a16896908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def shuffle(*dfs):\n",
    "  dataset = reduce(lambda a,b: np.append(a, b, axis=0), dfs)\n",
    "  permutation = np.random.permutation(dataset.shape[0])\n",
    "  shuffled = dataset[permutation]\n",
    "  return shuffled\n",
    "\n",
    "\n",
    "unit = 0.15\n",
    "\n",
    "on = unit * 0.1\n",
    "off = unit * 0.00\n",
    "\n",
    "\n",
    "def generate_cluster(unit, cross_cov, mu, count):\n",
    "    mu = np.array(mu)\n",
    "    sigma_1, sigma_2, sigma_3 = unit, unit, unit\n",
    "    sigma_1_2, sigma_1_3, sigma_2_3 = cross_cov\n",
    "    cov = np.array([\n",
    "      [sigma_1, sigma_1_2, sigma_1_3],\n",
    "      [sigma_1_2, sigma_2, sigma_2_3],\n",
    "      [sigma_1_3, sigma_2_3, sigma_3]\n",
    "    ])\n",
    "    ds = np.random.multivariate_normal(mu, cov, count)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def reduce_correlation(ds, noise):\n",
    "    count = ds.shape[0]\n",
    "    ds_t = ds.T\n",
    "    for i, x in enumerate(noise):\n",
    "        num = int(count*x)\n",
    "        ds_t[i].put(np.random.choice(count, num, replace=False), np.random.choice(ds.T[i], num))\n",
    "    return ds_t.T\n",
    "\n",
    "\n",
    "rv1 = generate_cluster(unit, (on, -off, off), [2,0,0], 250000) \n",
    "rv2 = generate_cluster(unit, (on, off, -off), [0,0,2], 500000)\n",
    "rv3 = generate_cluster(unit, (-on, off, off), [0,2,0], 500000)\n",
    "rv4 = generate_cluster(unit, (-on, -off, -off), [-1,-1,-1], 1000000)\n",
    "\n",
    "# rv5 = generate_cluster(unit, (on, -off, off), [0,0,2], 1250000)\n",
    "# rv6 = generate_cluster(unit, (-on, off, -off), [0,2,0], 1000000)\n",
    "# \n",
    "# data = np.append(shuffle(rv1, rv2, rv3, rv4), shuffle(rv5, rv6), axis=1)\n",
    "\n",
    "data = reduce(lambda a,b: np.append(a, b, axis=0), [rv1, rv2, rv3, rv4])\n",
    "\n",
    "# ind_var = np.random.uniform(-1, 1, [data.shape[0], 20])\n",
    "\n",
    "# data = np.append(data, ind_var, axis=1)\n",
    "\n",
    "data = shuffle(data)\n",
    "\n",
    "# data = reduce_correlation(data, [0.75, 0.75, 0.75])\n",
    "plt.hist2d(data[:, 0], data[:, 1], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(object):\n",
    "    def __init__(self, n_input_units, n_hidden_layers, n_hidden_units, n_latent_units,\n",
    "                 learning_rate=0.005, batch_size=100, min_beta=1.0, max_beta=1.0,\n",
    "                 distribution='normal', serial_layering=None):\n",
    "        self.n_input_units = n_input_units\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_latent_units = n_latent_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.min_beta = min_beta\n",
    "        self.max_beta = max_beta\n",
    "        self.distribution = distribution\n",
    "        self.serial_layering = serial_layering or [self.n_hidden_layers]\n",
    "        if serial_layering:\n",
    "            if not isinstance(serial_layering, (list, tuple)):\n",
    "                raise TypeError(\"Argument 'serial_layering' must be a list or tuple of integers.\")\n",
    "            elif not all([isinstance(x, int) for x in serial_layering]):\n",
    "                raise TypeError(\"Argument 'serial_layering' must be a list or tuple of integers.\")\n",
    "            elif sum(serial_layering) != self.n_hidden_layers:\n",
    "                raise ValueError(\"Groupings in 'serial_layering' must sum to 'n_hidden_layers'.\")\n",
    "\n",
    "    class Encoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_latent_units, distribution, serial_layering=None):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_latent_units = n_latent_units\n",
    "            self.distribution = distribution\n",
    "            self.serial_layering = serial_layering or [self.n_hidden_layers]\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "            self.mu_list = []\n",
    "            self.applied_mu_list = []\n",
    "            self.sigma_list = []\n",
    "            self.applied_sigma_list = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs, initializer=None):\n",
    "            self.hidden_layers.append(\n",
    "                tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid, kernel_initializer=initializer))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_mu_and_sigma(self, inputs):\n",
    "            if self.distribution == 'normal':\n",
    "                mu = tf.layers.Dense(units=self.n_latent_units)\n",
    "                applied_mu = mu.apply(inputs)\n",
    "                sigma = tf.layers.Dense(units=self.n_latent_units)\n",
    "                applied_sigma = sigma.apply(inputs)\n",
    "            elif self.distribution == 'vmf':\n",
    "                mu = tf.layers.Dense(units=self.n_latent_units + 1,\n",
    "                                     activation=lambda x: tf.nn.l2_normalize(x, axis=-1))\n",
    "                applied_mu = mu.apply(inputs)\n",
    "                sigma = tf.layers.Dense(units=1, activation=tf.nn.softplus)\n",
    "                applied_sigma = sigma.apply(inputs) + 1\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "\n",
    "            self.mu_list.append(mu)\n",
    "            self.applied_mu_list.append(applied_mu)\n",
    "            self.sigma_list.append(sigma)\n",
    "            self.applied_sigma_list.append(applied_sigma)\n",
    "\n",
    "            return self.applied_mu_list[-1], self.applied_sigma_list[-1]\n",
    "\n",
    "        def build(self, inputs):\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            initializer = None\n",
    "            layer = inputs\n",
    "            for group_size in self.serial_layering:\n",
    "                for i in range(group_size):\n",
    "                    layer = self.add_hidden_layer(layer, initializer=initializer)\n",
    "                self.add_mu_and_sigma(layer)\n",
    "\n",
    "                # initializer = tf.initializers.constant(np.diag(np.ones(self.n_hidden_units)))\n",
    "                initializer = tf.initializers.identity()\n",
    "\n",
    "            return self.applied_mu_list, self.applied_sigma_list\n",
    "\n",
    "        @property\n",
    "        def mu(self):\n",
    "            return self.mu_list[-1]\n",
    "\n",
    "        @property\n",
    "        def sigma(self):\n",
    "            return self.sigma_list[-1]\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            mu = sess.run([self.mu.kernel, self.mu.bias])\n",
    "\n",
    "            sigma = sess.run([self.sigma.kernel, self.sigma.bias])\n",
    "\n",
    "            return layers, mu, sigma\n",
    "\n",
    "    class Decoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_output_units, serial_layering=None):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_output_units = n_output_units\n",
    "            self.serial_layering = serial_layering or [self.n_hidden_layers]\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "            self.output_list = []\n",
    "            self.applied_output_list = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs, initializer=None):\n",
    "            self.hidden_layers.append(\n",
    "                tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid, kernel_initializer=initializer))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_output(self, inputs):\n",
    "            output = tf.layers.Dense(units=self.n_output_units)\n",
    "            applied_output = output.apply(inputs)\n",
    "            self.output_list.append(output)\n",
    "            self.applied_output_list.append(applied_output)\n",
    "            return applied_output\n",
    "\n",
    "        def build(self, inputs_list):\n",
    "            if len(inputs_list) != len(self.serial_layering):\n",
    "                raise ValueError(\"Number of inputs ({}) must equal number of serial layering groups ({}).\"\n",
    "                                 .format(len(inputs_list), len(self.serial_layering)))\n",
    "\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            initializer = None\n",
    "            for group_size, inputs in zip(self.serial_layering, inputs_list):\n",
    "                layer = inputs\n",
    "                for i in range(group_size):\n",
    "                    layer = self.add_hidden_layer(layer, initializer=initializer)\n",
    "                self.add_output(layer)\n",
    "\n",
    "                # initializer = tf.initializers.constant(np.diag(np.ones(self.n_hidden_units)))\n",
    "                initializer = tf.initializers.identity()\n",
    "\n",
    "            return self.applied_output_list\n",
    "\n",
    "        @property\n",
    "        def output(self):\n",
    "            return self.output_list[-1]\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            output = sess.run([self.output.kernel, self.output.bias])\n",
    "\n",
    "            return layers, output\n",
    "\n",
    "    def sampled_z(self, mu, sigma, batch_size):\n",
    "        if self.distribution == 'normal':\n",
    "            epsilon = tf.random_normal(tf.stack([int(batch_size), self.n_latent_units]))\n",
    "            z = mu + tf.multiply(epsilon, tf.exp(0.5 * sigma))\n",
    "            loss = tf.reduce_mean(-0.5 * self.beta * tf.reduce_sum(1.0 + sigma - tf.square(mu) - tf.exp(sigma), 1))\n",
    "        elif self.distribution == 'vmf':\n",
    "            self.q_z = VonMisesFisher(mu, sigma, validate_args=True, allow_nan_stats=False)\n",
    "            z = self.q_z.sample()\n",
    "            self.p_z = HypersphericalUniform(self.n_latent_units, validate_args=True, allow_nan_stats=False)\n",
    "            loss = tf.reduce_mean(-self.q_z.kl_divergence(self.p_z))\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return z, loss\n",
    "\n",
    "    def build_feature_loss(self, x, output):\n",
    "        return tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x, output), 1))\n",
    "\n",
    "    def initialize_tensors(self):\n",
    "        self.x = tf.placeholder(\"float32\", [self.batch_size, self.n_input_units])\n",
    "        self.beta = tf.placeholder(\"float32\", [1, 1])\n",
    "        self.encoder = self.Encoder(self.n_hidden_layers, self.n_hidden_units, self.n_latent_units,\n",
    "                                    self.distribution, self.serial_layering)\n",
    "        mu_list, sigma_list = self.encoder.build(self.x)\n",
    "        self.mu_list = mu_list\n",
    "        self.sigma_list = sigma_list\n",
    "\n",
    "        self.z_list = []\n",
    "        self.latent_loss_list = []\n",
    "        for mu, sigma in zip(self.mu_list, self.sigma_list):\n",
    "            z, latent_loss = self.sampled_z(mu, sigma, self.batch_size)\n",
    "            self.z_list.append(z)\n",
    "            self.latent_loss_list.append(latent_loss)\n",
    "\n",
    "        self.decoder = self.Decoder(self.n_hidden_layers, self.n_hidden_units, self.n_input_units, self.serial_layering)\n",
    "        self.output_list = self.decoder.build(self.z_list)\n",
    "\n",
    "        self.feature_loss_list = [self.build_feature_loss(self.x, output) for output in self.output_list]\n",
    "        self.loss_list = [feature_loss + latent_loss for feature_loss, latent_loss in\n",
    "                          zip(self.feature_loss_list, self.latent_loss_list)]\n",
    "\n",
    "    def total_steps(self, data_count, epochs):\n",
    "        num_batches = int(data_count / self.batch_size)\n",
    "        return (num_batches * epochs) - epochs\n",
    "\n",
    "    def generate_beta_values(self, data_count, epochs):\n",
    "        total_steps = self.total_steps(data_count, epochs)\n",
    "        beta_delta = self.max_beta - self.min_beta\n",
    "        log_beta_step = 5 / float(total_steps)\n",
    "        return [\n",
    "            self.min_beta + (beta_delta * (1 - math.exp(-5 + (i * log_beta_step))))\n",
    "            for i in range(total_steps)\n",
    "        ]\n",
    "\n",
    "    def generate_optimizers(self, data_count, epochs):\n",
    "        total_steps = self.total_steps(data_count, epochs)\n",
    "        num_groups = len(self.feature_loss_list)\n",
    "        group_steps = int(total_steps / num_groups)\n",
    "\n",
    "        optimizers = [\n",
    "            (tf.train.AdamOptimizer(self.learning_rate).minimize(feature_loss + latent_loss), feature_loss, latent_loss)\n",
    "            for feature_loss, latent_loss in zip(self.feature_loss_list, self.latent_loss_list)\n",
    "        ]\n",
    "\n",
    "        optimizer_indices = [\n",
    "            min(num_groups - 1, int(i / group_steps))\n",
    "            for i in range(total_steps)\n",
    "        ]\n",
    "\n",
    "        return optimizers, optimizer_indices\n",
    "\n",
    "    def train_from_rdd(self, data_rdd, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "\n",
    "        data_count = data_rdd.count()\n",
    "        beta_values = self.generate_beta_values(data_count, epochs)\n",
    "        optimizers, optimizer_indices = self.generate_optimizers(data_count, epochs)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch_index in range(epochs):\n",
    "                iterator = data_rdd.toLocalIterator()\n",
    "                batch_index = 0\n",
    "                while True:\n",
    "                    batch = np.array(list(islice(iterator, self.batch_size)))\n",
    "                    if batch.shape[0] == self.batch_size:\n",
    "                        beta = beta_values.pop(0) if beta_values else self.min_beta\n",
    "                        optimizer_index = optimizer_indices.pop(0) if optimizer_indices else -1\n",
    "                        optimizer, feature_loss, latent_loss = optimizers[optimizer_index]\n",
    "\n",
    "                        feed_dict = {self.x: np.array(batch), self.beta: np.array([[beta]])}\n",
    "\n",
    "                        if not batch_index % 1000:\n",
    "                            print(\"beta: {}\".format(beta))\n",
    "                            print(\"layer group: {}\".format(optimizer_index))\n",
    "\n",
    "                            f_ls, d_ls = sess.run([feature_loss, latent_loss], feed_dict=feed_dict)\n",
    "                            \n",
    "                            print(\"avg_feature_loss={}, avg_latent_loss={}\".format(np.mean(f_ls), np.mean(d_ls)))\n",
    "                            print('running batch {} in epoch {}'.format(batch_index, epoch_index))\n",
    "                            \n",
    "                        sess.run(optimizer, feed_dict=feed_dict)\n",
    "                        batch_index += 1\n",
    "                    else:\n",
    "                        print(\"incomplete batch: {}\".format(batch.shape))\n",
    "                        break\n",
    "\n",
    "            print(\"evaluating model...\")\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "    def train(self, data, visualize=False, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "\n",
    "        data_size = data.shape[0]\n",
    "        batch_size = self.batch_size\n",
    "        beta_values = self.generate_beta_values(data_size, epochs)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            i = 0\n",
    "            while (i * batch_size) < data_size:\n",
    "                batch = data[i * batch_size:(i + 1) * batch_size]\n",
    "                beta = beta_values.pop(0) if len(beta_values) > 0 else self.min_beta\n",
    "                feed_dict = {self.x: batch, self.beta: np.array([[beta]])}\n",
    "                sess.run(optimizer, feed_dict=feed_dict)\n",
    "                if visualize and (not i % int((data_size / batch_size) / 3) or i == int(data_size / batch_size) - 1):\n",
    "                    ls, d, f_ls, d_ls = sess.run([self.loss, self.output, self.feature_loss, self.latent_loss],\n",
    "                                                 feed_dict=feed_dict)\n",
    "                    plt.scatter(batch[:, 0], batch[:, 1])\n",
    "                    plt.show()\n",
    "                    plt.scatter(d[:, 0], d[:, 1])\n",
    "                    plt.show()\n",
    "                    print(i, ls, np.mean(f_ls), np.mean(d_ls))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoderModel(object):\n",
    "    def __init__(self, encoder_layers, mu, sigma, decoder_layers, output):\n",
    "        self.encoder = self.EncoderModel(encoder_layers, mu, sigma)\n",
    "        self.decoder = self.DecoderModel(decoder_layers, output)\n",
    "\n",
    "    def save(self, path):\n",
    "        encoder_layers, encoder_mu, encoder_sigma = self.encoder.dump()\n",
    "        decoder_layers, decoder_output = self.decoder.dump()\n",
    "        serializable_model = (encoder_layers, encoder_mu, encoder_sigma, decoder_layers, decoder_output)\n",
    "        pickle.dump(serializable_model, open(path, 'w+'))\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder.encode(x)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.encoder.encode(x)[0]\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder.decode(x)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return cls(*pickle.load(open(path, 'r')))\n",
    "\n",
    "    class Layer(object):\n",
    "        def __init__(self, kernel, bias, activation='linear'):\n",
    "            self.kernel = kernel\n",
    "            self.bias = bias\n",
    "            self.activation = activation\n",
    "\n",
    "        def dump(self):\n",
    "            return (self.kernel, self.bias, self.activation)\n",
    "\n",
    "        @property\n",
    "        def apply_func(self):\n",
    "            kernel, bias = self.kernel, self.bias\n",
    "\n",
    "            linear = lambda inputs: np.matmul(inputs, kernel) + bias\n",
    "\n",
    "            if self.activation == 'linear':\n",
    "                f = linear\n",
    "            elif self.activation == 'sigmoid':\n",
    "                f = lambda inputs: 1 / (1 + np.exp(-linear(inputs)))\n",
    "\n",
    "            return f\n",
    "\n",
    "        def apply(self, inputs):\n",
    "            return self.apply_func(inputs)\n",
    "\n",
    "    class EncoderModel(object):\n",
    "        def __init__(self, encoder_layers, mu, sigma):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in encoder_layers\n",
    "            ]\n",
    "            self.mu = VariationalAutoEncoderModel.Layer(*mu)\n",
    "            self.sigma = VariationalAutoEncoderModel.Layer(*sigma)\n",
    "\n",
    "        def dump(self):\n",
    "            encoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            encoder_mu = self.mu.dump()[:2]\n",
    "            encoder_sigma = self.sigma.dump()[:2]\n",
    "            return encoder_layers, encoder_mu, encoder_sigma\n",
    "\n",
    "        def encode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.mu.apply(x), self.sigma.apply(x)\n",
    "\n",
    "    class DecoderModel(object):\n",
    "        def __init__(self, decoder_layers, output):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in decoder_layers\n",
    "            ]\n",
    "            self.output = VariationalAutoEncoderModel.Layer(*output)\n",
    "\n",
    "        def dump(self):\n",
    "            decoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            decoder_output = self.output.dump()[:2]\n",
    "            return decoder_layers, decoder_output\n",
    "\n",
    "        def decode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.output.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'inputs' referenced before assignment",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-92ef8663561e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                min_beta=0.01, max_beta=1, distribution='normal', serial_layering=[2,1])\\\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mtrain_from_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-94a95098493d>\u001b[0m in \u001b[0;36mtrain_from_rdd\u001b[0;34m(self, data_rdd, epochs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_from_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mdata_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-94a95098493d>\u001b[0m in \u001b[0;36minitialize_tensors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_input_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_layering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_feature_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-94a95098493d>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, inputs_list)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_layering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'inputs' referenced before assignment"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = VariationalAutoEncoder(n_input_units=data.shape[1], n_hidden_layers=3, \n",
    "                               n_hidden_units=9, n_latent_units=1, \n",
    "                               learning_rate=0.005, batch_size=100, \n",
    "                               min_beta=0.01, max_beta=1, distribution='normal', serial_layering=[2,1])\\\n",
    "    .train_from_rdd(rdd, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = VariationalAutoEncoder(n_input_units=data.shape[1], n_hidden_layers=2, \n",
    "#                                n_hidden_units=9, n_latent_units=1, \n",
    "#                                learning_rate=0.005, batch_size=100, \n",
    "#                                min_beta=1, max_beta=1, distribution='vmf')\\\n",
    "#     .train(data, epochs=1, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.21000e+02, 2.18000e+03, 9.39000e+03, 7.42160e+04, 3.18314e+05,\n        3.27378e+05, 1.65921e+05, 6.05690e+04, 2.03770e+04, 8.98500e+03,\n        4.43100e+03, 2.56800e+03, 1.56200e+03, 1.03600e+03, 7.06000e+02,\n        4.92000e+02, 3.30000e+02, 2.62000e+02, 2.25000e+02, 1.45000e+02,\n        1.28000e+02, 1.00000e+02, 7.00000e+01, 7.60000e+01, 4.50000e+01,\n        5.20000e+01, 3.90000e+01, 3.40000e+01, 3.50000e+01, 3.40000e+01,\n        2.50000e+01, 2.40000e+01, 4.10000e+01, 4.50000e+01, 9.40000e+01,\n        1.47000e+02, 2.93000e+02, 5.02000e+02, 9.34000e+02, 1.76700e+03,\n        3.12300e+03, 5.81700e+03, 1.09550e+04, 1.89330e+04, 2.67950e+04,\n        3.28170e+04, 3.62460e+04, 3.78510e+04, 3.87970e+04, 4.02140e+04,\n        4.26260e+04, 4.66300e+04, 5.29790e+04, 5.29080e+04, 2.77050e+04,\n        1.15540e+04, 5.17700e+03, 2.43700e+03, 1.24900e+03, 9.59000e+02,\n        1.06600e+03, 1.89200e+03, 3.64900e+03, 7.84000e+03, 1.87800e+04,\n        5.60940e+04, 9.73400e+04, 5.37610e+04, 9.77500e+03, 8.51000e+02,\n        4.33000e+02, 5.96000e+02, 9.02000e+02, 1.39100e+03, 1.75200e+03,\n        2.18500e+03, 2.56200e+03, 3.02200e+03, 3.46400e+03, 3.78900e+03,\n        4.30500e+03, 4.80600e+03, 5.35500e+03, 5.88900e+03, 6.57700e+03,\n        7.54300e+03, 8.67900e+03, 9.76700e+03, 1.15120e+04, 1.39230e+04,\n        1.67980e+04, 2.11100e+04, 2.70210e+04, 3.49520e+04, 4.53470e+04,\n        5.72300e+04, 6.56530e+04, 6.65320e+04, 5.28870e+04, 1.34050e+04]),\n array([-1.20683071, -1.17842565, -1.15002059, -1.12161552, -1.09321046,\n        -1.0648054 , -1.03640033, -1.00799527, -0.97959021, -0.95118515,\n        -0.92278008, -0.89437502, -0.86596996, -0.83756489, -0.80915983,\n        -0.78075477, -0.7523497 , -0.72394464, -0.69553958, -0.66713451,\n        -0.63872945, -0.61032439, -0.58191933, -0.55351426, -0.5251092 ,\n        -0.49670414, -0.46829907, -0.43989401, -0.41148895, -0.38308388,\n        -0.35467882, -0.32627376, -0.29786869, -0.26946363, -0.24105857,\n        -0.21265351, -0.18424844, -0.15584338, -0.12743832, -0.09903325,\n        -0.07062819, -0.04222313, -0.01381806,  0.014587  ,  0.04299206,\n         0.07139713,  0.09980219,  0.12820725,  0.15661231,  0.18501738,\n         0.21342244,  0.2418275 ,  0.27023257,  0.29863763,  0.32704269,\n         0.35544776,  0.38385282,  0.41225788,  0.44066294,  0.46906801,\n         0.49747307,  0.52587813,  0.5542832 ,  0.58268826,  0.61109332,\n         0.63949839,  0.66790345,  0.69630851,  0.72471358,  0.75311864,\n         0.7815237 ,  0.80992876,  0.83833383,  0.86673889,  0.89514395,\n         0.92354902,  0.95195408,  0.98035914,  1.00876421,  1.03716927,\n         1.06557433,  1.0939794 ,  1.12238446,  1.15078952,  1.17919458,\n         1.20759965,  1.23600471,  1.26440977,  1.29281484,  1.3212199 ,\n         1.34962496,  1.37803003,  1.40643509,  1.43484015,  1.46324521,\n         1.49165028,  1.52005534,  1.5484604 ,  1.57686547,  1.60527053,\n         1.63367559]),\n <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ebc117e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sample = model.project(data)\n",
    "plt.hist(encoded_sample, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22535, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sample = np.array(rdd.sample(False, 0.01).collect())\n",
    "encoded_sample = model.project(vec_sample)\n",
    "\n",
    "# decoded_sample = data\n",
    "decoded_sample = model.decode(encoded_sample)\n",
    "decoded_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 22860.687369517636, 0.41072610349447514)\n",
      "(3, 12763.132987421812, 0.4873411057035125)\n",
      "(4, 9621.984985738864, 0.6049687548656997)\n",
      "(5, 9496.53577122682, 0.5700197372127715)\n",
      "(6, 9048.603016026918, 0.37332278940894126)\n",
      "(7, 8913.499825276993, 0.31950360789236504)\n",
      "(8, 8888.716481612686, 0.2757845583639317)\n",
      "(9, 8776.807228609861, 0.30912203993217174)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "encoded_x_sample = encoded_sample[:,0].reshape(-1,1)\n",
    "dencoded_y_sample = decoded_sample[:,0].reshape(-1,1)\n",
    "\n",
    "for i in range(2, 10):\n",
    "  z_train, z_test, x_train, x_test = train_test_split(encoded_x_sample, dencoded_y_sample, test_size=0.4)\n",
    "  gmm = GaussianMixture(i).fit(z_train)\n",
    "  bic = gmm.bic(z_test)\n",
    "  labels = gmm.predict(z_test)\n",
    "  ss = silhouette_score(x_test, labels)\n",
    "  print(i, bic, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.append(labels.reshape(-1,1), x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    1, ..., 9012, 9013, 9013]),\n",
       " array([0, 1, 0, ..., 1, 0, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
