{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbevers/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from itertools import islice\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from hyperspherical_vae.distributions import VonMisesFisher, HypersphericalUniform\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(*dfs):\n",
    "  dataset = reduce(lambda a,b: np.append(a, b, axis=0), dfs)\n",
    "  permutation = np.random.permutation(dataset.shape[0])\n",
    "  shuffled = dataset[permutation]\n",
    "  return shuffled\n",
    "\n",
    "\n",
    "unit = 0.15\n",
    "\n",
    "on = unit * 0.1\n",
    "off = unit * 0.00\n",
    "\n",
    "\n",
    "def generate_cluster(unit, cross_cov, mu, count):\n",
    "    mu = np.array(mu)\n",
    "    sigma_1, sigma_2, sigma_3 = unit, unit, unit\n",
    "    sigma_1_2, sigma_1_3, sigma_2_3 = cross_cov\n",
    "    cov = np.array([\n",
    "      [sigma_1, sigma_1_2, sigma_1_3],\n",
    "      [sigma_1_2, sigma_2, sigma_2_3],\n",
    "      [sigma_1_3, sigma_2_3, sigma_3]\n",
    "    ])\n",
    "    ds = np.random.multivariate_normal(mu, cov, count)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def reduce_correlation(ds, noise):\n",
    "    count = ds.shape[0]\n",
    "    ds_t = ds.T\n",
    "    for i, x in enumerate(noise):\n",
    "        num = int(count*x)\n",
    "        ds_t[i].put(np.random.choice(count, num, replace=False), np.random.choice(ds.T[i], num))\n",
    "    return ds_t.T\n",
    "\n",
    "\n",
    "rv1 = generate_cluster(unit, (on, -off, off), [2,0,0], 250000) \n",
    "rv2 = generate_cluster(unit, (on, off, -off), [0,0,2], 500000)\n",
    "rv3 = generate_cluster(unit, (-on, off, off), [0,2,0], 500000)\n",
    "rv4 = generate_cluster(unit, (-on, -off, -off), [-1,-1,-1], 1000000)\n",
    "\n",
    "# rv5 = generate_cluster(unit, (on, -off, off), [0,0,2], 1250000)\n",
    "# rv6 = generate_cluster(unit, (-on, off, -off), [0,2,0], 1000000)\n",
    "# \n",
    "# data = np.append(shuffle(rv1, rv2, rv3, rv4), shuffle(rv5, rv6), axis=1)\n",
    "\n",
    "data = reduce(lambda a,b: np.append(a, b, axis=0), [rv1, rv2, rv3, rv4])\n",
    "\n",
    "# ind_var = np.random.uniform(-1, 1, [data.shape[0], 20])\n",
    "\n",
    "# data = np.append(data, ind_var, axis=1)\n",
    "\n",
    "data = shuffle(data)\n",
    "\n",
    "# data = reduce_correlation(data, [0.75, 0.75, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([-2.86533005, -2.73414875, -2.60296746, -2.47178617, -2.34060487,\n",
       "        -2.20942358, -2.07824229, -1.94706099, -1.8158797 , -1.68469841,\n",
       "        -1.55351711, -1.42233582, -1.29115452, -1.15997323, -1.02879194,\n",
       "        -0.89761064, -0.76642935, -0.63524806, -0.50406676, -0.37288547,\n",
       "        -0.24170418, -0.11052288,  0.02065841,  0.15183971,  0.283021  ,\n",
       "         0.41420229,  0.54538359,  0.67656488,  0.80774617,  0.93892747,\n",
       "         1.07010876,  1.20129005,  1.33247135,  1.46365264,  1.59483394,\n",
       "         1.72601523,  1.85719652,  1.98837782,  2.11955911,  2.2507404 ,\n",
       "         2.3819217 ,  2.51310299,  2.64428428,  2.77546558,  2.90664687,\n",
       "         3.03782817,  3.16900946,  3.30019075,  3.43137205,  3.56255334,\n",
       "         3.69373463]),\n",
       " array([-2.9179612 , -2.7850824 , -2.65220359, -2.51932479, -2.38644599,\n",
       "        -2.25356719, -2.12068839, -1.98780958, -1.85493078, -1.72205198,\n",
       "        -1.58917318, -1.45629437, -1.32341557, -1.19053677, -1.05765797,\n",
       "        -0.92477916, -0.79190036, -0.65902156, -0.52614276, -0.39326395,\n",
       "        -0.26038515, -0.12750635,  0.00537245,  0.13825126,  0.27113006,\n",
       "         0.40400886,  0.53688766,  0.66976647,  0.80264527,  0.93552407,\n",
       "         1.06840287,  1.20128167,  1.33416048,  1.46703928,  1.59991808,\n",
       "         1.73279688,  1.86567569,  1.99855449,  2.13143329,  2.26431209,\n",
       "         2.3971909 ,  2.5300697 ,  2.6629485 ,  2.7958273 ,  2.92870611,\n",
       "         3.06158491,  3.19446371,  3.32734251,  3.46022132,  3.59310012,\n",
       "         3.72597892]),\n",
       " <matplotlib.image.AxesImage at 0x1104e9e90>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndgAoDIEdAApDYAeAwvwfmBjj9ln4V4EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1286ab110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist2d(data[:,0], data[:,1], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(object):\n",
    "    def __init__(self, n_input_units, n_hidden_layers, n_hidden_units, n_latent_units,\n",
    "                 learning_rate=0.005, batch_size=100, min_beta=1.0, max_beta=1.0,\n",
    "                 distribution='normal'):\n",
    "        self.n_input_units = n_input_units\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.n_latent_units = n_latent_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.min_beta = min_beta\n",
    "        self.max_beta = max_beta\n",
    "        self.distribution = distribution\n",
    "\n",
    "    class Encoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_latent_units, distribution):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_latent_units = n_latent_units\n",
    "            self.distribution = distribution\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs):\n",
    "            self.hidden_layers.append(tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_mu(self, inputs):\n",
    "            if self.distribution == 'normal':\n",
    "                self.mu = tf.layers.Dense(units=self.n_latent_units)\n",
    "            elif self.distribution == 'vmf':\n",
    "                self.mu = tf.layers.Dense(units=self.n_latent_units + 1, \n",
    "                                          activation=lambda x: tf.nn.l2_normalize(x, axis=-1))\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "                \n",
    "            self.applied_mu = self.mu.apply(inputs)\n",
    "            return self.applied_mu\n",
    "\n",
    "        def add_sigma(self, inputs):\n",
    "            if self.distribution == 'normal':\n",
    "                self.sigma = tf.layers.Dense(units=self.n_latent_units)\n",
    "                self.applied_sigma = self.sigma.apply(inputs)\n",
    "            elif self.distribution == 'vmf':\n",
    "                self.sigma = tf.layers.Dense(units=1, activation=tf.nn.softplus)\n",
    "                self.applied_sigma = self.sigma.apply(inputs) + 1\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "            return self.applied_sigma\n",
    "\n",
    "        def build(self, inputs):\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            layer = self.add_hidden_layer(inputs)\n",
    "\n",
    "            for i in range(self.n_hidden_layers - 1):\n",
    "                layer = self.add_hidden_layer(layer)\n",
    "\n",
    "            mu = self.add_mu(layer)\n",
    "            sigma = self.add_sigma(layer)\n",
    "\n",
    "            return mu, sigma\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            mu = sess.run([self.mu.kernel, self.mu.bias])\n",
    "\n",
    "            sigma = sess.run([self.sigma.kernel, self.sigma.bias])\n",
    "\n",
    "            return layers, mu, sigma\n",
    "\n",
    "    class Decoder(object):\n",
    "        def __init__(self, n_hidden_layers, n_hidden_units, n_output_units):\n",
    "            self.n_hidden_layers = n_hidden_layers\n",
    "            self.n_hidden_units = n_hidden_units\n",
    "            self.n_output_units = n_output_units\n",
    "\n",
    "        def init_hidden_layers(self):\n",
    "            self.hidden_layers = []\n",
    "            self.applied_hidden_layers = []\n",
    "\n",
    "        def add_hidden_layer(self, inputs):\n",
    "            self.hidden_layers.append(tf.layers.Dense(units=self.n_hidden_units, activation=tf.nn.sigmoid))\n",
    "            self.applied_hidden_layers.append(self.hidden_layers[-1].apply(inputs))\n",
    "            return self.applied_hidden_layers[-1]\n",
    "\n",
    "        def add_output(self, inputs):\n",
    "            self.output = tf.layers.Dense(units=self.n_output_units)\n",
    "            self.applied_output = self.output.apply(inputs)\n",
    "            return self.applied_output\n",
    "\n",
    "        def build(self, inputs):\n",
    "            self.init_hidden_layers()\n",
    "\n",
    "            layer = self.add_hidden_layer(inputs)\n",
    "\n",
    "            for i in range(self.n_hidden_layers - 1):\n",
    "                layer = self.add_hidden_layer(layer)\n",
    "\n",
    "            output = self.add_output(layer)\n",
    "\n",
    "            return output\n",
    "\n",
    "        def eval(self, sess):\n",
    "            layers = [\n",
    "                sess.run([l.kernel, l.bias])\n",
    "                for l in self.hidden_layers\n",
    "            ]\n",
    "\n",
    "            output = sess.run([self.output.kernel, self.output.bias])\n",
    "\n",
    "            return layers, output\n",
    "\n",
    "    def sampled_z(self, mu, sigma, batch_size):\n",
    "        if self.distribution == 'normal':\n",
    "            epsilon = tf.random_normal(tf.stack([int(batch_size), self.n_latent_units]))\n",
    "            z = mu + tf.multiply(epsilon, tf.exp(0.5 * sigma))\n",
    "            loss = tf.reduce_mean(-0.5 * self.beta * tf.reduce_sum(1.0 + sigma - tf.square(mu) - tf.exp(sigma), 1))\n",
    "        elif self.distribution == 'vmf':\n",
    "            self.q_z = VonMisesFisher(mu, sigma, validate_args=True, allow_nan_stats=False)\n",
    "            z = self.q_z.sample()\n",
    "            self.p_z = HypersphericalUniform(self.n_latent_units, validate_args=True, allow_nan_stats=False)\n",
    "            loss = tf.reduce_mean(-self.q_z.kl_divergence(self.p_z))\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return z, loss\n",
    "\n",
    "    def build_feature_loss(self, x, output):\n",
    "        return tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x, output), 1))\n",
    "\n",
    "    def initialize_tensors(self):\n",
    "        self.x = tf.placeholder(\"float32\", [self.batch_size, self.n_input_units])\n",
    "        self.beta = tf.placeholder(\"float32\", [1, 1])\n",
    "        self.encoder = self.Encoder(self.n_hidden_layers, self.n_hidden_units, self.n_latent_units, \n",
    "                                    self.distribution)\n",
    "        mu, sigma = self.encoder.build(self.x)\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        z, latent_loss = self.sampled_z(self.mu, self.sigma, self.batch_size)\n",
    "        self.z = z\n",
    "        self.latent_loss = latent_loss\n",
    "        \n",
    "        self.decoder = self.Decoder(self.n_hidden_layers, self.n_hidden_units, self.n_input_units)\n",
    "        self.output = self.decoder.build(self.z)\n",
    "        \n",
    "        self.feature_loss = self.build_feature_loss(self.x, self.output)\n",
    "        self.loss = self.feature_loss + self.latent_loss\n",
    "        \n",
    "\n",
    "    def generate_beta_values(self, data_count, epochs):\n",
    "        num_batches = int(data_count / self.batch_size)\n",
    "        total_steps = (num_batches * epochs) - epochs\n",
    "        beta_delta = self.max_beta - self.min_beta\n",
    "        log_beta_step = 5 / float(total_steps)\n",
    "        beta_values = [\n",
    "            self.min_beta + (beta_delta * (1 - math.exp(-5 + (i * log_beta_step))))\n",
    "            for i in range(total_steps)\n",
    "        ]\n",
    "        return beta_values\n",
    "\n",
    "    def train_from_rdd(self, data_rdd, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "\n",
    "        data_count = data_rdd.count()\n",
    "        beta_values = self.generate_beta_values(data_count, epochs)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch_index in range(epochs):\n",
    "                iterator = data_rdd.toLocalIterator()\n",
    "                batch_index = 0\n",
    "                while True:\n",
    "                    batch = np.array(list(islice(iterator, self.batch_size)))\n",
    "                    if batch.shape[0] == self.batch_size:\n",
    "                        beta = beta_values.pop(0) if len(beta_values) > 0 else self.min_beta\n",
    "                        feed_dict = {self.x: np.array(batch), self.beta: np.array([[beta]])}\n",
    "\n",
    "                        if not batch_index % 1000:\n",
    "                            print(\"beta: {}\".format(beta))\n",
    "                            ls, f_ls, d_ls = sess.run([self.loss, self.feature_loss, self.latent_loss],\n",
    "                                                      feed_dict=feed_dict)\n",
    "                            print(\"loss={}, avg_feature_loss={}, avg_latent_loss={}\".format(ls, np.mean(f_ls),\n",
    "                                                                                            np.mean(d_ls)))\n",
    "                            print('running batch {} in epoch {}'.format(batch_index, epoch_index))\n",
    "                        sess.run(optimizer, feed_dict=feed_dict)\n",
    "                        batch_index += 1\n",
    "                    else:\n",
    "                        print(\"incomplete batch: {}\".format(batch.shape))\n",
    "                        break\n",
    "\n",
    "            print(\"evaluating model...\")\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "    def train(self, data, visualize=False, epochs=1):\n",
    "        self.initialize_tensors()\n",
    "        \n",
    "        data_size = data.shape[0]\n",
    "        batch_size = self.batch_size\n",
    "        beta_values = self.generate_beta_values(data_size, epochs)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            i = 0\n",
    "            while (i * batch_size) < data_size:\n",
    "                batch = data[i * batch_size:(i + 1) * batch_size]\n",
    "                beta = beta_values.pop(0) if len(beta_values) > 0 else self.min_beta\n",
    "                feed_dict = {self.x: batch, self.beta: np.array([[beta]])}\n",
    "                sess.run(optimizer, feed_dict=feed_dict)\n",
    "                if visualize and (not i % int((data_size / batch_size) / 3) or i == int(data_size / batch_size) - 1):\n",
    "                    ls, d, f_ls, d_ls = sess.run([self.loss, self.output, self.feature_loss, self.latent_loss],\n",
    "                                                 feed_dict=feed_dict)\n",
    "                    plt.scatter(batch[:, 0], batch[:, 1])\n",
    "                    plt.show()\n",
    "                    plt.scatter(d[:, 0], d[:, 1])\n",
    "                    plt.show()\n",
    "                    print(i, ls, np.mean(f_ls), np.mean(d_ls))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            encoder_layers, eval_mu, eval_sigma = self.encoder.eval(sess)\n",
    "            decoder_layers, eval_output = self.decoder.eval(sess)\n",
    "\n",
    "        return VariationalAutoEncoderModel(encoder_layers, eval_mu, eval_sigma, decoder_layers, eval_output)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoderModel(object):\n",
    "    def __init__(self, encoder_layers, mu, sigma, decoder_layers, output):\n",
    "        self.encoder = self.EncoderModel(encoder_layers, mu, sigma)\n",
    "        self.decoder = self.DecoderModel(decoder_layers, output)\n",
    "\n",
    "    def save(self, path):\n",
    "        encoder_layers, encoder_mu, encoder_sigma = self.encoder.dump()\n",
    "        decoder_layers, decoder_output = self.decoder.dump()\n",
    "        serializable_model = (encoder_layers, encoder_mu, encoder_sigma, decoder_layers, decoder_output)\n",
    "        pickle.dump(serializable_model, open(path, 'w+'))\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder.encode(x)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.encoder.encode(x)[0]\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder.decode(x)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return cls(*pickle.load(open(path, 'r')))\n",
    "\n",
    "    class Layer(object):\n",
    "        def __init__(self, kernel, bias, activation='linear'):\n",
    "            self.kernel = kernel\n",
    "            self.bias = bias\n",
    "            self.activation = activation\n",
    "\n",
    "        def dump(self):\n",
    "            return (self.kernel, self.bias, self.activation)\n",
    "\n",
    "        @property\n",
    "        def apply_func(self):\n",
    "            kernel, bias = self.kernel, self.bias\n",
    "\n",
    "            linear = lambda inputs: np.matmul(inputs, kernel) + bias\n",
    "\n",
    "            if self.activation == 'linear':\n",
    "                f = linear\n",
    "            elif self.activation == 'sigmoid':\n",
    "                f = lambda inputs: 1 / (1 + np.exp(-linear(inputs)))\n",
    "\n",
    "            return f\n",
    "\n",
    "        def apply(self, inputs):\n",
    "            return self.apply_func(inputs)\n",
    "\n",
    "    class EncoderModel(object):\n",
    "        def __init__(self, encoder_layers, mu, sigma):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in encoder_layers\n",
    "            ]\n",
    "            self.mu = VariationalAutoEncoderModel.Layer(*mu)\n",
    "            self.sigma = VariationalAutoEncoderModel.Layer(*sigma)\n",
    "\n",
    "        def dump(self):\n",
    "            encoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            encoder_mu = self.mu.dump()[:2]\n",
    "            encoder_sigma = self.sigma.dump()[:2]\n",
    "            return encoder_layers, encoder_mu, encoder_sigma\n",
    "\n",
    "        def encode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.mu.apply(x), self.sigma.apply(x)\n",
    "\n",
    "    class DecoderModel(object):\n",
    "        def __init__(self, decoder_layers, output):\n",
    "            self.layers = [\n",
    "                VariationalAutoEncoderModel.Layer(kernel, bias, 'sigmoid')\n",
    "                for kernel, bias in decoder_layers\n",
    "            ]\n",
    "            self.output = VariationalAutoEncoderModel.Layer(*output)\n",
    "\n",
    "        def dump(self):\n",
    "            decoder_layers = [l.dump()[:2] for l in self.layers]\n",
    "            decoder_output = self.output.dump()[:2]\n",
    "            return decoder_layers, decoder_output\n",
    "\n",
    "        def decode(self, inputs):\n",
    "            x = inputs\n",
    "            for l in self.layers:\n",
    "                x = l.apply(x)\n",
    "            return self.output.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.993329432471\n",
      "loss=4.47393751144, avg_feature_loss=4.15156364441, avg_latent_loss=0.322373718023\n",
      "running batch 0 in epoch 0\n",
      "beta: 0.938384389304\n",
      "loss=2.28316926956, avg_feature_loss=1.24345874786, avg_latent_loss=1.03971064091\n",
      "running batch 1000 in epoch 0\n",
      "beta: 0.430860498018\n",
      "loss=1.22265672684, avg_feature_loss=0.52480918169, avg_latent_loss=0.697847485542\n",
      "running batch 2000 in epoch 0\n",
      "incomplete batch: (0,)\n",
      "evaluating model...\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = VariationalAutoEncoder(n_input_units=data.shape[1], n_hidden_layers=2, \n",
    "                               n_hidden_units=9, n_latent_units=1, \n",
    "                               learning_rate=0.005, batch_size=1000, \n",
    "                               min_beta=0.01, max_beta=1, distribution='normal')\\\n",
    "    .train_from_rdd(rdd, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = VariationalAutoEncoder(n_input_units=data.shape[1], n_hidden_layers=2, \n",
    "#                                n_hidden_units=9, n_latent_units=1, \n",
    "#                                learning_rate=0.005, batch_size=100, \n",
    "#                                min_beta=1, max_beta=1, distribution='vmf')\\\n",
    "#     .train(data, epochs=1, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 68., 248., 335., 357., 391., 415., 359., 391., 384., 343., 324.,\n",
       "        231., 233., 173., 141., 119.,  93.,  76.,  49.,  40.,  34.,  23.,\n",
       "         27.,  24.,  20.,  11.,  11.,  20.,  14.,   0.,   7.,   9.,   8.,\n",
       "         11.,  33., 136., 772., 653., 420., 250., 123.,  52.,  17.,  15.,\n",
       "         16.,  32.,  58.,  81., 143., 278., 490., 712., 806., 810., 691.,\n",
       "        443., 246.,  98.,  45.,  10.,   5.,   5.,   2.,   0.,   3.,   2.,\n",
       "         11.,   2.,   7.,   7.,  15.,   7.,  25.,  43.,  44.,  50.,  66.,\n",
       "         85., 116., 151., 166., 246., 267., 340., 430., 503., 598., 670.,\n",
       "        788., 891., 820., 805., 799., 721., 553., 420., 241., 128.,  68.,\n",
       "         16.]),\n",
       " array([-1.68547198, -1.65597804, -1.62648411, -1.59699018, -1.56749624,\n",
       "        -1.53800231, -1.50850838, -1.47901444, -1.44952051, -1.42002658,\n",
       "        -1.39053264, -1.36103871, -1.33154477, -1.30205084, -1.27255691,\n",
       "        -1.24306297, -1.21356904, -1.18407511, -1.15458117, -1.12508724,\n",
       "        -1.09559331, -1.06609937, -1.03660544, -1.0071115 , -0.97761757,\n",
       "        -0.94812364, -0.9186297 , -0.88913577, -0.85964184, -0.8301479 ,\n",
       "        -0.80065397, -0.77116004, -0.7416661 , -0.71217217, -0.68267823,\n",
       "        -0.6531843 , -0.62369037, -0.59419643, -0.5647025 , -0.53520857,\n",
       "        -0.50571463, -0.4762207 , -0.44672677, -0.41723283, -0.3877389 ,\n",
       "        -0.35824496, -0.32875103, -0.2992571 , -0.26976316, -0.24026923,\n",
       "        -0.2107753 , -0.18128136, -0.15178743, -0.1222935 , -0.09279956,\n",
       "        -0.06330563, -0.0338117 , -0.00431776,  0.02517617,  0.05467011,\n",
       "         0.08416404,  0.11365797,  0.14315191,  0.17264584,  0.20213977,\n",
       "         0.23163371,  0.26112764,  0.29062157,  0.32011551,  0.34960944,\n",
       "         0.37910338,  0.40859731,  0.43809124,  0.46758518,  0.49707911,\n",
       "         0.52657304,  0.55606698,  0.58556091,  0.61505484,  0.64454878,\n",
       "         0.67404271,  0.70353665,  0.73303058,  0.76252451,  0.79201845,\n",
       "         0.82151238,  0.85100631,  0.88050025,  0.90999418,  0.93948811,\n",
       "         0.96898205,  0.99847598,  1.02796992,  1.05746385,  1.08695778,\n",
       "         1.11645172,  1.14594565,  1.17543958,  1.20493352,  1.23442745,\n",
       "         1.26392138]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11054e6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sample = model.project(data)\n",
    "plt.hist(encoded_sample, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22535, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sample = np.array(rdd.sample(False, 0.01).collect())\n",
    "encoded_sample = model.project(vec_sample)\n",
    "\n",
    "# decoded_sample = data\n",
    "decoded_sample = model.decode(encoded_sample)\n",
    "decoded_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 22860.687369517636, 0.41072610349447514)\n",
      "(3, 12763.132987421812, 0.4873411057035125)\n",
      "(4, 9621.984985738864, 0.6049687548656997)\n",
      "(5, 9496.53577122682, 0.5700197372127715)\n",
      "(6, 9048.603016026918, 0.37332278940894126)\n",
      "(7, 8913.499825276993, 0.31950360789236504)\n",
      "(8, 8888.716481612686, 0.2757845583639317)\n",
      "(9, 8776.807228609861, 0.30912203993217174)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "encoded_x_sample = encoded_sample[:,0].reshape(-1,1)\n",
    "dencoded_y_sample = decoded_sample[:,0].reshape(-1,1)\n",
    "\n",
    "for i in range(2, 10):\n",
    "  z_train, z_test, x_train, x_test = train_test_split(encoded_x_sample, dencoded_y_sample, test_size=0.4)\n",
    "  gmm = GaussianMixture(i).fit(z_train)\n",
    "  bic = gmm.bic(z_test)\n",
    "  labels = gmm.predict(z_test)\n",
    "  ss = silhouette_score(x_test, labels)\n",
    "  print(i, bic, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.append(labels.reshape(-1,1), x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    1, ..., 9012, 9013, 9013]),\n",
       " array([0, 1, 0, ..., 1, 0, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
